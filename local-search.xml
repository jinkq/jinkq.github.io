<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>神经网络</title>
    <link href="/2023/03/02/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <url>/2023/03/02/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<h1 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h1><h2 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h2><ul><li><p>神经元：函数（第一层是每一个像素点，隐层是零件一步步组装的过程）</p></li><li><p>权重：关注什么样的像素图案</p></li><li><p>bias：加权和要有多大才能使神经元的激活有意义</p></li></ul><p>$$<br>a^{(1)}&#x3D;\sigma(Wa^{(0)}+b)<br>$$</p><p>其中$a$是某一层的神经元向量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Network</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, *args, **kwargs</span>):<br>        <span class="hljs-keyword">pass</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">feedforward</span>(<span class="hljs-params">self, a</span>):<br>        <span class="hljs-keyword">for</span> b, w <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(self.biases, self.weights):<br>            a = sigmoid(np.dot(w, a) + b)<br>        <span class="hljs-keyword">return</span> a<br></code></pre></td></tr></table></figure><ul><li>ReLU函数$ReLU(a)&#x3D;max(0, a)$比sigmoid函数更好训练</li></ul><h3 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h3><p>单个样本的cost：</p><img src="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20220322214549.png" style="zoom:50%;" /><p>损失：样本的平均cost</p><p>函数的梯度指出了函数最陡的增长方向，所以沿梯度的负方向走函数值下降得最快</p><h3 id="反向传播算法"><a href="#反向传播算法" class="headerlink" title="反向传播算法"></a>反向传播算法</h3><p><img src="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20220323150037.png"></p><ul><li>随机梯度下降：划分minibatch（比如一个minibatch是100个样本），用minibatch的平均改变值代替负梯度（全部样本的平均改变值）</li></ul><p>【醉汉下山法】</p><p>假设每层只有1个神经元：</p><p><img src="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20220323151113.png"></p><p><img src="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20220323151332.png"></p><p><img src="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20220323151213.png"></p><p>负梯度：</p><p><img src="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20220323151247.png"></p><p>拓展到多个神经元：</p><p><img src="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20220323151620.png"></p><h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><ul><li>非线性：导数不是常数，多层网络不退化成单层网络</li><li>几乎处处可为：保证优化中梯度的可计算性</li><li>计算简单</li></ul><h4 id="sigmoid函数"><a href="#sigmoid函数" class="headerlink" title="sigmoid函数"></a>sigmoid函数</h4><p>缺点：</p><ul><li>计算量大，反向传播求误差梯度时，求导复杂</li><li>反向传播的时候，很容易出现梯度消失的情况，从而无法完成深度神经网络的训练（导数从0开始，很快又趋近于0）</li></ul><h4 id="ReLU函数"><a href="#ReLU函数" class="headerlink" title="ReLU函数"></a>ReLU函数</h4><p>优点：梯度不会消失，计算速度很快，收敛很快</p><p>缺点：输出不是0均值</p><p>dead relu problem，某些神经元可能永远不会激活，导致参数不会更新</p><h4 id="softmax函数"><a href="#softmax函数" class="headerlink" title="softmax函数"></a>softmax函数</h4><p>multiclassification</p><h4 id="tanh函数"><a href="#tanh函数" class="headerlink" title="tanh函数"></a>tanh函数</h4><p>是0均值</p><p>梯度消失问题，但是缓解了一点</p><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><h4 id="交叉熵-cross-entropy-loss-function"><a href="#交叉熵-cross-entropy-loss-function" class="headerlink" title="交叉熵 cross entropy loss function"></a>交叉熵 cross entropy loss function</h4><p>用于衡量两个分布之间的距离</p><p>逻辑回归</p><p>logLoss (对数损失函数，LR)</p><p>hinge loss (合页损失函数，SVM)</p><p>exp-loss (指数损失函数，AdaBoost)</p><p>cross-entropy loss (交叉熵损失函数，Softmax)</p><p>quadratic loss (平方误差损失函数，线性回归)</p><p>absolution loss (绝对值损失函数， )</p><p>0-1 loss (0-1损失函数)</p><h3 id="层"><a href="#层" class="headerlink" title="层"></a>层</h3><p>图像处理：</p><p>input→（卷积层$\times$N+池化层）$\times$M→全连接层$\times$K→output</p><h4 id="全连接层-Fully-Connected-Layer"><a href="#全连接层-Fully-Connected-Layer" class="headerlink" title="全连接层 Fully Connected Layer"></a>全连接层 Fully Connected Layer</h4><h4 id="卷积层-Convolutional-layer"><a href="#卷积层-Convolutional-layer" class="headerlink" title="卷积层 Convolutional layer"></a>卷积层 Convolutional layer</h4><ul><li>局部感知野</li><li>参数共享</li></ul><h4 id="池化层-pooling-layer"><a href="#池化层-pooling-layer" class="headerlink" title="池化层 pooling layer"></a>池化层 pooling layer</h4><ul><li>使特征图变小，简化网络</li><li>特征压缩，提取主要特征</li></ul><p>常用池化层：</p><ul><li>最大池化</li><li>平均池化</li></ul><h3 id="迭代"><a href="#迭代" class="headerlink" title="迭代"></a>迭代</h3><ul><li><p>epoch：用训练集所有样本对模型进行一次完整训练</p></li><li><p>batch：用训练集中一小部分样本对模型权重进行一次反向传播的参数更新</p></li><li><p>iteration：用一个batch的数据对模型进行一次参数更新的过程</p></li></ul><p>梯度下降：</p><table><thead><tr><th>梯度下降方式</th><th>Training Set Size</th><th>Batch Size</th><th>Number of Batches</th></tr></thead><tbody><tr><td>BGD</td><td>N</td><td>N</td><td>1</td></tr><tr><td>SGD</td><td>N</td><td>1</td><td>N</td></tr><tr><td>Mini-Batch</td><td>N</td><td>B</td><td>N&#x2F;B</td></tr></tbody></table><h2 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h2><h3 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h3><p>$$<br>a^{(t)}&#x3D;tanh(W_{ax}x^{(t)}+W_{aa}a^{(t-1)}+b_a)<br>$$</p><p>$$<br>\hat{y}^{(t)}&#x3D;softmax(W_{ya}a^{(t)}+b_y)<br>$$</p><p><img src="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20220324153051.png"></p><p><img src="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20220324154733.png"></p><p>特点：</p><ul><li>串联结构，后面结果的生成要参考前面的信息</li><li>所有特征共享同一套参数<ul><li>面对不同的输入（两个方面），能学到不同的相应的结果</li><li>极大减少了训练参数量</li><li>输入和输出数据在不同例子中可以有不同长度</li></ul></li></ul><h4 id="损失函数-1"><a href="#损失函数-1" class="headerlink" title="损失函数"></a>损失函数</h4><p>单个时间步的损失函数：<br>$$<br>L^{<t>}(\hat{y}^{<t>},y^{<t>})&#x3D;-y^{<t>}log y^{<t>}-(1-y^{<t>})log(1-y^{<t>})<br>$$</p><p>整个序列的损失函数：所有时间步的损失求和</p><p>$$<br>L(\hat{y},y)&#x3D;\sum^T_{t&#x3D;1}L^{<t>}(\hat{y}^{<t>},y^{<t>})<br>$$</p><h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><h5 id="梯度消失"><a href="#梯度消失" class="headerlink" title="梯度消失"></a>梯度消失</h5><p>当序列太长时，连乘过多，容易导致梯度消失，无法将信息传递过去，参数更新只能捕捉到局部依赖关系，没法再捕捉序列之间的长期关联或依赖关系。这会导致渐变在向后传播时呈指数级收缩。由于梯度极小，内部权重几乎没有调整，因此较早的层无法进行任何学习。<br>$$<br>\frac{\partial L_t}{\partial W_x}&#x3D;\sum^t_{k&#x3D;1}[\frac{\partial L_t}{\partial O_t}\frac{\partial O_t}{\partial S_t}(\prod^t_{j&#x3D;t+1-k}\frac{\partial tanh(\theta_j)}{\partial \theta_j})X_{t+1-k}W_s^{k-1}]<br>$$<br>由于$W_s^{k-1}$很小的时候连乘会趋于0，因此会梯度消失</p><p>梯度消失解决办法：</p><ul><li><p>选择relu等梯度大部分落在常数上的激活函数。relu函数的导数在正数部分是恒等于1的，因此在深层网络中使用relu激活函数就不会导致梯度消失的问题。</p></li><li><p>batch normalization</p><p>BN就是通过对每一层的输出规范为均值和方差一致的方法，消除了权重参数放大缩小带来的影响，进而解决梯度消失的问题，或者可以理解为BN将输出从饱和区拉到了非饱和区。采用 Batch Normalization 层，对网络中计算得到中间值进行归一化，使得中 间计算结果的取值在均值为 0，方差为 1 这样的分布内。那么此时，在 sigmoid 和 tanh 中，函数取值处于中间变化较大的部分，梯度取值也相对较大</p></li></ul><h5 id="梯度爆炸"><a href="#梯度爆炸" class="headerlink" title="梯度爆炸"></a>梯度爆炸</h5><p>使网络变得不稳定。在极端情况下，权重的值变得非常大，以至于溢出，导致NaN值</p><p>解决办法：</p><ul><li>梯度修剪，观察梯度向量，如果大于某个阈值，缩放梯度向量，保证它不会太大</li><li>权重正则化：检查网络权重的大小，并惩罚产生较大权重值的损失函数</li><li>改变损失函数：把sigmoid改为ReLU，relu函数的导数在正数部分是恒等于1的，因此在深层网络中使用relu激活函数就不会导致梯度消失的问题</li></ul><h3 id="LSTM-Long-Short-Term-Memory"><a href="#LSTM-Long-Short-Term-Memory" class="headerlink" title="LSTM Long Short-Term Memory"></a>LSTM Long Short-Term Memory</h3><p>RNN：想把所有东西记住，不管有没有用</p><p>LSTM：设计记忆细胞，具备选择性记忆的功能，记忆重要信息，过滤掉噪声信息，减轻记忆负担</p><p><img src="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20220324162626.png"></p><p><img src="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20220324164635.png"></p><ul><li>$C_t$，记忆细胞：在LSTM每个时间步里面都有一个以及细胞，让LSTM有能力自由选择每个时间步里记忆的内容。</li><li>$h_t$，状态：【本门考试分数】</li><li>$f_t$，遗忘门：[0, 1]的向量，选择性遗忘上一个时间步带来的记忆【遗忘复习高数时对线代无关的记忆】</li><li>$i_t$，输入门：[0, 1]的向量，选择性保留这一个时间步新生成的记忆（来自$X_t$）【复习线代获得的记忆】</li></ul><p>$$<br>C_t&#x3D;f_tC_{t-1}+i_t\tilde{C_t}<br>$$</p><ul><li>$o_t$，输出门：【只用到其中一部分记忆用来答题】</li><li>tanh：【记忆转化为答题能力的过程】</li></ul><h4 id="LSTM优点"><a href="#LSTM优点" class="headerlink" title="LSTM优点"></a>LSTM优点</h4><ul><li>缓解RNN的梯度消失问题：通过调节$W_{hf},W_{hi},W_{hg}$可以灵活控制$\frac{\partial C_j}{\partial C_{j-1}}$的值，当要从n时刻长期记忆某个东西直到m时刻时，该路径上的$\prod_{t&#x3D;n}^m\limits \frac{\partial C_j}{\partial C_{j-1}}\approx 1\times1\times…\times1$</li></ul><h4 id="解决LSTM梯度消失问题"><a href="#解决LSTM梯度消失问题" class="headerlink" title="解决LSTM梯度消失问题"></a>解决LSTM梯度消失问题</h4><ol><li>使用门控循环单元（GRU）替代LSTM。GRU也是一种递归神经网络，可以在处理序列数据时具有与LSTM相似的记忆能力，但是GRU只有两个门控单元，因此具有更少的参数，同时也更容易训练。</li><li>使用长短期记忆网络（LSTM）的变种，如Peephole LSTM，对输入门、遗忘门和输出门进行扩展，以便更好地控制信息的流动，从而减少梯度消失的问题。</li><li>对于输入数据进行归一化处理，将其缩放到一个较小的范围内。这有助于减少梯度消失问题的影响，并使神经网络更容易训练。</li><li>使用梯度裁剪技术。这种方法可以在训练过程中对梯度进行剪切，使其不会超出一个预定的范围。这有助于避免梯度爆炸问题，并减少梯度消失的问题。</li><li>采用注意力机制（Attention Mechanism）。该方法能够使网络在处理序列数据时更加关注重要的部分，并将注意力集中在最相关的信息上，从而避免了长序列中梯度消失的问题。</li></ol><h3 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h3><p>序列预测任务是给定了n个词，去预测第n+1个词，把关注点放在前面n个词的哪一些。引入注意力机制可以更好地解释应该重点关注前面序列里的哪一些</p><h4 id="transformer和LSTM区别"><a href="#transformer和LSTM区别" class="headerlink" title="transformer和LSTM区别"></a>transformer和LSTM区别</h4><p>RNN是串行训练，要把上一步输入全部输入进来才能做下一步训练，transformer加了注意力机制可以并行训练，没有记忆力这个东西，只是通过注意力机制去找到这个东西是要关注前面的哪一些词，但并没有记得之前一共发生过什么东西</p><p>Transformer和LSTM是用于处理序列数据的两种不同的神经网络架构，它们有以下几个区别：</p><ol><li>结构不同：LSTM是一种递归神经网络，使用门控单元（如输入门、遗忘门、输出门）来控制序列中信息的流动。而Transformer则是一种基于自注意力机制的神经网络，使用多头注意力机制来对序列数据进行编码。</li><li>并行性不同：由于LSTM的递归结构，每个时间步的计算都依赖于前一个时间步的输出。这意味着LSTM在计算时必须按照时间步序列化计算，无法进行并行化处理。相比之下，Transformer的计算是全局的，可以并行计算，从而更加高效。</li><li>记忆能力不同：LSTM通过门控单元来控制信息的流动，从而具有很强的记忆能力，能够处理长序列数据。而Transformer使用自注意力机制，可以同时考虑序列中的所有位置，从而不需要像LSTM一样依赖于递归计算，因此在长序列上表现更好。</li><li>可解释性不同：由于LSTM的递归结构，很难对其内部的计算过程进行解释。而Transformer的注意力机制可以将模型的注意力集中在输入序列中的不同部分，因此更容易解释模型的决策过程。</li></ol><p>总的来说，LSTM适合处理单向、多层次、长期的时间序列数据，例如语音识别和自然语言处理。而Transformer适合处理双向、并行化、全局的时间序列数据，例如机器翻译和自然语言生成。</p><h2 id="生成对抗网络"><a href="#生成对抗网络" class="headerlink" title="生成对抗网络"></a>生成对抗网络</h2><h3 id="GAN"><a href="#GAN" class="headerlink" title="GAN"></a>GAN</h3><p><img src="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20220329111525.png"></p><p><img src="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20220329112132.png"></p><ul><li>全连接层升维</li><li>把7$\times$7的数据变成28$\times$28的图像：插值（UpSampling2D）&#x2F;反卷积</li></ul><p><img src="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20220329112427.png"></p><ul><li>图像二分类：表示层（卷积池化卷积池化），应用层</li></ul><p><img src="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20220329112632.png"></p><p><img src="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20220329112702.png"></p><ol><li>固定G，把真实样本和generate的样本输入D，训练D</li><li>固定D，训练G</li><li>输入噪声给G，生成样本</li></ol><h4 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h4><p><img src="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20220329113720.png"></p><p><img src="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20220329113745.png"></p><p>原问题化为KL散度的问题：</p><p><img src="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20220329153116.png"></p><ul><li>生成器没生成真实的样本：惩罚小</li><li>生成器生成不真实的样本：惩罚大</li></ul><p>loss偏向于生成“稳妥”的样本</p><h3 id="WGAN"><a href="#WGAN" class="headerlink" title="WGAN"></a>WGAN</h3><p><img src="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20220329153349.png"></p><ul><li>训练稳定性大幅增长</li></ul><p><img src="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20220329153517.png"></p><h3 id="CGAN"><a href="#CGAN" class="headerlink" title="CGAN"></a>CGAN</h3><p>输入G的不只是噪声，还有条件（比如红头发），把概率分布改为条件概率分布</p><h3 id="DCGAN"><a href="#DCGAN" class="headerlink" title="DCGAN"></a>DCGAN</h3><p>卷积神经网络+对抗神经网络</p><ul><li>在不改变GAN原理的情况下，增加增强稳定性的tricks</li></ul><p><img src="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20220329113339.png"></p><h2 id="防止过拟合"><a href="#防止过拟合" class="headerlink" title="防止过拟合"></a>防止过拟合</h2><ul><li>池化</li><li>dropout</li><li>early stopping</li><li>正则化</li></ul><h2 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h2><p>强化学习有很多状态，要学习在每个状态下要采取什么动作。</p><p><img src="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20220325165554.png"></p>]]></content>
    
    
    <categories>
      
      <category>计算机</category>
      
      <category>机器学习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>面试题</title>
    <link href="/2023/03/02/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E9%9D%A2%E8%AF%95%E9%A2%98/"/>
    <url>/2023/03/02/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E9%9D%A2%E8%AF%95%E9%A2%98/</url>
    
    <content type="html"><![CDATA[<h1 id="面试题"><a href="#面试题" class="headerlink" title="面试题"></a>面试题</h1><p>Logistic回归的损失函数和梯度分别是多少、SVM的数学推导、GBDT回归的梯度代表什么</p>]]></content>
    
    
    <categories>
      
      <category>计算机</category>
      
      <category>机器学习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>机器学习</title>
    <link href="/2023/03/02/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    <url>/2023/03/02/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h1 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h1><h2 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h2><p>以下是进行预测任务特征工程时需要考虑的关键步骤：</p><ol><li>数据探索：在开始进行特征工程之前，需要探索数据集并了解不同变量之间的关系。这可以涉及可视化数据，计算汇总统计信息，识别潜在的相关性或模式。</li><li>特征选择：并非所有特征都与特定预测任务相关或有用。特征选择涉及识别最重要的特征，并删除冗余、无关或噪声较大的特征。这可以使用相关性分析、特征重要性排名或基于模型的选择等方法实现。</li><li>特征提取：有时可用的特征可能不够信息丰富，或者需要转换以显示有用的模式或关系。特征提取涉及从现有特征中创建新特征，例如计算统计量、跨时间段聚合信息或将分类变量转换为数值形式。</li><li>特征缩放：许多机器学习模型要求特征在相似范围内进行缩放，以获得良好的性能。特征缩放涉及将数据规范化，以减少异常值的影响并确保所有特征具有相似的范围。</li><li>特征编码：分类变量在预测任务中可能会出现问题，因为需要将它们转换为数值形式。特征编码涉及将分类变量转换为可用于机器学习模型的形式，例如进行独热编码或使用标签编码。</li><li>迭代细化：特征工程通常是一个迭代过程，在每一轮特征工程后，需要监视模型的性能。这可以帮助识别需要进一步改进和微调特征工程过程的领域。</li></ol><h2 id="数据不服从高斯分布为什么也可以ML"><a href="#数据不服从高斯分布为什么也可以ML" class="headerlink" title="数据不服从高斯分布为什么也可以ML"></a>数据不服从高斯分布为什么也可以ML</h2><p>渐进理论，样本量足够大，用中心极限定理可以近似高斯分布</p><p>机器学习算法的理论基础通常建立在样本满足一些假设条件的基础上，例如正态分布或同方差性等。然而，在实际应用中，数据往往不满足这些假设条件，特别是在大数据集中，这种假设条件往往难以满足。因此，机器学习算法可以用于非正态分布数据。</p><p>以下是一些原因：</p><ol><li>机器学习算法的鲁棒性：许多机器学习算法具有一定的鲁棒性，即它们不会受到数据分布的轻微偏差的影响，而且能够在许多实际应用中表现出良好的性能。例如，决策树、随机森林和支持向量机等算法经常被用于非正态分布数据。</li><li>数据预处理：数据预处理是数据科学中的重要步骤。在处理非正态分布数据时，可以通过一些预处理技术来使数据更适合机器学习算法。例如，可以进行对数变换、幂变换或归一化等处理，以使数据更符合算法假设条件。这些技术通常可以在保持数据质量的同时提高算法的性能。</li><li>非参数方法：非参数方法是指不需要假设数据分布的一类机器学习算法。这些算法可以对非正态分布的数据建模，并能够在一些领域中取得良好的性能。例如，K近邻、神经网络和决策树等算法不需要数据分布假设，因此可以用于非正态分布数据。</li></ol><p>综上所述，尽管机器学习算法的理论基础建立在一些假设条件的基础上，但在实际应用中，这些假设条件经常难以满足。在处理非正态分布数据时，可以使用一些机器学习算法和数据预处理技术来提高模型性能，并在许多实际应用中获得良好的结果。</p><h2 id="bias-variance-tradeoff"><a href="#bias-variance-tradeoff" class="headerlink" title="bias variance tradeoff"></a>bias variance tradeoff</h2><ul><li>bias：预测值的期望和实际值的差</li><li>variance：预测值的方差</li></ul><p>嘈杂的数据集：</p><ul><li>模型复杂度越大，偏差(Bias)越小</li><li>模型复杂度越大，方差(Variance)越大</li></ul><p>通常一个模型Variance很高，意味着它往往很复杂，而复杂的模型往往会对某些样本点非常敏感，也就很可能Overfit。同样的，一个模型如果Bias很高，意味着他一直都很不准，所以Underfitting。而我们的目标则是在模型的复杂度之间找到一个平衡，从而使Bias 和Variance都尽可能低，从而得到一个Good Fitting的结果</p><p><img src="https://jkq-pic.oss-cn-shanghai.aliyuncs.com/img/20221213155957.png"></p><p><img src="https://jkq-pic.oss-cn-shanghai.aliyuncs.com/img/20221213155840.png"></p><h2 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h2><h3 id="ROC-AUC-Sensitivity-Specificity"><a href="#ROC-AUC-Sensitivity-Specificity" class="headerlink" title="ROC, AUC, Sensitivity, Specificity"></a>ROC, AUC, Sensitivity, Specificity</h3><ul><li><strong>真阳性率（True Positive Rate，简称TPR）</strong>：TP&#x2F;(TP+FN)，代表的含义是：实际观测为<strong>阳</strong>的样本中，模型能够正确识别出来的比例。TPR又称为Sensitivity。</li><li><strong>真阴性率（True Negative Rate，简称TNR）</strong>：TN&#x2F;(FP+TN)，代表的含义是：实际观测为<strong>阴</strong>的样本中，模型能够正确识别出来的比例。TNR又称为Specificity。</li><li><strong>假阳性率（False Positive Rate，简称FPR）</strong>：FP&#x2F;(FP+TN)，代表的含义是：实际观测为<strong>阴</strong>的样本中，被模型错误地划分成阳性的比例。FPR&#x3D;1-Specificity。</li></ul><p><strong>TPR越高、FPR越低</strong>，说明模型的预测能力越好</p><p>以FPR为横坐标、TPR为纵坐标，将每一个阈值所对应的(FPR,TPR)放入坐标系中。用红色线条将所有的点连接起来——此即为<strong>ROC曲线</strong></p><p><img src="https://jkq-pic.oss-cn-shanghai.aliyuncs.com/img/20221213104352.png"></p><p><strong>AUC</strong>：ROC曲线下的面积</p><p>优点：不受正负样本比例的影响</p><p>缺点：</p><ul><li><p>对FPR和TPR两种错误的代价同等看待</p></li><li><p>没有给出模型误差的空间分布信息</p></li></ul>]]></content>
    
    
    <categories>
      
      <category>计算机</category>
      
      <category>机器学习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>树</title>
    <link href="/2023/03/02/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E6%A0%91/"/>
    <url>/2023/03/02/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E6%A0%91/</url>
    
    <content type="html"><![CDATA[<h1 id="树"><a href="#树" class="headerlink" title="树"></a>树</h1><h2 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h2><p>优点：</p><ul><li>因为是非参数模型，不需要对样本进行预先假设，可以处理复杂样本。</li><li>计算速度快，结果可解释性强。</li><li>可以同时处理分类和预测问题，对缺失值不敏感。</li></ul><p>缺点：</p><ul><li>容易过拟合</li><li>特征之间存在相互关联时，数据结果表现较差。</li></ul><p>优化方法：</p><ul><li><p>进行剪枝防止过拟合。</p><p>剪枝包括**预剪枝(Prepruning)<strong>和</strong>后剪枝(Postpruning),**前者通过对连续型变量设置阈值，来控制树的深度，或者控制节点的个数，在节点开始划分之前就进行操作，进而防止过拟合现象。后者是自底向上对非叶节点进行考察，如果这个内部节点换成叶节点能提升决策树的泛化能力，那就把它换掉。</p></li><li><p>运用交叉验证的方法选择合适的参数。</p></li><li><p>通过模型集成的方法（Bagging-parallel&#x2F;Boosting-serial），基于决策树构建更复杂的模型</p></li></ul><h2 id="回归树"><a href="#回归树" class="headerlink" title="回归树"></a>回归树</h2><h3 id="CART"><a href="#CART" class="headerlink" title="CART"></a>CART</h3><h2 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h2>]]></content>
    
    
    <categories>
      
      <category>计算机</category>
      
      <category>机器学习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>聚类</title>
    <link href="/2023/03/02/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E8%81%9A%E7%B1%BB/"/>
    <url>/2023/03/02/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E8%81%9A%E7%B1%BB/</url>
    
    <content type="html"><![CDATA[<h1 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h1><h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><ul><li>基于划分</li><li>基于层次</li><li>基于密度</li><li>基于网格</li><li>基于模型</li></ul><h3 id="基于划分"><a href="#基于划分" class="headerlink" title="基于划分"></a>基于划分</h3><ul><li><p>代表算法：K-means</p></li><li><p>优点：收敛速度快</p></li><li><p>缺点：要求类别数目 k 可以合理地估计，并且初始中心的选择和噪声会对聚类结果产生很大影响</p></li></ul><h4 id="K-means-vs-KNN"><a href="#K-means-vs-KNN" class="headerlink" title="K-means vs KNN"></a>K-means vs KNN</h4><ul><li><p>K-means：聚类，无监督</p><ol><li>随机选择k个样本作为初始聚类中心 a&#x3D;a1,a2,…ak ；</li><li>针对数据集中每个样本$x_i$ 计算它到 k 个聚类中心的距离并将其分到距离最小的聚类中心所对应的类中；</li><li>针对每个类别$a_j$ ，重新计算它的聚类中心$a_j&#x3D;\frac{1}{|ci|}\sum \limits_{x∈c_i}x$（即属于该类的所有样本的质心）；</li><li>重复上面2、3两步，直到达到某个中止条件（迭代次数、最小误差变化等）</li></ol><p>优点：</p><ul><li>容易理解，聚类效果不错，虽然是局部最优， 但往往局部最优就够了；</li><li>处理大数据集的时候，该算法可以保证较好的伸缩性；</li><li>当簇近似高斯分布的时候，效果非常不错；</li><li>算法复杂度低。</li></ul><p>缺点：</p><ul><li>K 值需要人为设定，不同 K 值得到的结果不一样；</li><li>对初始的簇中心敏感，不同选取方式会得到不同结果；</li><li>对异常值敏感；</li><li>不适合太离散的分类、样本类别不平衡的分类、非凸形状的分类。</li></ul></li><li><p>KNN：分类&#x2F;回归，有监督</p><p>将预测点与所有点距离进行计算，然后保存并排序，选出前面K个值看看哪些类别比较多，则预测的点属于哪类</p><p>优点：</p><ul><li>简单易用，相比其他算法，KNN算是比较简洁明了的算法。即使没有很高的数学基础也能搞清楚它的原理。</li><li>模型训练时间快，上面说到KNN算法是惰性的，这里也就不再过多讲述。<br>预测效果好。</li><li>对异常值不敏感</li></ul><p>缺点：</p><ul><li>对内存要求较高，因为该算法存储了所有训练数据</li><li>对不相关的功能和数据规模敏感</li></ul></li></ul><h4 id="KNN-K值选取"><a href="#KNN-K值选取" class="headerlink" title="KNN-K值选取"></a>KNN-K值选取</h4><p>从选取一个较小的K值开始，不断增加K的值，然后计算验证集合的方差，最终找到一个比较合适的K值</p><h3 id="基于层次"><a href="#基于层次" class="headerlink" title="基于层次"></a>基于层次</h3><p>对给定的数据进行层次分解，直到满足某种条件为止。分为自底向上法（AGNES）和自顶向下法（DIANA）</p><ul><li>优点：距离和规则的相似度容易定义，限制少，不需要预先制定簇的个数，可以发现簇的层次关系</li><li>缺点：计算复杂度太高，奇异值也能产生很大影响，算法很可能聚类成链状</li></ul><h3 id="基于密度"><a href="#基于密度" class="headerlink" title="基于密度"></a>基于密度</h3><p>寻找被低密度区域分离的高密度区域。与基于距离的聚类算法不同的是，基于距离的聚类算法的聚类结果是球状（凸）的簇，而基于密度的聚类算法可以发现任意形状的簇。基于密度的聚类方法是从数据对象分布区域的密度着手的。如果给定类中的数据对象在给定的范围区域中，则数据对象的密度超过某一阈值就继续聚类。这种方法通过连接密度较大的区域，能够形成不同形状的簇，而且可以消除孤立点和噪声对聚类质量的影响，以及发现任意形状的簇</p><h4 id="OPTICS"><a href="#OPTICS" class="headerlink" title="OPTICS"></a>OPTICS</h4><p>OPTICS（Ordering Points To Identify the Clustering Structure）是一种密度聚类算法，用于发现任意形状和任意密度的聚类结构。它可以处理高维数据和噪声数据，同时不需要预先指定簇的数量。</p><p>OPTICS算法的主要思想是，对于每个数据点，通过计算其与周围点的距离来评估其密度，并根据密度大小将其分类为核心点、边界点或噪声点。接着，从核心点开始构建聚类簇，通过对距离和密度进行扫描，将相邻的核心点分配到同一个簇中。最终，OPTICS算法可以得到一个聚类结构图，其中每个簇对应一个局部密度较高的区域。</p><p>OPTICS算法有一些优点。首先，它可以自适应地发现不同形状和密度的聚类结构，而不需要事先知道簇的数量。其次，它可以处理噪声数据和高维数据。此外，它不需要预先设置距离阈值，因此可以灵活地处理不同密度的数据。</p><p>然而，OPTICS算法也存在一些缺点。首先，它的计算复杂度较高，因此对于大规模数据集，其性能可能较差。其次，其结果可能受到参数设置的影响，例如邻域大小等。因此，在使用OPTICS算法时，需要根据具体情况进行参数调整和结果评估。</p><h3 id="基于网格"><a href="#基于网格" class="headerlink" title="基于网格"></a>基于网格</h3><p>将空间量化为有限数目的单元，可以形成一个网格结构，所有聚类都在网格上进行。基本思想就是将每个属性的可能值分割成许多相邻的区间，并创建网格单元的集合。每个对象落入一个网格单元，网格单元对应的属性空间包含该对象的值</p><ul><li>优点：处理速度快，其处理时间独立于数据对象数，而仅依赖于量化空间中的每一维的单元数</li><li>缺点：只能发现边界是水平或垂直的簇，而不能检测到斜边界。另外，在处理高维数据时，网格单元的数目会随着属性维数的增长而成指数级增长</li></ul><h3 id="基于模型"><a href="#基于模型" class="headerlink" title="基于模型"></a>基于模型</h3><p>试图优化给定的数据和某些数学模型之间的适应性的。该方法给每一个簇假定了一个模型，然后寻找数据对给定模型的最佳拟合。假定的模型可能是代表数据对象在空间分布情况的密度函数或者其他函数。这种方法的基本原理就是假定目标数据集是由一系列潜在的概率分布所决定的。簇的数目是基于标准的统计数字自动决定的，噪声或孤立点也是通过统计数字来分析的。基于模型的聚类方法试图优化给定的数据和某些数据模型之间的适应性</p>]]></content>
    
    
    <categories>
      
      <category>计算机</category>
      
      <category>机器学习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>My New Post</title>
    <link href="/2022/09/15/My-New-Post/"/>
    <url>/2022/09/15/My-New-Post/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>Diary</category>
      
      <category>Life</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2022/09/14/hello-world/"/>
    <url>/2022/09/14/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
