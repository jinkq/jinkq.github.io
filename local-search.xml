<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>BS模型</title>
    <link href="/2023/05/08/%E9%87%91%E8%9E%8D/%E8%A1%8D%E7%94%9F%E5%93%81/%E6%9C%9F%E6%9D%83/BS%E6%A8%A1%E5%9E%8B/"/>
    <url>/2023/05/08/%E9%87%91%E8%9E%8D/%E8%A1%8D%E7%94%9F%E5%93%81/%E6%9C%9F%E6%9D%83/BS%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="BS模型"><a href="#BS模型" class="headerlink" title="BS模型"></a>BS模型</h1><p>布朗运动是一种最简单的<strong>连续随机过程</strong>，它是描述证券价格随机性的基本模型。而对于期权或其他衍生品这些金融工具，它们的价格是相关证券资产价格的函数。因此可以说<strong>证券价格是一个随机过程，而衍生品价格是该随机过程的函数。伊藤引理提供了对随机过程的函数做微分的框架；这对于衍生品的定价意义非凡</strong>（在此之前，人们是不知道如何对随机过程的函数做微分的）。<strong>通过伊藤引理，可以写出金融衍生品价格的随机微分方程，通过对其求解便可以得到衍生品价格的模型。</strong>BS 公式就是一个最简单的例子。（BS模型是一个偏微分方程，而BS公式是一个解析形式的表达式）</p><h2 id="布朗运动"><a href="#布朗运动" class="headerlink" title="布朗运动"></a>布朗运动</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>${W_t}$满足：</p><ol><li>$W_0&#x3D;0$</li><li>For all $0&lt;s&lt;t$，$W_t-W_s\sim N(0,t-s)$</li><li>对不重叠的区间$[s_i,t_i]$，$W_{t_i}-W_{s_i}$独立</li><li>$W_t$在$t$上连续</li></ol><p>布朗运动是<strong>马尔可夫过程</strong>。$t&#x3D;0$时位置为0，在任意有限时间区间$\varDelta t$内，布朗运动的变化服从$N(0,\varDelta t))$</p><h3 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h3><ol><li><p>轨迹频繁穿越时间轴$t$</p></li><li><p>$\forall t$，$W_t$不会偏离正负$\sigma$太远</p></li><li><p>令$M_t$为$0$到$t$内$W_t$所能达到的最大值$M_t&#x3D;\underset{0\leqslant s\leqslant t}{\max}W_s$，则<br>$$<br>P(M_t\geqslant a)&#x3D;2P(W_t\geqslant a)&#x3D;2-2\phi(\frac{a}{\sqrt{t}})\<br>$$<br>$$<br>E[M_t]&#x3D;\sqrt{\frac{2t}{\pi}}<br>$$</p></li><li><p>布朗运动连续，但处处不可微分</p></li><li><p>$W_t$是<strong>均值为0</strong>，<strong>方差为$\sigma^2 t$<strong>的</strong>正态</strong>随机变量</p></li></ol><h3 id="数学性质（标准布朗运动，-sigma-2-x3D-1-）"><a href="#数学性质（标准布朗运动，-sigma-2-x3D-1-）" class="headerlink" title="数学性质（标准布朗运动，$\sigma^2&#x3D;1$）"></a>数学性质（标准布朗运动，$\sigma^2&#x3D;1$）</h3><ol><li><p>pdf：$f_{W_t}(x)&#x3D;\frac{1}{\sqrt{2\pi t}}e^{\frac{x^2}{2t}}$</p></li><li><p>期望：$E[W_t]&#x3D;0$</p></li><li><p>方差：$Var(W_t)&#x3D;t$</p></li><li><p>$W_t&#x3D;W_t-W_0\sim N(0,t)$</p></li><li><p>协方差：$Cov(W_s,W_t)&#x3D;s\quad(s\leqslant t)$</p><p>证：<br>$$<br>\begin{aligned}<br>Cov(W_s,W_t)&amp;&#x3D;E[W_sW_t]\<br>&amp;&#x3D;E[W_s((W_t-W_s)+W_s)]\<br>&amp;&#x3D;E[W_s(W_t-W_s)]+E[W_s^2]\<br>&amp;&#x3D;E[W_s]E[W_t-W_s]+E[W_s^2]（W_s-W_0和W_t-W_s独立）\<br>&amp;&#x3D;E[W_s^2]\<br>&amp;&#x3D;s<br>\end{aligned}<br>$$</p></li><li><p>相关系数：$Corr(W_s,W_t)&#x3D;\frac{Cov(W_s,W_t)}{\sigma_{W_s}\sigma_{W_t}}&#x3D;\frac{s}{\sqrt{st}}&#x3D;\sqrt{\frac{s}{t}}$</p></li></ol><h3 id="二次变分"><a href="#二次变分" class="headerlink" title="二次变分"></a>二次变分</h3><p>古典微积分：<br>$$<br>\begin{aligned}<br>\sum_i[f(t_{i+1})-f(t_i)]^2&amp;\leqslant \sum_i(t_{i+1}-t_i)^2f’(s_i)^2\<br>&amp;\leqslant \underset{s\in [0,T]}{\max}f’(s)^2\cdot\sum_i(t_{i+1}-t_i)^2\<br>&amp;\leqslant \underset{s\in [0,T]}{\max}f’(s)^2\cdot \underset{i}{max}{t_{i+1}-t_i}\cdot T<br>\end{aligned}<br>$$<br>当$[0,T]$划分得越来越细，$\underset{i}{max}{t_{i+1}-t_i}\rightarrow0$，二次变分&#x3D;0</p><p>布朗运动：</p><p>$W_t$的二次变分为$T$<br>$$<br>\underset{\underset{i}{max}{t_{i+1}-t_i}\rightarrow0}{lim}\sum_i(W_{t_{i+1}}-W_{t_i})^2&#x3D;T<br>$$</p><ul><li><strong>$(dW)^2&#x3D;dt$</strong></li></ul><h3 id="带漂移的布朗运动"><a href="#带漂移的布朗运动" class="headerlink" title="带漂移的布朗运动"></a>带漂移的布朗运动</h3><p>$$<br>X_t&#x3D;\mu t+\sigma W_t\<br>$$<br>$$<br>X_t&#x3D;\int_0^t \mu ds+\int_0^t \sigma dW\<br>$$<br>$$<br>E[X_t]&#x3D;\int_0^t \mu ds&#x3D;\mu t（dW均值为0，方差为1）\<br>$$<br>$$<br>Var(X_t)&#x3D;\int_0^t \sigma^2 dW&#x3D;\sigma^2t<br>$$</p><h3 id="几何布朗运动"><a href="#几何布朗运动" class="headerlink" title="几何布朗运动"></a>几何布朗运动</h3><p>几何布朗运动：<br>$$<br>X_t&#x3D;e^{Y_t}<br>$$<br>其中$Y_t$是漂移布朗运动。</p><p>股价$S_t$服从几何布朗运动：<br>$$<br>\frac{dS_t}{S_t}&#x3D;\mu dt+\sigma dW_t\<br>$$<br>$$<br>dS_t&#x3D;\mu S_tdt+\sigma S_tdW_t<br>$$</p><p>用几何布朗运动描述股价的原因：</p><ol><li>正态分布（连续复利收益率）</li><li>马尔可夫过程</li><li>时间上处处不可微，二次变分不为0（收益率存在尖点）</li><li>GBM（至少在一定程度上）符合人们对市场的观察。例如，直观的说，股票的价格看起来很像随机游走，再例如，股票价格不会为负，这样起码GBM比普通的布朗运动合适，因为后者是可以为负的。对简单收益率做测试做测试，发现基本服从正态分布。收益率是正态的，股价就是GBM模型</li></ol><h2 id="伊藤引理"><a href="#伊藤引理" class="headerlink" title="伊藤引理"></a>伊藤引理</h2><h3 id="泰勒展开"><a href="#泰勒展开" class="headerlink" title="泰勒展开"></a>泰勒展开</h3><p>$f(W_t)$是连续平滑函数<br>$$<br>\varDelta f&#x3D;f(x+\varDelta x)-f(x)&#x3D;f’(x)\varDelta x+\frac{1}{2}f’’(x)(\varDelta x)^2+…<br>$$<br>一般函数：<br>$$<br>df&#x3D;f’(x)dx（其它是一阶无穷小）<br>$$<br>布朗运动：<br>$$<br>\begin{aligned}<br>\varDelta f&amp;&#x3D;f(W_t+\varDelta W_t)-f(W_t)\<br>&amp;&#x3D;f’(W_t)\varDelta W_t+\frac{1}{2}f’’(W_t)(\varDelta W_t)^2+…<br>\end{aligned}<br>$$</p><p>$$<br>df(W_t)&#x3D;f’(W_t)\varDelta W_t+\frac{1}{2}f’’(W_t)dt<br>$$</p><p>上式即为<strong>伊藤引理</strong></p><h3 id="偏导数"><a href="#偏导数" class="headerlink" title="偏导数"></a>偏导数</h3><p>古典微积分：$f(x,t)$，$x$是标量<br>$$<br>df&#x3D;\frac{\partial f}{\partial t}dt+\frac{\partial f}{\partial x}dx<br>$$<br>布朗运动：$x&#x3D;W_t$<br>$$<br>\begin{aligned}<br>df(W_t,t)&amp;&#x3D;\frac{\partial f}{\partial t}dt+\frac{\partial f}{\partial W_t}dW_t+\frac{1}{2}\frac{\partial^2f}{\partial W_t^2}(dW_t)^2\<br>&amp;&#x3D;(\frac{\partial f}{\partial t}+\frac{1}{2}\frac{\partial^2f}{\partial W_t^2})dt+\frac{\partial f}{\partial W_t}dW_t<br>\end{aligned}<br>$$</p><h3 id="伊藤漂移扩散过程（伊藤过程）"><a href="#伊藤漂移扩散过程（伊藤过程）" class="headerlink" title="伊藤漂移扩散过程（伊藤过程）"></a>伊藤漂移扩散过程（伊藤过程）</h3><p>$$<br>dX_t&#x3D;a(X_t,t)dt+b(X_t,t)dW_t<br>$$</p><p>令$f(X_t,t)$为$X_t$的二阶连续可导函数，由伊藤引理<br>$$<br>df&#x3D;\frac{\partial f}{\partial t}dt+\frac{\partial f}{\partial X_t}dX_t+\frac{1}{2}\frac{\partial^2 f}{\partial X_t^2}(dX_t)^2<br>$$<br>将$dX_t&#x3D;a(X_t,t)dt+b(X_t,t)dW_t$代入得：<br>$$<br>df(X_t,t)&#x3D;(\frac{\partial f}{\partial t}+\frac{\partial f}{\partial X_t}a+\frac{1}{2}\frac{\partial^2 f}{\partial X_t^2}b^2)dt+\frac{\partial f}{\partial X_t}bdW_t<br>$$</p><h3 id="几何布朗运动求解"><a href="#几何布朗运动求解" class="headerlink" title="几何布朗运动求解"></a>几何布朗运动求解</h3><p>$$<br>dS_t&#x3D;\mu S_t dt+\sigma S_t dW<br>$$</p><p>其中$\mu$是年化收益率，$\sigma$是年化波动率<br>$$<br>a&#x3D;\mu S_t,b&#x3D;\sigma S_t,f&#x3D;lnS_t\<br>$$<br>$$<br>\frac{\partial f}{\partial t}&#x3D;0,\frac{\partial f}{\partial S_t}&#x3D;\frac{1}{S_t},\frac{\partial^2 f}{\partial S_t^2}&#x3D;-\frac{1}{S_t^2}<br>$$<br>代入得：<br>$$<br>\begin{aligned}<br>d(lnS_t)&amp;&#x3D;(\frac{1}{S_t}\mu S_t-\frac{1}{2}\frac{1}{S_t^2}\sigma^2 S_t^2)dt+\frac{1}{S_t}\sigma S_tdW_t\<br>&amp;&#x3D;(\mu -\frac{1}{2}\sigma^2)dt+\sigma dW_t<br>\end{aligned}<br>$$</p><p>则$lnS_t$是带漂移项的布朗运动</p><p>对上式两边积分，得：<br>$$<br>lnS_t&#x3D;lnS_0+(\mu -\frac{1}{2}\sigma^2)t+\sigma W_t\<br>$$<br>$$<br>lnS_t\sim N(lnS_0+(\mu -\frac{1}{2}\sigma^2)t,\sigma^2t)\<br>$$</p><p>$$<br>S_t&#x3D;S_0e^{(\mu -\frac{1}{2}\sigma^2)t+\sigma W_t}<br>$$</p><h2 id="BS模型-1"><a href="#BS模型-1" class="headerlink" title="BS模型"></a>BS模型</h2><h3 id="模型假设"><a href="#模型假设" class="headerlink" title="模型假设"></a>模型假设</h3><ol><li>欧式期权，期权不能在到期日（expiration date）之前行使</li><li>期权的标的资产$S_t$的价格服从几何布朗运动，因此标的资产收益率必须服从对数正态分布。The price of the underlying asset follows geometric Brownian motion, so the return of underlying asset follows the lognormal distribution.</li><li>可以做空，证券可被分割</li><li>市场没有摩擦，没有税收和交易费用，没有卖空限制。There is no friction in the market, no tax and transaction costs, and no short position restrictions.</li><li>期权期限内，标的资产不支付股息</li><li>期权期限内，标的资产年波动率$\sigma$已知且保持不变</li><li>市场不存在无风险套利机会</li><li>标的资产交易是连续的</li><li>无风险利率$r$为常数且已知</li></ol><h3 id="BS方程"><a href="#BS方程" class="headerlink" title="BS方程"></a>BS方程</h3><p>$C$为欧式看涨期权价格，$C(S,t)$（其中$dS&#x3D;\mu S dt+\sigma SdW$），对$C$运用伊藤引理得：<br>$$<br>dC&#x3D;(\frac{\partial C}{\partial t}+\mu S\frac{\partial C}{\partial S}+\frac{1}{2}\sigma^2S^2\frac{\partial^2 C}{\partial S^2})dt+\frac{\partial C}{\partial S}\sigma SdW<br>$$<br>构建组合$P$：</p><ul><li>-1期权</li><li>+$\frac{\partial C}{\partial S}$标的股票</li></ul><p>则$P&#x3D;-C+\frac{\partial C}{\partial S}S$（Delta对冲）<br>$$<br>\begin{aligned}<br>\varDelta P&amp;&#x3D;-\varDelta C+\frac{\partial C}{\partial S}\varDelta S\<br>&amp;&#x3D;-(\frac{\partial C}{\partial t}+\mu S\frac{\partial C}{\partial S}+\frac{1}{2}\sigma^2S^2\frac{\partial^2 C}{\partial S^2})\varDelta t-\frac{\partial C}{\partial S}\sigma S\varDelta W+\frac{\partial C}{\partial S}(\mu S\varDelta t+\sigma S\varDelta W)\<br>&amp;&#x3D;-(\frac{\partial C}{\partial t}+\frac{1}{2}\sigma^2 S^2\frac{\partial^2 C}{\partial S^2})\varDelta t<br>\end{aligned}<br>$$<br>$\varDelta P$不含$\varDelta W$，是无风险组合。</p><p>在无套利市场中，$\varDelta P&#x3D;Pr\varDelta t$，代入得：<br>$$<br>(-C+\frac{\partial C}{\partial S}S)r\varDelta t&#x3D;-(\frac{\partial C}{\partial t}+\frac{1}{2}\sigma^2S^2\frac{\partial^2 C}{\partial S^2})\varDelta t<br>$$<br>即<br>$$<br>\frac{\partial C}{\partial t}+rS\frac{\partial C}{\partial S}+\frac{1}{2}\sigma^2S^2\frac{\partial^2 C}{\partial S^2}&#x3D;rC<br>$$<br>此为<strong>BS方程</strong></p><h3 id="风险中性定价"><a href="#风险中性定价" class="headerlink" title="风险中性定价"></a>风险中性定价</h3><p>风险中性世界里$\mu&#x3D;r$，切换到风险厌恶世界后，$\mu$的期望和折现率都变化，变化相互抵消。将概率测度$P$转换到$Q$定价，不用考虑风险偏好</p><h4 id="为什么BS方程中一阶项前面是无风险利率-r-，而不是风险利率（或者历史回报率）-mu-？"><a href="#为什么BS方程中一阶项前面是无风险利率-r-，而不是风险利率（或者历史回报率）-mu-？" class="headerlink" title="为什么BS方程中一阶项前面是无风险利率$r$，而不是风险利率（或者历史回报率）$\mu$？"></a>为什么BS方程中一阶项前面是无风险利率$r$，而不是风险利率（或者历史回报率）$\mu$？</h4><ul><li><p>风险中性定价，$\mu$经过测度变换可以得到$r$</p></li><li><p>融资成本所对应的回报率是$r$，而不是$\mu$</p></li></ul><h2 id="BS期权定价公式"><a href="#BS期权定价公式" class="headerlink" title="BS期权定价公式"></a>BS期权定价公式</h2><h4 id="风险中性下推导B-S公式"><a href="#风险中性下推导B-S公式" class="headerlink" title="风险中性下推导B-S公式"></a>风险中性下推导B-S公式</h4><h4 id="BS公式和BS模型的区别"><a href="#BS公式和BS模型的区别" class="headerlink" title="BS公式和BS模型的区别"></a>BS公式和BS模型的区别</h4><p>BS公式和BS模型的公式基本相同，但是BS模型比BS公式更为复杂，包括了更多的变量和参数。</p><p>下面是BS公式和BS模型的公式：</p><p>BS公式：</p><p>$$C&#x3D;S_tN(d_1)-Ke^{-r(T-t)}N(d_2)$$</p><p>其中，$C$是欧式看涨期权的价格，$S_t$是标的资产的当前价格，$K$是期权的行权价格，$r$是无风险利率，$T-t$是期权到期时间，$N$是标准正态分布的累积分布函数，$d_1$和$d_2$分别为：</p><p>$$d_1&#x3D;\frac{\ln\left(\frac{S_t}{K}\right)+(r+\frac{1}{2}\sigma^2)(T-t)}{\sigma\sqrt{T-t}}$$</p><p>$$d_2&#x3D;d_1-\sigma\sqrt{T-t}$$</p><p>BS模型：</p><p>$$C&#x3D;S_tN(d_1)-Ke^{-r(T-t)}N(d_2)$$</p><p>其中，$C$、$S_t$、$K$、$r$、$T-t$、$N$、$d_1$和$d_2$的含义与BS公式相同，但是BS模型还考虑了标的资产的随机漂移和股息率等因素，因此$d_1$和$d_2$的计算方式略有不同：</p><p>$$d_1&#x3D;\frac{\ln\left(\frac{S_t}{K}\right)+(r+\frac{1}{2}\sigma^2)(T-t)}{\sigma\sqrt{T-t}}+ \frac{b}{\sigma}\sqrt{T-t}$$</p><p>$$d_2&#x3D;d_1-\sigma\sqrt{T-t}$$</p><p>其中，$b$是标的资产的随机漂移率，也称为股息率。可以看出，BS模型在计算$d_1$时多了一个$\frac{b}{\sigma}\sqrt{T-t}$的项。</p><h4 id="Brownian-Motion"><a href="#Brownian-Motion" class="headerlink" title="Brownian Motion"></a>Brownian Motion</h4><ul><li>Ito Lemma</li><li>判断组合是不是marting’</li></ul><h4 id="BS-Formula公式和基本假设"><a href="#BS-Formula公式和基本假设" class="headerlink" title="BS Formula公式和基本假设"></a>BS Formula公式和基本假设</h4><h4 id="BS-Merton-Formula推导"><a href="#BS-Merton-Formula推导" class="headerlink" title="BS Merton Formula推导"></a>BS Merton Formula推导</h4><p>两种推导，一种是replication，一种是option+stock组合成risk-free portfolio（后者更被preferred）</p>]]></content>
    
    
    <categories>
      
      <category>金融</category>
      
      <category>衍生品</category>
      
      <category>期权</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>互换</title>
    <link href="/2023/05/08/%E9%87%91%E8%9E%8D/%E8%A1%8D%E7%94%9F%E5%93%81/%E4%BA%92%E6%8D%A2/"/>
    <url>/2023/05/08/%E9%87%91%E8%9E%8D/%E8%A1%8D%E7%94%9F%E5%93%81/%E4%BA%92%E6%8D%A2/</url>
    
    <content type="html"><![CDATA[<h1 id="互换">互换</h1><h3 id="利率互换">利率互换</h3><h3id="如果买了国债怎么用swap来hedge">如果买了国债，怎么用swap来hedge</h3>]]></content>
    
    
    <categories>
      
      <category>金融</category>
      
      <category>衍生品</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>期权</title>
    <link href="/2023/05/08/%E9%87%91%E8%9E%8D/%E8%A1%8D%E7%94%9F%E5%93%81/%E6%9C%9F%E6%9D%83/%E6%9C%9F%E6%9D%83/"/>
    <url>/2023/05/08/%E9%87%91%E8%9E%8D/%E8%A1%8D%E7%94%9F%E5%93%81/%E6%9C%9F%E6%9D%83/%E6%9C%9F%E6%9D%83/</url>
    
    <content type="html"><![CDATA[<h1 id="期权">期权</h1><h3 id="vanilla-option">Vanilla Option</h3><p>最基本的、标准化的欧式期权</p><h3 id="影响期权价格的因素">影响期权价格的因素</h3><ul><li>price of underlyingasset同一条件下，看涨期权价格与标的资产价格呈正比A is proportional tob，看跌期权与之呈反比A is inversely proportional to b。</li><li>strike price执行价格的高低对期权的影响与标的资产相反。</li><li>Price volatility of underlyingasset同一条件下，波动率越高的期权价格越高。</li><li>Remaining time before duedate同一条件下，有效期越长的期权价格越高。</li><li>risk-free rate期权价格与无风险利率呈反比</li><li>【optional】Income of underlying asset in holdingperiod由于标的资产的分红付息等将降低标的资产的价格，而执行价格并未因此进行相应的调整，因此在期权有效期内，标的资产产生的红利将使看涨期权价格下降，而使看跌期权价格上升（Sincethe dividend of the underlying asset will decrease the price of theunderlying asset, but the strike price has not been adjustedaccordingly, the dividend generated by the underlying asset willdecrease the price of call options and increase the price of put optionsduring the effective period of the option）</li></ul><h3id="假设标的资产近期将会出现大波动市场有一个call和一个put可供选择请构建出一个你觉得适合后市的期权套利组合并画出对应的盈亏图">假设标的资产近期将会出现大波动，市场有一个CALL和一个PUT可供选择，请构建出一个你觉得适合后市的期权套利组合，并画出对应的盈亏图</h3><h3 id="波动率">波动率</h3><h4 id="波动率微笑">波动率微笑</h4><p>波动率微笑（volatilitysmile）是指在金融市场上，不同执行价格的期权隐含波动率呈现出不同的形态，通常表现为执行价格远离标的资产现价的期权隐含波动率较高，而执行价格接近标的资产现价的期权隐含波动率较低，呈现出一个微笑的形态。</p><p>波动率微笑是由市场上期权买卖双方对未来市场波动性的不同预期所导致的。在市场中，投资者通常会根据其对未来市场波动性的预期来决定是否购买期权。当投资者预期市场波动性较高时，他们更倾向于购买执行价格远离标的资产现价的期权，以获得更高的潜在收益。因此，这些期权的价格会被市场推高，而其隐含波动率也会较高。相反，当投资者预期市场波动性较低时，他们更倾向于购买执行价格接近标的资产现价的期权，以保证稳定的收益，这些期权的价格则会被市场推低，而其隐含波动率也会较低。</p><p>For options with the same maturity date and underlying assets butdifferent strike prices, the farther the strike price deviates from thespot price of the underlying assets, the greater the implied volatility.In the empirical study, the implied volatility calculated by thetraditional BS option pricing model presents a phenomenon called"volatility smile"</p><h4 id="volatility-skew">Volatility skew</h4><p>波动率偏斜（Volatilityskew）指的是相同到期日、相同标的资产、相同执行价格的期权，但不同行权价的隐含波动率不同的现象。</p><p>通常情况下，波动率偏斜表现为，执行价格较低的看涨期权的隐含波动率高于执行价格较高的看涨期权的隐含波动率，而执行价格较高的看跌期权的隐含波动率高于执行价格较低的看跌期权的隐含波动率。这是由于投资者对市场上涨的担忧要大于对市场下跌的担忧，因此对于执行价格较低的看涨期权的需求较高，其价格也会相应地被推高，进而导致隐含波动率的上升；相反，对于执行价格较高的看涨期权的需求较低，其价格也会相应地被推低，进而导致隐含波动率的下降。</p><p>波动率偏斜在金融市场中比较常见，特别是在股票市场和外汇市场中。</p><h3 id="bs模型">BS模型</h3><p>布朗运动是一种最简单的连续随机过程，它是描述证券价格随机性的基本模型。而对于期权或其他衍生品这些金融工具，它们的价格是相关证券资产价格的函数。因此可以说<strong>证券价格是一个随机过程，而衍生品价格是该随机过程的函数。伊藤引理提供了对随机过程的函数做微分的框架；这对于衍生品的定价意义非凡</strong>（在此之前，人们是不知道如何对随机过程的函数做微分的）。<strong>通过伊藤引理，可以写出金融衍生品价格的随机微分方程，通过对其求解便可以得到衍生品价格的模型。</strong>BS公式就是一个最简单的例子。（BS 模型是一个偏微分方程，而 BS公式是一个解析形式的表达式）</p><h4 id="bs模型假设">BS模型假设</h4><p>Black-Scholes 期权的价格模型是建立在严格的假设基础上的，包括以下几点：</p><ol type="1"><li>期权的标的资产的价格服从几何布朗运动，因此标的资产收益率必须服从对数正态分布。The price of the underlying asset followsgeometric Brownian motion, so the return of underlying asset follows thelognormal distribution.</li><li>市场没有摩擦，没有税收和交易成本，没有卖空限制。There is no frictionin the market, no tax and transaction costs, and no short positionrestrictions.</li><li>无风险利率不变。</li><li>期权不能在到期日之前行使，必须是欧式期权expiration date</li></ol><h4id="为什么black-scholes方程注意不是bs模型中一阶项前面是无风险利率r而不是风险利率或者历史回报率u">为什么blackscholes方程（注意不是bs模型）中一阶项前面是无风险利率r，而不是风险利率（或者历史回报率）u？</h4><p>风险中性定价，u经过测度变换可以得到r</p><p>融资成本所对应的回报率是r，而不是u</p><h4 id="布朗运动">布朗运动</h4><p><strong>布朗运动是一个连续随机过程。一个随机过程是定义在时域或者空间域上的依次发生的一系列随机变量的集合。</strong>以时域为例，如果这些随机变量在整个实数时域上都有定义，那么这个随机过程为连续随机过程；反之，如果这些随机变量仅仅在时域上一些离散的点有定义，那么该随机过程为离散随机过程</p><h5id="为什么用几何布朗运动描述股票价格">为什么用几何布朗运动描述股票价格</h5><p>其实很简单，GBM（至少在一定程度上）符合人们对市场的观察。例如，直观的说，股票的价格看起来很像随机游走，再例如，股票价格不会为负，这样起码GBM比普通的布朗运动合适，因为后者是可以为负的。</p><p>再稍微复杂一点，对收益率做测试（ S(t)/S(t-1) -1）做测试，发现，哎居然还基本是个正态分布。收益率是正态的，股价就是GBM模型</p><p>总之，就是大家做了很多统计测试，发现假设成GBM还能很好的逼近真实数值，比较接近事实</p><h4 id="风险中性下推导b-s公式">风险中性下推导B-S公式</h4><h4 id="bs公式和bs模型的区别">BS公式和BS模型的区别</h4><p>BS公式和BS模型的公式基本相同，但是BS模型比BS公式更为复杂，包括了更多的变量和参数。</p><p>下面是BS公式和BS模型的公式：</p><p>BS公式：</p><p><spanclass="math display">\[C=S_tN(d_1)-Ke^{-r(T-t)}N(d_2)\]</span></p><p>其中，<spanclass="math inline">\(C\)</span>是欧式看涨期权的价格，<spanclass="math inline">\(S_t\)</span>是标的资产的当前价格，<spanclass="math inline">\(K\)</span>是期权的行权价格，<spanclass="math inline">\(r\)</span>是无风险利率，<spanclass="math inline">\(T-t\)</span>是期权到期时间，<spanclass="math inline">\(N\)</span>是标准正态分布的累积分布函数，<spanclass="math inline">\(d_1\)</span>和<spanclass="math inline">\(d_2\)</span>分别为：</p><p><spanclass="math display">\[d_1=\frac{\ln\left(\frac{S_t}{K}\right)+(r+\frac{1}{2}\sigma^2)(T-t)}{\sigma\sqrt{T-t}}\]</span></p><p><span class="math display">\[d_2=d_1-\sigma\sqrt{T-t}\]</span></p><p>BS模型：</p><p><spanclass="math display">\[C=S_tN(d_1)-Ke^{-r(T-t)}N(d_2)\]</span></p><p>其中，<span class="math inline">\(C\)</span>、<spanclass="math inline">\(S_t\)</span>、<spanclass="math inline">\(K\)</span>、<spanclass="math inline">\(r\)</span>、<spanclass="math inline">\(T-t\)</span>、<spanclass="math inline">\(N\)</span>、<spanclass="math inline">\(d_1\)</span>和<spanclass="math inline">\(d_2\)</span>的含义与BS公式相同，但是BS模型还考虑了标的资产的随机漂移和股息率等因素，因此<spanclass="math inline">\(d_1\)</span>和<spanclass="math inline">\(d_2\)</span>的计算方式略有不同：</p><p><spanclass="math display">\[d_1=\frac{\ln\left(\frac{S_t}{K}\right)+(r+\frac{1}{2}\sigma^2)(T-t)}{\sigma\sqrt{T-t}}+\frac{b}{\sigma}\sqrt{T-t}\]</span></p><p><span class="math display">\[d_2=d_1-\sigma\sqrt{T-t}\]</span></p><p>其中，<spanclass="math inline">\(b\)</span>是标的资产的随机漂移率，也称为股息率。可以看出，BS模型在计算<spanclass="math inline">\(d_1\)</span>时多了一个<spanclass="math inline">\(\frac{b}{\sigma}\sqrt{T-t}\)</span>的项。</p><h4 id="optimal-hedge-ratio">optimal hedge ratio</h4><p>最优对冲比率（Optimal hedgeratio）是指在进行期货或期权交易时，用来确定期权头寸与期货头寸之间最佳的数量关系，以达到最小化投资组合风险的比率。</p><p>最优对冲比率是在现实市场情况下计算得出的，通常是通过回归分析等统计方法来确定。该比率表明了在给定期权头寸的情况下，需要持有多少头寸的期货合约才能实现最小化风险的效果。一般来说，最优对冲比率取决于期权的到期日、执行价格、标的资产价格波动率以及市场的流动性等因素。</p><p>最优对冲比率的计算对于期权和期货交易者来说都非常重要。对于期权交易者来说，最优对冲比率可以帮助他们在持有期权的同时，通过期货合约对价格波动的风险进行对冲，从而减小投资组合的风险；对于期货交易者来说，最优对冲比率可以帮助他们确定多空头寸之间的最佳数量关系，以达到最小化风险的效果</p><h5 id="期货最优对冲比率的计算">期货最优对冲比率的计算</h5><p>期货最优对冲比率是指在持有期货合约的同时，持有多少的标的资产才能实现最小化风险的效果。最优对冲比率的计算需要使用线性回归方法，即将标的资产价格的变动作为自变量，将期货合约价格的变动作为因变量，通过回归分析来确定最优对冲比率。</p><p>具体而言，计算过程如下：</p><ol type="1"><li>收集一段时间内标的资产和期货合约的价格数据。</li><li>将标的资产价格的变动作为自变量，将期货合约价格的变动作为因变量，进行线性回归分析。</li><li>计算回归系数，即标的资产价格变动对期货合约价格变动的敏感度，得到最优对冲比率。</li></ol><p>例如，假设投资者持有原油期货合约，并且持有1000桶原油的现货头寸。通过分析历史数据，计算出最优对冲比率为0.8。则投资者需要持有800桶原油的现货头寸，才能实现最小化风险的效果。</p><h5 id="期权最优对冲比率的计算">期权最优对冲比率的计算</h5><p>期权最优对冲比率是指在持有期权的同时，持有多少的标的资产才能实现最小化风险的效果。最优对冲比率的计算需要使用风险中性定价方法，即将标的资产价格的变动作为风险因素，通过期权的定价公式来计算最优对冲比率。</p><p>具体而言，计算过程如下：</p><ol type="1"><li>确定期权的到期日、执行价格和类型，以及标的资产的价格和波动率等参数。</li><li>根据期权的定价公式，计算出期权的理论价格。</li><li>通过偏导数公式计算期权价格对标的资产价格的敏感度，即Delta值。</li><li>计算最优对冲比率，即期权持有人需要持有多少的标的资产才能实现最小化风险的效果，即最优对冲比率等于Delta值除以标的资产价格的变动量。</li></ol><p>例如，假设投资者持有某只股票的看涨期权，执行价格为100元，到期日为三个月后，标的资产价格为110元，波动率为20%。通过期权定价公式计算出期权价格为6元。偏导数公式计算出Delta值为0.65。则最优对冲比率为0.65/10=0.065，即投资者需要持有15.38股标的资产，才能实现最小化风险的效果。</p><h4 id="martingale基础知识">Martingale基础知识</h4><p>在概率论和数学金融中，Martingale是一种随机过程，通常用于描述一个随机变量序列的演变。Martingale序列的一个重要特点是其在未来的预期值等于当前的值。这个性质被称为“无偏增长”或者“无记忆性”。</p><p>换句话说，如果一系列随机变量X1, X2, ...,Xt是Martingale序列，那么在给定任何t时刻，我们可以预期Xt的未来值与当前的值相等。这个特性可以表示为：</p><p>E(Xt+1 | X1, X2, ..., Xt) = Xt</p><p>其中，E表示期望值。这个等式意味着，假设我们已经知道了当前时刻的随机变量值，那么下一时刻的期望值就等于当前的值。</p><p>Martingale序列通常用于研究随机过程的特性和性质，例如在金融领域，Martingale序列可以被用来描述股票价格的随机变化。如果一支股票价格的随机漂移是一个Martingale序列，那么其价格的期望值在任何时刻都应该等于当前价格，即股票价格不存在趋势或者偏差，是随机游走的。</p><p>在数学金融中，Martingale序列也可以用于衡量金融衍生品的价格。如果一个金融衍生品的价格符合Martingale序列的性质，那么在风险中性世界中，该衍生品的价格应该等于其期望收益的贴现值，即其价格应该等于其未来收益的期望值。这个性质被称为“风险中性定价法”，是金融衍生品定价的基本原理之一。</p><h3 id="期权价格-期权价值">期权价格 &amp; 期权价值</h3><p>期权价格和期权价值的关系是：期权价格等于期权内在价值和时间价值的总和，也就是期权价值。如果期权内在价值为负，那么期权的价格就等于时间价值。如果期权内在价值为正，那么期权的价格就等于内在价值加上时间价值。</p><h3 id="各种option-strategy">各种option strategy</h3><h4 id="brownian-motion">Brownian Motion</h4><ul><li>Ito Lemma</li><li>判断组合是不是marting'</li></ul><h4 id="bs-formula公式和基本假设">BS Formula公式和基本假设</h4><h4 id="bs-merton-formula推导">BS Merton Formula推导</h4><p>两种推导，一种是replication，一种是option+stock组合成risk-freeportfolio（后者更被preferred）</p><h3 id="put-call-parity">Put-Call Parity</h3><p>可以用来解释call由put，time value和intrisic value构成</p><h2 id="binomial-model">Binomial Model</h2><p><span class="math inline">\(c=\pi_uc_u+\pi_dc_d\)</span></p><h3 id="realized-vol和im-vol">Realized vol和im vol</h3><ul><li>各自的定义</li><li>两者的区别</li></ul><p>实现波动率（Realized Volatility）和隐含波动率（ImpliedVolatility）是衡量金融市场波动性的两个重要指标，二者的区别如下：</p><ol type="1"><li>实现波动率是根据历史数据计算的波动率，是过去某一段时间内实际价格的波动程度的测度。而隐含波动率是根据市场上对期权价格的交易计算得到的，是市场对未来波动率的预期的测度。</li><li>实现波动率是基于历史数据计算的，其计算方法可以有多种，比如对数收益率、价格变动率等。而隐含波动率是根据期权的市场价格和其他参数，使用期权定价模型计算得到的。</li><li>实现波动率是已知的，因为历史价格已经发生了，而隐含波动率是未知的，因为它是根据市场价格反推出来的。</li><li>实现波动率反映了市场实际的波动程度，而隐含波动率反映了市场对未来波动率的预期。实现波动率可以帮助投资者评估风险，而隐含波动率可以帮助投资者评估期权价格的合理性以及市场对未来波动率的看法。</li><li>实现波动率通常用于构建波动率交易策略，而隐含波动率通常用于期权定价和风险管理。</li></ol><h3 id="im-vol-surface特点">im vol surface特点</h3><h3 id="local-vol模型">Local Vol模型</h3><ul><li>由Dupire拟合</li><li>Pros &amp; cons，还问过和原来BS Model的区别</li></ul><h3 id="heston-model">Heston Model</h3><h3 id="dynamic-hedge-static-hedge">Dynamic hedge &amp; statichedge</h3><h3 id="奇异期权">奇异期权</h3><h4 id="亚式期权的定价">亚式期权的定价</h4><ul><li>Rainbow<ul><li>Corr &amp; option price的关系</li><li>Snowball</li><li>Accumulator</li></ul></li></ul><h3 id="greek">Greek</h3><table style="width:100%;"><thead><tr class="header"><th>Greek</th><th>Call</th><th>Put</th></tr></thead><tbody><tr class="odd"><td>Delta(<span class="math inline">\(\frac{\partial}{\partialS_t}\)</span>)</td><td><span class="math inline">\(\phi(d_1)\)</span></td><td><span class="math inline">\(-\phi(-d_1)\)</span></td></tr><tr class="even"><td>Gamma(<span class="math inline">\(\frac{\partial^2}{\partialS_t^2}\)</span>)</td><td><spanclass="math inline">\(\frac{\phi(d_1)}{S_t\sigma\sqrt{\tau}}=Ke^{-r\tau}\frac{\phi(d_2)}{S_t^2\sigma\sqrt{\tau}}\)</span></td><td>同Call</td></tr><tr class="odd"><td>Vega(<span class="math inline">\(\frac{\partial}{\partial\sigma}\)</span>)</td><td><spanclass="math inline">\(S_t\tau\psi(d_1)=Ke^{-r\tau}\sqrt{\tau}\phi(d_2)\)</span></td><td>同Call</td></tr><tr class="even"><td>Theta(<span class="math inline">\(\frac{\partial}{\partialt}\)</span>)</td><td><spanclass="math inline">\(-Ke^{-r\tau}\phi(d_2)-\frac{S_t\phi(d_1)\sigma}{2\sqrt{\tau}}\)</span></td><td><spanclass="math inline">\(Ke^{-r\tau}\phi(-d_2)-\frac{S_t\phi(d_1)\sigma}{2\sqrt{\tau}}\)</span></td></tr><tr class="odd"><td>Rho(<span class="math inline">\(\frac{\partial}{\partialr}\)</span>)</td><td><span class="math inline">\(K\tau e^{-r\tau}\phi(d_2)\)</span></td><td><span class="math inline">\(-K\taue^{-r\tau}\phi(-d_2)\)</span></td></tr></tbody></table><p>delta：call option为正，put option为负</p><p>gamma：多头一定为正</p><p>vega：多头为正，空头为负</p><h4 id="高阶greek-formula">高阶Greek formula</h4><h5 id="vanna">Vanna</h5><p><span class="math display">\[Vanna=\frac{\partial^2 V}{\partial S \partial \sigma}=\frac{\partial\varDelta}{\partial \sigma}\]</span></p><p>Vanna衡量的是标的资产价格和波动率之间的敏感度，也就是当标的资产价格和波动率同时变化时，期权价格的变化量。</p><h5 id="vomma">Vomma</h5><p><span class="math display">\[Vomma=\frac{\partial^2 V}{\partial \sigma^2}=\frac{\partialVega}{\partial \sigma}\]</span></p><p>Vomma衡量的是波动率对期权价格的敏感度，也就是当波动率变化时，期权价格的变化量。Vomma的具体定义是，对于一个持有期权头寸的投资者，当波动率变动一个单位时，Vega值的变化量。如果Vomma值为正，那么当波动率上涨时，期权价格也会上涨，反之则会下跌。</p><h4 id="greek-formula的问题">Greek formula的问题</h4><ul><li>ITM/ATM/OTM delta的数值<ul><li>ITM期权的Delta数值为正数，接近于1；</li><li>ATM期权的Delta数值接近于0，可能略有正数或负数；</li><li>OTM期权的Delta数值为负数，接近于0。</li></ul></li><li>ATM delta略大于0.5的原因</li><li>类似call/put的Gamma，Vega公式的区别</li><li>delta of at the money call option</li></ul><h4 id="hedge">Hedge</h4><p>delta hedge和gamma hedge risk</p><p>一个银行卖出100 put option, strike price 3700， 算怎么delta hedge</p><h4 id="greek扩展问题">Greek扩展问题</h4><p>做空call各类greek的方向</p><p>delta的衍生含义，e.g. ITM probability, binary option price</p><p>gamma的衍生含义</p><p>gamma和vega的区别</p><p>各类greek关于T, K, S变化的关系</p><p>e.g. ATM处，T越小，gamma越大</p><p>gamma/vega关系，gamma/theta关系</p><h5 id="各种greek之间可能的互相作用">各种greek之间可能的互相作用</h5><p>各种Greek值代表了期权价格对不同因素的敏感程度，它们之间可能存在互相作用的关系，如下所示：</p><ol type="1"><li>Delta和Gamma：Delta值表示期权价格对标的资产价格变化的敏感程度，Gamma值则表示Delta值对标的资产价格变化的敏感程度。当Gamma值较高时，Delta值可能会剧烈变化，因此Delta和Gamma值之间存在密切关系。</li><li>Delta和Theta：Delta值对时间的敏感程度较低，而Theta值则表示期权价格随时间流逝而减少的速度。当期权到期时间越来越近时，Delta值可能会变化，因此Delta和Theta值之间也存在一定的关系。</li><li>Vega和Theta：Vega值表示期权价格对波动率变化的敏感程度，而Theta值则表示期权价格随时间流逝而减少的速度。当随着时间的推移，波动率逐渐减小，Vega值可能会下降，从而影响到Theta值的变化。</li><li>Vega和Gamma：Vega值也表示期权价格对波动率变化的敏感程度，而Gamma值则表示Delta值对标的资产价格变化的敏感程度。当波动率变化时，Gamma值可能会发生变化，进而影响到Delta值和期权价格的变化。</li></ol><h5 id="人们怎么用greek为什么重要">人们怎么用Greek，为什么重要</h5><p>人们可以通过Greek值来评估期权价格和标的资产价格变化、波动率变化以及时间流逝对期权价格的影响程度，从而更好地进行风险管理和投资决策。</p><p>以下是Greek值的具体用途：</p><ol type="1"><li>Delta值：Delta值可以帮助投资者确定期权价格对标的资产价格变化的敏感程度。投资者可以通过调整Delta值，来进行动态对冲和风险管理。</li><li>Gamma值：Gamma值表示Delta值对标的资产价格变化的敏感程度。投资者可以利用Gamma值来调整Delta值，以便更好地进行风险管理。</li><li>Vega值：Vega值表示期权价格对波动率变化的敏感程度。投资者可以通过调整Vega值，来控制风险敞口和波动率风险。</li><li>Theta值：Theta值表示期权价格随时间流逝而减少的速度。投资者可以利用Theta值来管理时间价值和期权到期日的风险。</li></ol><p>Greek值在风险管理和投资决策中具有重要作用，能够帮助投资者更好地控制风险和获得收益。</p><h4 id="greek在不同strategy上的表现">greek在不同strategy上的表现</h4><p>Greek值在不同策略上的表现会有所不同。以下是一些常见的期权交易策略以及它们与Greek值之间的关系：</p><ol type="1"><li>Delta中性策略：Delta中性策略旨在通过对冲Delta风险来获得收益。这些策略通常涉及同时买入或卖出期权和标的资产，并根据标的资产价格的变化调整Delta值。在这种情况下，Delta值是策略中最重要的Greek值之一。</li><li>Gamma scalping策略：Gammascalping策略旨在通过利用Gamma值来获得收益。这些策略通常涉及买入或卖出期权，以利用Gamma值的增加或减少，同时对冲Delta风险。在这种情况下，Gamma值是策略中最重要的Greek值之一。</li><li>Vega交易策略：Vega交易策略旨在通过利用波动率的变化来获得收益。这些策略通常涉及买入或卖出期权，以利用Vega值的增加或减少。在这种情况下，Vega值是策略中最重要的Greek值之一。</li><li>时间价值策略：时间价值策略旨在通过利用时间价值的变化来获得收益。这些策略通常涉及买入或卖出期权，以利用Theta值的增加或减少。在这种情况下，Theta值是策略中最重要的Greek值之一。</li></ol><p>总之，每种策略对Greek值的依赖程度是不同的，因此在选择策略时需要考虑Greek值。同时，需要注意的是，不同Greek值之间的交互作用也可能影响策略的表现。</p><ul><li>long gamma long theta, long gamma short vega</li></ul><p><imgsrc="https://jkq-pic.oss-cn-shanghai.aliyuncs.com/img/20220912105717.png" /></p><h4 id="题目">题目</h4><h5 id="delta和underlying-price用vanilla-call-example">delta和underlyingprice，用vanilla call example</h5><h5id="什么情况下希望sell-the-gamma用什么option-structureshort-a-call-and-put-at-the-same-time">什么情况下希望sellthe gamma，用什么option structure：short a call and put at the sametime</h5><p><imgsrc="https://jkq-pic.oss-cn-shanghai.aliyuncs.com/img/20220912111629.png" /></p><p><imgsrc="https://jkq-pic.oss-cn-shanghai.aliyuncs.com/img/20220912113154.png" /></p>]]></content>
    
    
    <categories>
      
      <category>金融</category>
      
      <category>衍生品</category>
      
      <category>期权</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>面试</title>
    <link href="/2023/03/03/%E9%87%91%E8%9E%8D/%E9%9D%A2%E8%AF%95/"/>
    <url>/2023/03/03/%E9%87%91%E8%9E%8D/%E9%9D%A2%E8%AF%95/</url>
    
    <content type="html"><![CDATA[<h1 id="面试">面试</h1><h2 id="experience-invest-in-stocks">experience invest in stocks</h2><h2 id="what-factor-you-find-is-most-efficent">what factor you find ismost efficent</h2><h2 id="experience-in-buying-individual-stock">experience in buyingindividual stock</h2><h2id="pe-low-understanding-of-this-phenomenon-科技公司高金融企业低interest-rate-impactsensitivity-is-different">P/Elow, understanding of this phenomenon, 科技公司高，金融企业低（interestrate impact，sensitivity is different）</h2><h2 id="股票和外汇risk-indicator">股票和外汇risk indicator</h2><h2 id="cta-risk-parity-in-equicy-currency-commodity-futures">cta riskparity, in equicy, currency, commodity futures</h2><h2 id="multi-strategies">multi strategies</h2><h2id="多资产配置叫股债混合型基金uture-period.">多资产配置叫股债混合型基金utureperiod.</h2>]]></content>
    
    
    <categories>
      
      <category>金融</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>宏观</title>
    <link href="/2023/03/03/%E9%87%91%E8%9E%8D/%E5%AE%8F%E8%A7%82/"/>
    <url>/2023/03/03/%E9%87%91%E8%9E%8D/%E5%AE%8F%E8%A7%82/</url>
    
    <content type="html"><![CDATA[<h1 id="宏观">宏观</h1><h2id="结合当前宏观经济阐述下半年我国宏观经济走向以及资产配置建议">结合当前宏观经济阐述下半年我国宏观经济走向以及资产配置建议</h2><h2 id="美联储是否加息有什么影响">美联储是否加息，有什么影响</h2><h2id="经济政策和货币政策三年内的例子这两种政策怎么被用">经济政策和货币政策，三年内的例子，这两种政策怎么被用</h2><h2 id="quantitative-easing-machanism">quantitative easingmachanism</h2><h2 id="currency-interest-rate-parity">currency interest rateparity</h2><h2 id="fundamental-driver-of-currency-change">fundamental driver ofcurrency change</h2>]]></content>
    
    
    <categories>
      
      <category>金融</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>量化</title>
    <link href="/2023/03/03/%E9%87%91%E8%9E%8D/%E9%87%8F%E5%8C%96/"/>
    <url>/2023/03/03/%E9%87%91%E8%9E%8D/%E9%87%8F%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<h1 id="量化">量化</h1><h2id="量化交易都有哪些主要的策略类型">量化交易都有哪些主要的策略类型</h2><p>Alpha 策略，CTA 策略以及高频交易策略</p>]]></content>
    
    
    <categories>
      
      <category>金融</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>远期和期货</title>
    <link href="/2023/03/03/%E9%87%91%E8%9E%8D/%E8%A1%8D%E7%94%9F%E5%93%81/%E8%BF%9C%E6%9C%9F%E5%92%8C%E6%9C%9F%E8%B4%A7/"/>
    <url>/2023/03/03/%E9%87%91%E8%9E%8D/%E8%A1%8D%E7%94%9F%E5%93%81/%E8%BF%9C%E6%9C%9F%E5%92%8C%E6%9C%9F%E8%B4%A7/</url>
    
    <content type="html"><![CDATA[<h1 id="远期和期货">远期和期货</h1><h2 id="远期和期货定价">远期和期货定价</h2><h3 id="远期价格f_0">远期价格<spanclass="math inline">\(F_0\)</span></h3><ul><li><p>投资资产（investment asset）： <span class="math display">\[F_0=S_0e^{cT}\]</span> 其中<span class="math inline">\(c\)</span>是持有成本（cost ofcarry），具体如下： <span class="math display">\[c=\begin{cases}  r\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 无股息资产\\  r-q\ \ \ \ \ \ \ \ 连续收益率为q的资产\\  r-r_f\ \ \ \ \  \ 无风险收益率为r_f的外汇\\  r-q+u\ 连续收益率为q且存储成本率为u的资产\end{cases}\]</span></p></li><li><p>投资资产，收益现值为<span class="math inline">\(I\)</span>：<span class="math display">\[F_0=(S_0-I)e^{rT}\]</span></p></li><li><p>投资资产，存储成本现值为<span class="math inline">\(U\)</span>：<span class="math display">\[F_0=(S_0+U)e^{rT}\]</span></p></li><li><p>消费资产（consumption asset），便利收益率（convenienceyield）为<span class="math inline">\(y\)</span>，持有成本（cost ofcarry）为<span class="math inline">\(c\)</span>： <spanclass="math display">\[F_0=S_0e^{(c-y)T}\]</span></p></li></ul><h3 id="远期价值f">远期价值<span class="math inline">\(f\)</span></h3><p><span class="math display">\[f=(F_0-K)e^{-rT}\]</span></p><p>其中<span class="math inline">\(F_0\)</span>是远期价格，<spanclass="math inline">\(K\)</span>是交割价格</p><h2 id="远期价格-vs-期货价格">远期价格 vs 期货价格</h2><ul><li><span class="math inline">\(S\)</span>和<spanclass="math inline">\(r\)</span>正相关：期货价格&gt;远期价格（<spanclass="math inline">\(S\)</span>增大，<spanclass="math inline">\(r\)</span>增大，收益的投资收益增大；<spanclass="math inline">\(S\)</span>减小，<spanclass="math inline">\(r\)</span>减小，亏损的融资利率减小）</li><li><span class="math inline">\(S\)</span>和<spanclass="math inline">\(r\)</span>负相关：期货价格&lt;远期价格</li></ul><h2 id="远期">远期</h2><h3id="从利率的期限结构来看5-年即期利率为每年-1010-年即期利率为每年-15那么从第-5-年到第-10年的隐含远期收益率是多少">从利率的期限结构来看，5年即期利率为每年 10%，10 年即期利率为每年 15%。那么从第 5 年到第10年的隐含远期收益率是多少？</h3><p>设本金为<span class="math inline">\(x\)</span> <spanclass="math display">\[x\cdot(1+10\%)^5(1+r)^5=x\cdot(1+15\%)^{10}\]</span></p><h3 id="cost-of-carry">Cost of carry</h3><h2 id="给两个即期利率算远期利率">给两个即期利率算远期利率</h2><h2 id="期货">期货</h2><h3 id="远期和期货的区别">远期和期货的区别</h3><h3 id="期货定价五个要素">期货定价五个要素</h3>]]></content>
    
    
    <categories>
      
      <category>金融</category>
      
      <category>衍生品</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>固收</title>
    <link href="/2023/03/03/%E9%87%91%E8%9E%8D/%E5%9B%BA%E6%94%B6/"/>
    <url>/2023/03/03/%E9%87%91%E8%9E%8D/%E5%9B%BA%E6%94%B6/</url>
    
    <content type="html"><![CDATA[<h1 id="固收">固收</h1><h2id="给出每期的现金流和即期利率求出债券价格和久期">给出每期的现金流和即期利率，求出债券价格和久期</h2><h2 id="债券的yield和rate-of-return的区别">债券的“yield”和“rate ofreturn”的区别</h2><p>债券的“yield”是“internal rate ofreturn”或“yield-to-maturity”或“promised-yield”。如果你持有债券到期，那就是你的收入。</p><p>债券的“rate ofreturn”是已实现现金流的内部收益率对持有者，如果债券在到期日之前出售，（已实现的）“rateof return”可以为正，可以为负。</p><p>假设你购买了一份promised5%的债券如果你卖掉该债券，你的资本将受损失并且得到一个负的“rate ofreturn”。然而，如果你持有债券直至到期，你将会得到promised 5%</p><h2id="怎么measure-fix-income-productbond-interest-rate-swap">怎么measurefix income product(bond, interest rate swap)</h2><h2 id="carry-of-a-bondcoupun---financing-rate">carry of a bond(coupun -financing rate)</h2><h2 id="rho-of-a-bondyield-curve">rho of a bond(yield curve)</h2><h2 id="diff-between-stock-bond-and-credit-bond">diff between stock bondand credit bond</h2><h2 id="micro-driver-of-bond-privacy">micro driver of bond privacy</h2><h2 id="inflation-data-soran-bond">inflation data soran bond</h2><h2 id="国债市场基本面">国债市场基本面</h2><h2 id="loan和bond的区别">loan和bond的区别</h2><ul><li>liquidity trading：bond可以在二级市场交易【OTC】，loan不可以</li><li>lender：load的lender是bank，bond的lender可以是很多主体（retail,commercial bank, ...）</li><li>time to maturity ：loan到期时间短，bond到期时间长</li></ul>]]></content>
    
    
    <categories>
      
      <category>金融</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>定量计算</title>
    <link href="/2023/03/03/%E9%87%91%E8%9E%8D/%E5%AE%9A%E9%87%8F%E8%AE%A1%E7%AE%97/"/>
    <url>/2023/03/03/%E9%87%91%E8%9E%8D/%E5%AE%9A%E9%87%8F%E8%AE%A1%E7%AE%97/</url>
    
    <content type="html"><![CDATA[<h1 id="定量计算">定量计算</h1><h2id="如果一只股票每年连续复利收益率的标准差是10那么连续复利4年股票收益率的标准差是多少">如果一只股票每年连续复利收益率的标准差是10%，那么连续复利4年股票收益率的标准差是多少</h2><p>假设连续复利收益率遵循布朗运动算法，收益的方差与复利计算期呈线性增长。这是因为随机游走中的连续回报是有限的。而独立随机变量喝的方差的和就是方差的和。这意味着4年的σ2 等于1年 σ2 的4倍。因此，4年的 σ 是1年 σ的2倍，因此，答案就是20%。</p><h2id="stock-expected-10-vol-is-10-corr-is-16-construct-portfolio-to-achieve-min-variance">2stock expected 10%, vol is 10%, corr is 16%, construct portfolio toachieve min variance</h2><h2 id="var">VaR</h2><p>Under a certain confidence level, the maximum possible loss of acertain financial asset or portfolio value in a specific futureperiod.</p>]]></content>
    
    
    <categories>
      
      <category>金融</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>回测</title>
    <link href="/2023/03/03/%E9%87%91%E8%9E%8D/%E5%9B%9E%E6%B5%8B/"/>
    <url>/2023/03/03/%E9%87%91%E8%9E%8D/%E5%9B%9E%E6%B5%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="回测">回测</h1><h2 id="常见指标">常见指标</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 年化收益率</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">annualized_return</span>(<span class="hljs-params">nav</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">pow</span>(nav[-<span class="hljs-number">1</span>] / nav[<span class="hljs-number">0</span>], <span class="hljs-number">250</span> / <span class="hljs-built_in">len</span>(nav)) - <span class="hljs-number">1</span><br><span class="hljs-comment"># 年化波动率</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">annualized_vol</span>(<span class="hljs-params">nav</span>):<br>    <span class="hljs-keyword">return</span> nav.pct_change().dropna().std() * np.sqrt(<span class="hljs-number">250</span>)<br><span class="hljs-comment"># 年化夏普比率</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sharp_ratio</span>(<span class="hljs-params">nav</span>):<br>    <span class="hljs-keyword">return</span> annualized_return(nav) / annualized_vol(nav)<br><span class="hljs-comment"># 最大回撤</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">max_drawdown</span>(<span class="hljs-params">nav</span>):<br>    drawdown = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> index <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(nav)):<br>        cur_drawdown = nav[index] / <span class="hljs-built_in">max</span>(nav[<span class="hljs-number">0</span>:index]) - <span class="hljs-number">1</span><br>        <span class="hljs-keyword">if</span> cur_drawdown &gt; drawdown:<br>            drawdown = cur_drawdown<br>    <span class="hljs-keyword">return</span> drawdown<br></code></pre></td></tr></table></figure><h3 id="turnover">turnover</h3><h3 id="heater-ratio">heater ratio</h3><p>收益率为正的1</p>]]></content>
    
    
    <categories>
      
      <category>金融</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>金融经济</title>
    <link href="/2023/03/03/%E9%87%91%E8%9E%8D/%E9%87%91%E8%9E%8D%E7%BB%8F%E6%B5%8E/"/>
    <url>/2023/03/03/%E9%87%91%E8%9E%8D/%E9%87%91%E8%9E%8D%E7%BB%8F%E6%B5%8E/</url>
    
    <content type="html"><![CDATA[<h1 id="金融经济">金融经济</h1><h2 id="alpha-beta">Alpha, Beta</h2><ul><li>alpha: For unsystematic risk, Alpha is the return that investors getunrelated to market fluctuations. It is generally used to measureinvestors' investment skills.</li><li>beta: It represents the systematic risk of investment, reflectingthe sensitivity of the strategy to changes in the overall market.</li></ul><h2 id="capm">CAPM</h2><ul><li>缺陷</li></ul><h2 id="apt">APT</h2><h2 id="fama-french">Fama-French</h2><h2 id="barra模型">Barra模型</h2><h2id="什么是混沌理论可以用它来预测股票收益率吗如果可以请说明为什么">什么是混沌理论？可以用它来预测股票收益率吗？如果可以，请说明为什么？</h2><p>第一问：大家可以百度出很多内容，这里不过多陈述。</p><p>第二问：如果你想预测股票收益率，建议你使用神经网络或者其他非线性模型。混论理论在自然科学中是伟大的，但它在金融中却是失败的</p><h2 id="系统性风险和非系统性风险">系统性风险和非系统性风险</h2><p>Systematic risk refers to those risk factors that can affect thewhole financial market, including economic cycle, changes in nationalmacroeconomic policies, etc. Such risks cannot be offset or weakened bydiversification.</p><p>Non systematic risk is a risk related to a specific company orindustry, and it has nothing to do with economic, political and otherfactors that affect all financial variables. By diversifying investment,non systematic risks can be reduced, and if diversification issufficient and effective, such risks can also be eliminated.</p>]]></content>
    
    
    <categories>
      
      <category>金融</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Brain Teasers</title>
    <link href="/2023/03/03/%E6%95%B0%E5%AD%A6/Brain%20Teasers/"/>
    <url>/2023/03/03/%E6%95%B0%E5%AD%A6/Brain%20Teasers/</url>
    
    <content type="html"><![CDATA[<h1 id="brain-teasers">Brain Teasers</h1><h2id="你在一个战场上看守着-100个凶手而你的枪只有一颗子弹如果任何一个凶手的生还几率不是零他就会试图逃跑如果凶手确已死亡他就不会企图逃跑你如何阻止他们逃跑">你在一个战场上看守着100个凶手，而你的枪只有一颗子弹。如果任何一个凶手的生还几率不是零，他就会试图逃跑。如果凶手确已死亡，他就不会企图逃跑。你如何阻止他们逃跑?</h2><h2id="人排队登上一架正好有-100-个座位的飞机每位乘客都有一张分配到特定座位的机票第一个上车的人喝醉了随便找了个座位坐下其余的旅客如果发现分配给他们的座位是空的就会坐进去如果他们发现自己的座位有人坐他们会随机选择一个座位大家都上了车都坐好了最后一个登机人坐在指定座位上的概率是多少">100人排队登上一架正好有 100个座位的飞机。每位乘客都有一张分配到特定座位的机票。第一个上车的人喝醉了，随便找了个座位坐下。其余的旅客如果发现分配给他们的座位是空的，就会坐进去。如果他们发现自己的座位有人坐，他们会随机选择一个座位。大家都上了车，都坐好了。最后一个登机人坐在指定座位上的概率是多少?</h2><p>令<spanclass="math inline">\(p_n\)</span>为n个人坐在指定座位上的概率。要确定<spanclass="math inline">\(p_{100}\)</span>。设A为最后一个人坐在自己座位上的事件，<spanclass="math inline">\(B_i\)</span>为当还有i个座位时上车的人选择的座位号。遵循1到i的离散均匀分布，因此<spanclass="math inline">\(P(B_i = k) =\frac{1}{i}\)</span>，因为有i个座位，人们随机选择一个。从n=1的情况开始</p><h2 id="的阶乘100后面有多少个零">100 的阶乘（100!）后面有多少个零？</h2><p>因为10=2$<spanclass="math inline">\(5，所以0的个数就是100!因式分解后2\)</span><spanclass="math inline">\(5（必须配对）的个数。显然因式分解中2的个数比5多，因此问题划归为5的个数决定了后面0的数量。先来数5因子有几个：在100内，5作为因子的数有5, 10, 15, 20, 25... 总共有20个。但是注意到25, 50,75, 100都包含了2个5作为因子（25=1\)</span><spanclass="math inline">\(5\)</span><span class="math inline">\(5,50=2\)</span><spanclass="math inline">\(5\)</span>$5等）。因此对于这些数要多数一次。所以总共就是有24个5因子，100!后面有24个0。</p><h2id="来自不同银行的-8-位宽客聚在一起喝酒他们都想知道在坐-8-个人的平均工资然而每个人都不愿意向其他人透露自己的薪水你能想出一个策略让这-8-个人在不知道别人薪水的情况下计算出在座各位稍微平均工资吗">来自不同银行的8 位宽客聚在一起喝酒。他们都想知道在坐 8个人的平均工资。然而，每个人都不愿意向其他人透露自己的薪水。你能想出一个策略让这8 个人在不知道别人薪水的情况下计算出在座各位稍微平均工资吗？</h2><p>让第1个宽客选择一个随机数a，把这个随机数加到他的工资中，假设这个数是b。第2个宽客把他自己的工资加到b中，按照这个方法，依次到第8个宽客，假设最后结果是c，同时第八个宽客把结果c再给到第一个宽客手中。然后第一个宽客从c中减去a得到d，最后将d除以8，就得到了大家的平均工资。</p><h2id="一栋大楼有三部电梯你怎么安排这三部电梯是这个大楼各楼层的人都能最快的按到自己想要的电梯">一栋大楼有三部电梯，你怎么安排这三部电梯是这个大楼各楼层的人都能最快的按到自己想要的电梯</h2><h2 id="epi和pie哪个大"><spanclass="math inline">\(e^\pi\)</span>和<spanclass="math inline">\(\pi^e\)</span>哪个大</h2><p>两边取对数 <span class="math display">\[a=lne^\pi=\pi\\b=ln\pi ^e=eln\pi\]</span> 构造函数 <span class="math display">\[f(x)=x-elnx\]</span> 令 <span class="math display">\[f&#39;(x)=1-\frac{e}{x}=0\]</span> 则 <span class="math display">\[x=e为极小值点\]</span> 而<span class="math inline">\(\pi&gt;e\)</span>，所以<spanclass="math inline">\(e^\pi&gt;\pi^e\)</span></p><h2id="你在一个木板的x位置5是末端0是起点每次你有一半概率向左走或向右走问最终走到末端的概率">你在一个木板的x位置，5是末端，0是起点，每次你有一半概率向左走或向右走，问最终走到末端的概率</h2><h2 id="如果xland-xland-xland-...2求x">如果<spanclass="math inline">\(x\land x\land x\land ...=2\)</span>，求<spanclass="math inline">\(x\)</span></h2><p><span class="math display">\[\underset{n\rightarrow \infty}{\lim}\frac{x\land x\land x\land ...}{n}=2\Leftrightarrow \underset{n\rightarrow \infty}{\lim}\frac{x\land x\landx\land ...}{n-1}=2\]</span></p><p>则当n 趋近于无穷大时，加上或减去一个<spanclass="math inline">\(x\land\)</span>应该会得到相同的结果： <spanclass="math display">\[x\land x\land x\land ...=x\land(x\land x\land  ...)=x\land 2\Rightarrowx=\sqrt{2}\]</span></p><h2id="一个钟表按顺时针方向编号1-12从墙上掉了下来摔成三块你会发现每一块上的数字之和是相等的那么每一块上的数字是多少呢">一个钟表（按顺时针方向编号1-12）从墙上掉了下来，摔成三块。你会发现每一块上的数字之和是相等的。那么，每一块上的数字是多少呢？</h2><p>使用求和方程，(1+12)*12/2=78，所以每一块的数字和必须是26，每一块上的数字必须是连续的，因为题目已经说明不允许有奇形怪状的碎块，我们很容易知道：5+6+7+8=26，但是为什么再找不到更多的连续数字加起来是26呢？</p><p>这样的假设从12点到1点在钟表上就不正确了。一旦这个错误的假设被消除，那我们的思路就变得很明朗：12+1=13，11+2=13。因此第二个碎块应该是11，12，1，2。那最后一块自然而然就是3，4，9，10。</p><h2id="假设有98个不同的整数从1到100有什么好的方法找出两个缺失的整数在1100内">假设有98个不同的整数从1到100。有什么好的方法找出两个缺失的整数在[1,100]内？</h2><p>将缺失的整数表示为<span class="math inline">\(x\)</span>和<spanclass="math inline">\(y\)</span>。现有的整数表示为<spanclass="math inline">\(z_1\)</span>到<spanclass="math inline">\(z_{98}\)</span>。应用求和方程：</p><p><span class="math display">\[\sum_{n=1}^{100} n=x+y+\sum_{i=1}^{98} z_i \Rightarrowx+y=\frac{102\times 101}{2}-\sum_{i=1}^{98}z_i\]</span> <span class="math display">\[\sum_{n=1}^{100}n^2=x^2+y^2+\sum_{i=1}^{98}z_i^2\Rightarrowx^2+y^2=\frac{100\times 101 \times 201}{6}-\sum_{i=1}^{98}z_i^2\]</span> 【<span class="math inline">\(\sum^n_{i=1}\limitsi^2=\frac{n(n+1)(2n+1)}{6}\)</span>】</p><p>通过上面两个方程可以解出x和y。</p><h2 id="bo3-tennis-gamebet-on-two-games-or-three-games">BO3 tennisgame，bet on two games or three games?</h2><p>设A、B赢的概率分别是p，q</p><p>两场结束的概率：<span class="math inline">\(p^2+q^2\)</span></p><p>三场结束的概率：<span class="math inline">\(2pq\)</span></p>]]></content>
    
    
    <categories>
      
      <category>数学</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>随机过程</title>
    <link href="/2023/03/03/%E6%95%B0%E5%AD%A6/%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B/"/>
    <url>/2023/03/03/%E6%95%B0%E5%AD%A6/%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="随机过程">随机过程</h1><h2 id="鞅">鞅</h2><h3 id="停时">停时</h3><p>假如你在做地铁，但是你没有地铁路程图，如果别人和你说到什么站做什么事，然后你坐车到了那个站，你看见站牌名，就知道你可以开始行动了。这就是停时。</p>]]></content>
    
    
    <categories>
      
      <category>数学</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>岭回归和LASSO回归</title>
    <link href="/2023/03/03/%E6%95%B0%E5%AD%A6/%E5%B2%AD%E5%9B%9E%E5%BD%92%E5%92%8CLASSO%E5%9B%9E%E5%BD%92/"/>
    <url>/2023/03/03/%E6%95%B0%E5%AD%A6/%E5%B2%AD%E5%9B%9E%E5%BD%92%E5%92%8CLASSO%E5%9B%9E%E5%BD%92/</url>
    
    <content type="html"><![CDATA[<h1 id="岭回归和lasso回归">岭回归和LASSO回归</h1><h2 id="岭回归">岭回归</h2><p>和Beyesian回归比较</p><p>和PCA比较 <span class="math display">\[L(\beta)=||Y-X\beta||^2+\lambda ||\beta||^2\]</span></p><p>Ridge regression penalizes coefficients with L2 regularization. Ridgeregression can decrease some coefficients, but can not select features.And it has closed form. Under the condition of many small/medium sizedeffects, using ridge regression is better.</p><ul><li>有偏估计</li></ul><h4 id="岭回归模型">岭回归模型</h4><p><span class="math display">\[\begin{aligned}\frac{\partial L}{\partial \beta}&amp;=-2X^\top(Y-X\beta)+2\lambda\beta\\&amp;=2(X^\top X+\lambda I_{pp})\beta-2X^\top Y\\&amp;=0\end{aligned}\]</span> <span class="math display">\[\hat{\beta}_{ridge}=(X^\top X+\lambda I_{pp})^{-1}X^\top Y\]</span></p><h5 id="期望和方差">期望和方差</h5><p><span class="math display">\[\begin{aligned}E[\hat{\beta}_{ridge}]&amp;=E[(X^\top X+\lambda I_{pp})^{-1}X^\top Y]\\&amp;=(X^\top X+\lambda I_{pp})^{-1}X^\top E[Y]\\&amp;=(X^\top X+\lambda I_{pp})^{-1}X^\top X \beta\\&amp;\ne \beta\end{aligned}\]</span></p><p>Denote <span class="math inline">\(\hat{\beta}\)</span> as theestimate of ordinary linear regression, and <spanclass="math inline">\(W_\lambda = (X^\top X+\lambda I_{pp})^{-1}X^\topX\)</span>. Then <span class="math display">\[\begin{aligned}W_\lambda \hat{\beta} &amp;= (X^\top X+\lambda I_{pp})^{-1}X^\top X(X^\top X)^{-1}X^\top Y\\&amp;=(X^\top X+\lambda I_{pp})^{-1}X^\top Y\\&amp;=\hat{\beta}_{ridge}\end{aligned}\]</span> Since <span class="math display">\[((X^\top X)^{-1})^\top=((X^\top X)^{\top})^{-1}=(X^\top X)^{-1}\]</span> <span class="math display">\[Var(\hat{\beta})=\sigma^2 (X^\top X)^{-1}\]</span></p><p>then</p><p><span class="math display">\[\begin{aligned}Var(\hat{\beta}_{ridge})&amp;=Var(W_\lambda \hat{\beta})\\&amp;=E[(W_\lambda \hat{\beta}-W_\lambda \beta)(W_\lambda\hat{\beta}-W_\lambda \beta)^\top]\\&amp;=W_\lambda E[ \hat{\beta}\hat{\beta}^\top] W_\lambda^\top-W_\lambda\beta \beta^\top W_\lambda^\top\\&amp;=W_\lambda (\beta\beta^\top +\sigma^2(X^\topX)^{-1})W_\lambda^\top-W_\lambda \beta \beta^\top W_\lambda^\top\\&amp;=\sigma^2 W_\lambda (X^\top X)^{-1} W_\lambda^\top\\&amp;=\sigma^2 (X^\top X+\lambda I_{pp})^{-1}X^\top X [(X^\top X+\lambdaI_{pp})^{-1}]^\top\end{aligned}\]</span></p><h3 id="岭回归vs线性回归">岭回归vs线性回归</h3><h4 id="方差">方差</h4><p><span class="math display">\[\begin{aligned}    Var(\hat{\beta})-Var(\hat{\beta}_{ridge})&amp;=\sigma^2 (X^\topX)^{-1}-\sigma^2 (X^\top X+\lambda I_{pp})^{-1}X^\top X [(X^\topX+\lambda I_{pp})^{-1}]^\top\\    &amp;=\sigma^2 W_\lambda[2\lambda(X^\top X)^{-2}+\lambda^2 (X^\topX)^{-3}]W_\lambda^\top\\    &amp;=\sigma^2(X^\top X+\lambda I_{pp})^{-1}[2\lambdaI_{pp}+\lambda^2(X^\top X)^{-1}][(X^\top X+\lambda I_{pp})^{-1}]^\top\end{aligned}\]</span> The difference is non-negative definite as each component inthe matrix product is non-negative definite. <spanclass="math display">\[Var(\hat{\beta})\succcurlyeq  Var(\hat{\beta}_{ridge})\]</span> Hence, he variance of the ridge estimator is less than thevariance of the maximum likelihood estimator for the ordinary regressionmodel.</p><h4 id="mse">MSE</h4><p><span class="math display">\[MSE[\hat{\beta}_{ridge}]=Var(\hat{\beta}_{ridge})+Bias^2(\hat{\beta}_{ridge})\]</span> <span class="math display">\[MSE[\hat{\beta}]=Var(\hat{\beta})+Bias^2(\hat{\beta})=Var(\hat{\beta})\]</span></p><p><span class="math display">\[\begin{aligned}Bias^2(\hat{\beta}_{ridge})&amp;=(E[\hat{\beta}_{ridge}-\beta])^2\\&amp;=[(X^\top X+\lambda I_{pp})^{-1}X^\top X \beta-\beta]^2\\&amp;=[(X^\top X+\lambda I_{pp})^{-1}(X^\top X+\lambda I_{pp}-\lambdaI_{pp}) \beta-\beta]^2\\&amp;=[-\lambda(X^\top X+\lambda I_{pp})^{-1} \beta]^2\\&amp;=\lambda^2 (X^\top X+\lambda I_{pp})^{-1} \beta \beta^\top[(X^\topX+\lambda I_{pp})^{-1} ]^\top\end{aligned}\]</span> <span class="math display">\[\begin{aligned}    MSE[\hat{\beta}]-MSE[\hat{\beta}_{ridge}]&amp;=Var(\hat{\beta})-Var(\hat{\beta}_{ridge})-Bias^2(\hat{\beta}_{ridge})\\    &amp;=\sigma^2(X^\top X+\lambda I_{pp})^{-1}[2\lambdaI_{pp}+\lambda^2(X^\top X)^{-1}][(X^\top X+\lambda I_{pp})^{-1}]^\top\\    &amp;\quad -\lambda^2 (X^\top X+\lambda I_{pp})^{-1} \beta\beta^\top[(X^\top X+\lambda I_{pp})^{-1} ]^\top\\    &amp;=\lambda(X^\top X+\lambdaI_{pp})^{-1}[2\sigma^2I_{pp}+\lambda\sigma^2(X^\top X)^{-1}-\lambda\beta \beta^\top]\\    &amp;\quad [(X^\top X+\lambda I_{pp})^{-1}]^\top\end{aligned}\]</span> <spanclass="math inline">\(MSE[\hat{\beta}]-MSE[\hat{\beta}_{ridge}]\succeq0\)</span> only when <spanclass="math inline">\(2\sigma^2I_{pp}+\lambda\sigma^2(X^\topX)^{-1}-\lambda \beta \beta^\top\succeq 0\)</span>, that is <spanclass="math inline">\(2\sigma^2I_{pp}-\lambda \beta \beta^\top\succeq0\)</span>, <span class="math inline">\(\lambda \leqslant2\sigma^2(\beta^\top\beta)^{-1}\)</span>.</p><h4 id="优劣势">优劣势</h4><p>线性回归和岭回归都是线性模型，用于拟合数据之间的线性关系。它们的主要区别在于模型复杂度的控制和正则化项的引入。</p><p>通常来说，当训练数据的特征数比样本数大很多，或者特征之间存在多重共线性（即某些特征之间存在较高的相关性），线性回归的表现会比较差，此时可以使用岭回归。</p><p>另外，在存在离群点（outlier）的情况下，线性回归的表现也可能较差，因为离群点会对模型产生较大的影响。此时，可以使用带有正则化项的岭回归来降低离群点的影响。</p><p>总之，当样本数比特征数多，且不存在多重共线性或离群点时，线性回归可能会是更好的选择；而当特征数比样本数大或存在多重共线性或离群点时，岭回归可能会表现更好。当然，最好的方法是尝试不同的模型并进行比较。</p><h2 id="lasso回归">LASSO回归</h2><p><span class="math display">\[L(\beta)=||Y-X\beta||^2+\lambda ||\beta||^1\]</span></p><p>LASSO regression penalizes coefficients with L1 regularization. LASSOregression can make some coefficients exactly equal to 0, achievingfeature selection. But it has no closed form. Under the condition ofonly a few variables with medium/large effect, using LASSO regression isbetter</p><h2 id="岭回归-vs-lasso回归">岭回归 vs LASSO回归</h2><p>岭回归和Lasso回归是两种经典的正则化线性回归算法，它们都是通过在代价函数中引入正则化项来控制模型复杂度，避免过拟合。两者的主要区别在于使用的正则化项不同，岭回归使用L2正则化，而Lasso回归使用L1正则化。这两种正则化方式对应的参数惩罚项分别是参数平方和和参数绝对值和。</p><p>Ridge regression penalizes coefficents with <spanclass="math inline">\(L_2\)</span> regularization.Ridge regression candecrease some coefficients, but can not select features. And it hasclosed form. Under the condition of many small/medium sized effects,using ridge regression is better.</p><p>LASSO regression penalizes coefficents with <spanclass="math inline">\(L_1\)</span> regularization. LASSO regression canmake some coefficients exactly equal to 0, achieving feature selection.But it has no closed form. Under the condition of only a few variableswith medium/large effect, using LASSO regression is better.</p><h3 id="岭回归的优势">岭回归的优势</h3><ol type="1"><li>可以处理多重共线性：当数据中存在多个高度相关的自变量时，最小二乘回归会导致过拟合，而岭回归可以通过对参数进行平滑处理，减小多重共线性的影响，提高模型的鲁棒性。</li><li>对异常值不敏感：由于岭回归对参数进行了平滑处理，因此可以有效地降低异常值的影响，提高模型的稳定性。</li><li>可以得到更加稳定的结果：由于岭回归引入的L2正则化项能够控制模型的复杂度，因此可以有效地减小模型的方差，降低模型的波动性，从而得到更加稳定的结果。</li><li>有闭式解</li></ol><h3 id="lasso回归的优势">Lasso回归的优势</h3><ol type="1"><li>可以进行特征选择：由于L1正则化倾向于将一些参数压缩到0，因此可以用于特征选择，即通过Lasso回归来筛选出对目标变量有重要影响的特征，从而减小模型的复杂度。</li><li>可以用于稀疏数据集：在处理稀疏数据集时，Lasso回归通常比岭回归更为有效，因为L1正则化能够将某些系数压缩为0，从而能够更好地处理稀疏数据。</li></ol><p>LASSO回归无闭式解：L1正则化项对于参数的求导结果不是可解析的，因此无法直接求出最小化目标函数时的回归系数的闭式解。</p><p>为了解决LASSO回归无法直接求解的问题，通常采用迭代算法来逼近最小化目标函数时的回归系数。常用的算法有坐标下降（CoordinateDescent）算法、最小角回归（Least Angle Regression,LAR）算法、以及基于梯度下降（GradientDescent）的算法等。这些算法都可以在一定程度上逼近最小化目标函数时的回归系数，但无法得到闭式解。</p><p>综上所述，岭回归和Lasso回归都是非常实用的线性回归算法，在不同的应用场景下具有不同的优势。如果数据集中存在多重共线性，或者需要得到更加稳定的结果，岭回归可能更加合适；而如果需要进行特征选择或者处理稀疏数据集，则Lasso回归可能更加适用。</p><h3 id="ridge-vs-lasso稀疏解">Ridge vs LASSO稀疏解</h3><p>图中彩色线条即为目标函数的等高线，黑色线条即为约束条件。根据岭回归和LASSO回归的约束条件，分别得到下图两个图像。</p><p><imgsrc="https://jkq-pic.oss-cn-shanghai.aliyuncs.com/img/20230131092039.png" /></p><p>右图为LASSO回归图像，目标函数的等高线，大部分时候都会在角的地方相交。例如图中所示，横坐标<spanclass="math inline">\(\hat{\beta}_1\)</span>即为0，因此就产生了稀疏性【在高纬度情况下，同样道理】；而在岭回归的图像中，约束条件画出来是一个圆，所以相交的地方出现在具有稀疏性的位置的概率就变得非常小了。<strong>这就从直观上来解释了为什么LASSO回归能产生稀疏性，而岭回归不行的原因了。</strong></p>]]></content>
    
    
    <categories>
      
      <category>数学</category>
      
      <category>回归</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>线性回归</title>
    <link href="/2023/03/03/%E6%95%B0%E5%AD%A6/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    <url>/2023/03/03/%E6%95%B0%E5%AD%A6/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</url>
    
    <content type="html"><![CDATA[<h1 id="线性回归">线性回归</h1><h2 id="基本假设">基本假设</h2><ol type="1"><li><p>The Two Variables Should be in a <strong>Linear</strong>Relationship</p><ul><li>检验方法：散点图scatter plot</li><li>解决：According to the characteristics of the data, performnon-linear transformation on the independent/dependent variables. Forexample, take square, cubic, logarithmic, etc</li></ul></li><li><p>residuals follow <strong>normal distribution</strong></p><ul><li>检验方法：QQ图（quantile of target distribution and sampledistribution, if the data points are distributed near the straight liney=x)或ks检验</li><li>影响：置信区间会变得很不稳定</li><li>解决：<ul><li>寻找遗漏的自变量</li><li>检验并剔除异常值</li><li>对自变量和/或因变量进行非线性变换</li></ul></li></ul></li><li><p>There Should be No <strong>Multicollinearity</strong> in theData多重共线性</p><ul><li>Independent variables shall be independent of each other</li><li>检验方法：<ul><li>VIF (variance inflation factor)(&lt;10 is ok), <spanclass="math inline">\(VIF=\frac{1}{1-R^2}\)</span>，<spanclass="math inline">\(R^2\)</span> is the multiple correlationcoefficient of linear regression with this variable as the dependentvariable and the other variables as the independent variables. 表示Itrepresents the variance of the regression coefficient estimator over thevariance when the independent variables without multicollinearity</li><li>correlation matrix (&gt;0.8)【VIF和corr有关，VIF =皮尔逊相关系数矩阵的余子式/皮尔逊相关系数矩阵的行列式】</li></ul></li><li>影响：<ul><li>high correlated: variance of estimate be larger，矩阵<spanclass="math inline">\((X^TX)\)</span>几乎不可逆，<spanclass="math inline">\((X^TX)^{-1}\)</span>变得很大，使得方差<spanclass="math inline">\(Var(\hat{\beta}|X)=\sigma^2(X^TX)^{-1}\)</span>增大，系数估计不准确；X中元素轻微变化就会引起<spanclass="math inline">\((X^TX)^{-1}\)</span>很大变化，导致OLS估计值<spanclass="math inline">\(\hat{\beta}\)</span>发生很大变化</li><li>strict correlated: The rank of the X will be smaller than the numberof independent variables, making (XTX) irreversible, the estimate has nosolution</li></ul></li><li>解决：<ul><li>For highly correlated independent variables, only reserve one ofthem</li><li>PCA</li><li>Ridge/LASSO</li><li>如不关心具体的回归系数，只关心整个方程的预测能力，可不必理会多重共线性。多重共线性的主要后果是使得对单个变量的贡献估计不准，但所有变量的整体效应仍可较准确地估计</li><li>如关心具体的回归系数，但多重共线性并不影响所关心变量的显著性，也可不必理会。在方差膨胀的情况下，系数依然显著；如没有多重共线性，只会更显著</li><li>如多重共线性影响所关心变量的显著性，应设法进行处理。如增大样本容量，剔除导致严重共线性的变量，将变量标准化，或对模型设定进行修改</li></ul></li></ul></li><li><p>There Should be No <strong>Autocorrelation</strong> in theData自相关性</p><ul><li><p>The residuals should be independent of eachother.扰动项方差矩阵非主对角线（main diagonal）元素不为0</p></li><li><p>检验方法：DW Statistics</p><ul><li>BG检验：<ul><li>思路：扰动项存在阶自相关→p阶自回归方程系数不全为零→<spanclass="math inline">\(\varepsilon_i\)</span>不可观测，用<spanclass="math inline">\(e_i\)</span>替代进行辅助回归→因为残差是解释变量的函数，还需要在辅助回归中引入解释变量</li><li>原假设：<spanclass="math inline">\(H_0:\gamma_1=...=\gamma_p=0\)</span></li><li>辅助回归：<spanclass="math inline">\(e_t=\gamma_1e_{t-1}+...+\gamma_pe_{t-p}+\delta_2x_{t2}+...+\delta_Kx_{tK}+v_t(t=p+1,...,n)\)</span></li><li>LM检验：<spanclass="math inline">\(LM=(n-p)R^2\xrightarrow{d}\mathcal{X}^2(p)\)</span></li></ul></li></ul></li><li><p>影响：【自相关不影响无偏性、一致性、渐进正态】Reduce the accuracyof the model，普通标准误的t检验、F检验失效，不是BLUE</p></li><li><p>解决：</p><ul><li><p>GLS</p></li><li><p>ARIMA</p></li><li><p>CO估计法：</p><ol type="1"><li><p>假设扰动项<spanclass="math inline">\(\varepsilon_t\)</span>存在自相关，为一阶自回归形式<spanclass="math inline">\(\varepsilon_t=\rho\varepsilon_{t-1}+u_t\)</span>，<spanclass="math inline">\(u_t\)</span>为白噪声</p></li><li><p><img src="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20210623153310.png" style="zoom:50%;" /></p><p>新扰动项<spanclass="math inline">\((\varepsilon_t-\rho\varepsilon_{t-1})=u_t\)</span>为白噪声</p></li><li><p>对方程（8.14）进行OLS估计，但损失一个样本容量，仍不是BLUE</p></li><li><p>补上一个方程</p><p><img src="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20210623153537.png" style="zoom:50%;" /></p></li></ol></li></ul></li></ul></li><li><p>There Should be <strong>Homoscedasticity</strong> Among theData同方差性</p><ul><li><p>The variance of the residuals should beconstant，扰动项方差矩阵主对角线元素不相等</p></li><li><p>检验方法：White Test</p><ul><li>BP检验：<ul><li>思路：存在异方差→<spanclass="math inline">\(\varepsilon_i\)</span>对<spanclass="math inline">\(x_i\)</span>的回归系数不全为0→扰动项<spanclass="math inline">\(\varepsilon_i^2\)</span>不可观测，用残差平方<spanclass="math inline">\(e^2_i\)</span>代替进行辅助回归</li><li>原假设：<spanclass="math inline">\(H_0:\delta_2=...=\delta_K=0\)</span></li><li>辅助回归：<spanclass="math inline">\(e_i^2=\delta_1+\delta_2x_{i2}+...+\delta_Kx_{iK}+error_i\)</span></li><li>LM检验：<spanclass="math inline">\(LM=nR^2\xrightarrow{d}\mathcal{X}^2(K-1)\)</span></li></ul></li><li>White检验：<ul><li>思路：BP检验假设条件方差函数为线性函数，可能忽略了高次项。在BP检验的辅助回归中加入所有二次项（平方项和交叉项）</li><li>辅助回归：<spanclass="math inline">\(e_i^2=\delta_1+\delta_2x_{i2}+\delta_3x_{i3}+\delta_4x_{i2}^2+\delta_5x_{i3}^2+\delta_6x_{i2}x_{i3}+error_i\)</span></li><li>优点：可检验任何形式的异方差；因为根据泰勒展开式，二次函数可很好地逼近任何光滑函数</li><li>缺点：如果解释变量较多，则解释变量的二次项将更多，在辅助回归中将损失较多样本容量</li></ul></li></ul></li><li><p>影响：【异方差不影响无偏性、渐进正态】，估计不有效，误差较大，普通标准误的t检验、F检验失效，不是BLUE</p></li><li><p>解决：</p><ul><li><p>使用稳健回归。即，不实用最小二乘法的平方和形式的目标函数，从而降低了残差方差较大的数据点对参数估计的影响。</p></li><li><p>使用加权回归。即，根据每个数据点的拟合值的方差为每个数据点分配一个权重，给予较高方差的数据点较小的权重，从而缩小残差和</p></li><li><p>WLS：方差较小的观测值包含的信息量较大，给予方差较小的观测值较大的权重，然后进行加权最小二乘法估计。通过变量转换，使得变换后的模型满足球形扰动项的假定(变为同方差)，然后进行OLS估计</p><p>步骤：</p><ol type="1"><li>假设异方差形式为<spanclass="math inline">\(Var(\varepsilon_i|x_i)=\sigma_i^2=\sigma^2v_i\)</span></li><li>回归模型两边同时乘权重<spanclass="math inline">\(\frac{1}{\sqrt{v_i}}\)</span>，新扰动项<spanclass="math inline">\(\frac{\varepsilon_i}{\sqrt{v_i}}\)</span>不再有异方差</li><li>对回归模型进行OLS回归，最小化加权残差平方和<spanclass="math inline">\(min\sum^n_{i=1}\limits(\frac{e_i}{\sqrt{v_i}})^2=\sum^n_{i=1}\limits\frac{e_i^2}{v_i}\)</span>，权重为方差的倒数<spanclass="math inline">\(\frac{1}{v_i}\)</span></li></ol></li><li><p>FWLS：</p><ul><li>why FWLS：使用WLS虽可得到BLUE估计，但须知道每位个体的方差，但<spanclass="math inline">\(\{\sigma^2_i\}^n_{i=1}\)</span>在实践中通常不知</li><li>思路：先用样本数据估计<spanclass="math inline">\(\{\sigma^2_i\}^n_{i=1}\)</span>，再使用WLS</li><li>步骤：<ol type="1"><li>BP检验时进行辅助回归<spanclass="math inline">\(e_i^2=\delta_1+\delta_2x_{i2}+...+\delta_Kx_{iK}+error_i\)</span></li><li>得到<span class="math inline">\(\sigma^2_i\)</span>的估计值<spanclass="math inline">\(\hat{\sigma^2_i}=\hat{\delta_1}+\hat{\delta_2}x_{i2}+...+\hat{\delta_K}x_{iK}\)</span></li><li>为保证<spanclass="math inline">\(\hat{\sigma^2_i}\)</span>始终为正，假设条件方差函数为对数形式：<spanclass="math inline">\(ln(e_i^2)=\delta_1+\delta_2x_{i2}+...+\delta_Kx_{iK}+error_i\)</span>，对此方程进行OLS回归，得到<spanclass="math inline">\(lne^2_i\)</span>的预测值<spanclass="math inline">\(ln(\hat{\sigma_i^2})\)</span>，则<spanclass="math inline">\(\hat{\sigma_i^2}=e^{ln\hat{\sigma^2_i}}\)</span>一定为正</li><li>以<spanclass="math inline">\(\frac{1}{\hat{\sigma^2_i}}\)</span>为权重对原方程进行WLS估计</li></ol></li><li>优点：在大样本下比OLS更有效率</li><li>缺点：由于<spanclass="math inline">\(\hat{\beta_{FWLS}}\)</span>是y的非线性函数，一般有偏，所以不是BLUE</li></ul></li></ul></li></ul></li><li><p>exogenous外生性</p><ul><li><p>The <strong>residuals</strong> should be independent toindependent variables. 残差向量和每一个解释变量正交</p></li><li><p><span class="math inline">\(E(\varepsilon|X)=0\)</span>,condition on X, the expectation of residual is 0.</p></li></ul></li></ol><h2 id="ols优良性质">OLS优良性质</h2><h3 id="blue">BLUE</h3><p>在经典假设下，OLS估计量估计量具有，线性、无偏性和有效性三个优良性质，称为最佳线性无偏估计量（bestlinear unbiased estimator,BLUE）</p><p>球形扰动项下的估计量是最佳线性无偏估计量（BLUE），这个结论叫<strong>高斯马尔科夫定理</strong></p><h3 id="大样本">大样本</h3><ul><li>一致：<span class="math inline">\(\underset{n\rightarrow\infty}{plim}\hat{\beta_n}=\beta\)</span></li><li>渐进正态Asymptotic normal distribution：<spanclass="math inline">\(\hat{\beta_n}\xrightarrow{d} N(\beta,\frac{\sigma^2}{n})\)</span></li></ul><h2 id="线性回归模型">线性回归模型</h2><p><span class="math display">\[Y=X\beta+\varepsilon\]</span></p><p><span class="math display">\[\begin{aligned}L_\beta &amp;=(Y-X\beta)^\top (Y-X\beta)\\&amp;=Y^\top Y-Y^\top X\beta-\beta^\top X^\top Y+\beta^\top X^\topX\beta\end{aligned}\]</span></p><p><span class="math display">\[\begin{aligned}\frac{\partial L_\beta}{\partial \beta}&amp;=-X^\top Y-X^\top Y +X^\topX\beta+(\beta^\top X^\top X)^\top\\&amp;=-2X^\top Y+2X^\top X\beta\\&amp;=2X^\top (X\beta - Y)\\&amp;=0\end{aligned}\]</span></p><p><span class="math display">\[\hat{\beta}=(X^\top X)^{-1}X^\top Y\]</span></p><h3 id="梯度下降法求解hatbeta">梯度下降法求解<spanclass="math inline">\(\hat{\beta}\)</span></h3><p>梯度下降法损失函数用MSE： <span class="math display">\[MSE(\beta)=\frac{1}{n}(Y-X\beta)^\top (Y-X\beta)\]</span></p><p><span class="math display">\[\nabla MSE(\beta)=\frac{2}{n}X^\top (X\beta - Y)\]</span></p><p>梯度下降： <span class="math display">\[\beta = \beta-\alpha\nabla MSE(\beta)\]</span> 其中<span class="math inline">\(\alpha\)</span>是学习率</p><h4id="梯度下降法损失函数用mse的原因">梯度下降法损失函数用MSE的原因</h4><p>在使用梯度下降法求解线性回归时，常用的损失函数是均方误差（MSE）而不是误差平方和（SSE）。</p><p>误差平方和（SSE）是指所有样本点的预测值与真实值之差的平方和，而均方误差（MSE）是指所有样本点的预测值与真实值之差的平方和除以样本数量。这两个指标在一定程度上是相关的，但在优化过程中，使用MSE可以更好地优化模型。</p><ul><li>MSE可以缓解样本数量的影响。SSE随着样本数量的增加而增加，而MSE则不会随样本数量变化而变化，这使得使用MSE更具有泛化性能。因此，当使用梯度下降算法优化模型时，使用MSE可以更好地应对不同大小的数据集，而不需要调整超参数</li></ul><p>综上所述，使用MSE作为损失函数可以更好地应对不同大小的数据集，并且可以使梯度下降算法更加高效</p><h3 id="期望和方差">期望和方差</h3><p><span class="math display">\[\begin{aligned}E[\hat{\beta}]&amp;=E[(X^\top X)^{-1}X^\top (X\beta+\varepsilon)]\\&amp;=(X^\top X)^{-1}X^\top X\beta + (X^\top X)^{-1}X^\topE[\varepsilon]\\&amp;=\beta\end{aligned}\]</span></p><p><span class="math display">\[\begin{aligned}Var(\hat{\beta})&amp;=E[(\hat{\beta}-E[\hat{\beta}])(\hat{\beta}-E[\hat{\beta}])]\\&amp;=E[(\hat{\beta} -\beta)(\hat{\beta}-\beta)^\top]\\                &amp;=E[\hat{\beta}\hat{\beta}^\top-\hat{\beta}\beta^\top -\beta \hat{\beta}^\top +\beta \beta^\top]\\                &amp;=E[\hat{\beta}\hat{\beta}^\top]-E[\hat{\beta}]\beta^\top-\betaE[\hat{\beta}^\top]+\beta \beta^\top\\                &amp;=E[(X^\top X)^{-1}X^\top YY^\top X((X^\topX)^{-1})^\top]-\beta\beta^\top-\beta \beta^\top+\beta \beta^\top\\                &amp;=(X^\top X)^{-1}X^\top E[YY^\top]X(X^\topX)^{-1}-\beta\beta^\top\\                &amp;=(X^\top X)^{-1}X^\top E[(X\beta +\varepsilon)(X\beta + \varepsilon)^\top]X(X^\topX)^{-1}-\beta\beta^\top\\                &amp;=(X^\top X)^{-1}X^\top E[(X\beta +\varepsilon)(\beta^\top X^\top + \varepsilon^\top)]X(X^\topX)^{-1}-\beta\beta^\top\\                &amp;=(X^\top X)^{-1}X^\top E[X\beta \beta^\top X^\top +X\beta\varepsilon^\top+\varepsilon\beta^\top X^\top +\varepsilon\varepsilon^\top]X(X^\top X)^{-1}-\beta\beta^\top\\                &amp;=(X^\top X)^{-1}X^\top (X\beta \beta^\top X^\top +\sigma^2 I_n )X(X^\top X)^{-1}-\beta\beta^\top\\                &amp;=\beta\beta^\top +\sigma^2 (X^\topX)^{-1}-\beta\beta^\top\\                &amp;=\sigma^2 (X^\top X)^{-1}\end{aligned}\]</span></p><h3 id="损失函数用残差平方和的原因">损失函数用残差平方和的原因</h3><p>正态分布MLE是二次，所以损失函数不用更高次的和</p><h3 id="遗漏变量冗余变量与扰动项">遗漏变量、冗余变量与扰动项</h3><h4 id="遗漏变量">遗漏变量</h4><p><span class="math display">\[\beta=\left( \begin{array}{c}    \beta_0\\    \beta_1\\    \beta_2\\\end{array} \right)\\X=(1,x_{1t},x_{2t})\\r=X\beta+\varepsilon\\\beta=(X^\top X)^{-1}X^\top r\\\]</span></p><p>Denote <span class="math display">\[X&#39;=(1,x_{1t})\\\begin{aligned}\beta&#39;=\left( \begin{array}{c}    \hat{\alpha}\\    \hat{\beta}\\\end{array} \right)&amp;=(X&#39;^\top X&#39;)^{-1}X&#39;^\top r\\&amp;=(X&#39;^\top X&#39;)^{-1}X&#39;^\top (X\beta+\varepsilon)\\&amp;=(X&#39;^\top X&#39;)^{-1}X&#39;^\top X\beta+(X&#39;^\topX&#39;)^{-1}X&#39;^\top \varepsilon\\\end{aligned}\]</span> Then <span class="math display">\[E[\beta&#39;]=(X&#39;^\top X&#39;)^{-1}X&#39;^\top X\beta\\\begin{aligned}\left( \begin{array}{c}    \hat{\alpha}\\    E[\hat{\beta}]\\\end{array} \right)&amp;=\left( \begin{array}{c}    1&amp;x_{1t}\\    x_{1t}&amp; x_{1t}x_{1t}^\top\\\end{array} \right)^{-1}\left( \begin{array}{c}    1&amp;x_{1t}&amp;x_{2t}\\    x_{1t}&amp;x_{1t}x_{1t}^\top &amp;x_{1t}x_{2t}^\top\end{array} \right)\left( \begin{array}{c}    \beta_0\\    \beta_1\\    \beta_2\\\end{array} \right)\\&amp;=\left( \begin{array}{c}    1&amp;0&amp;k_1\\    0&amp;1&amp;k_2\\\end{array} \right)\left( \begin{array}{c}    \beta_0\\    \beta_1\\    \beta_2\\\end{array} \right)\end{aligned}\]</span> <span class="math display">\[E[\hat{\beta}]=\beta_1+\frac{Cov(x_1, x_2)}{Var(x_1)}\beta_2\]</span> If <span class="math inline">\(k_2\ne0\)</span>, <spanclass="math inline">\(\hat{\beta}\)</span> is not an unbiased estimatorof <span class="math inline">\(\beta_1\)</span> in (1), and it's notconsistent. The bias direction is based on <spanclass="math inline">\(k_2\propto Cov(x_{1t},x_{2t})\)</span>.</p><p>由于遗漏变量<spanclass="math inline">\(x_2\)</span>被归入扰动项中，可能增大扰动项的方差，影响OLS估计的精确度</p><h4 id="冗余变量">冗余变量</h4><p>Denote <span class="math display">\[\begin{aligned}X&#39;&amp;=(1,x_{1t},x_{2t},x_{3t})\\&amp;=\left( \begin{matrix}    1&amp;      x_{11}&amp;     x_{21}&amp;     x_{31}\\    1&amp;      x_{12}&amp;     x_{22}&amp;     x_{32}\\    ...&amp;        ...&amp;        ...&amp;        ...\\    1&amp;      x_{1T}&amp;     x_{2T}&amp;     x_{3T}\\\end{matrix} \right)\end{aligned}\]</span></p><p><span class="math display">\[A=\left( \begin{matrix}    1&amp;      0&amp;      0\\    0&amp;      1&amp;      0\\    0&amp;      0&amp;      1\\    0&amp;      0&amp;      0\\\end{matrix} \right)\]</span></p><p>Then <span class="math display">\[X=X&#39;A\\\beta&#39;=\left( \begin{array}{c}    \hat{\gamma}_0\\    \hat{\gamma}_1\\    \hat{\gamma}_2\\    \hat{\gamma}_3\end{array} \right)=(X&#39;^\top X&#39;)^{-1}X&#39;^\topX&#39;A\beta+(X&#39;^\top X&#39;)^{-1}X&#39;^\top \varepsilon\\E[\beta&#39;]=(X&#39;^\top X&#39;)^{-1}X&#39;^\top X&#39;A\beta=A\beta\\\left( \begin{array}{c}    E[\hat{\gamma}_0]\\    E[\hat{\gamma}_1]\\    E[\hat{\gamma}_2]\\    E[\hat{\gamma}_3]\end{array} \right)=\left( \begin{matrix}    1&amp;      0&amp;      0\\    0&amp;      1&amp;      0\\    0&amp;      0&amp;      1\\    0&amp;      0&amp;      0\\\end{matrix} \right)\left( \begin{array}{c}    \beta_0\\    \beta_1\\    \beta_2\\\end{array} \right)=\left( \begin{array}{c}    \beta_0\\    \beta_1\\    \beta_2\\    0\end{array} \right)\]</span> That is <span class="math display">\[E[\hat{\gamma}_1]=\beta_1\\E[\hat{\gamma}_2]=\beta_2\]</span> <span class="math inline">\(\hat{\gamma}_1\)</span> and <spanclass="math inline">\(\hat{\gamma}_2\)</span> are unbiased estimators of<span class="math inline">\(\beta_1\)</span> and <spanclass="math inline">\(\beta_2\)</span> in (1). So they areconsistent.</p><p>影响：引入无关变量后，受到无关变量的干扰，估计量<spanclass="math inline">\(\hat{\beta}\)</span>的方差一般会增大</p><h4 id="x有扰动项">X有扰动项</h4>Denote $$ =(<span class="math display">\[\begin{matrix}    0&amp;      \delta_{11}&amp;        \delta_{21}\\    0&amp;      \delta_{12}&amp;        \delta_{22}\\    ...&amp;        ...&amp;        ...\\    0&amp;      \delta_{1T}&amp;        \delta_{2T}\\\end{matrix}\]</span>)\ E[]=0\ X'=X+\<span class="math display">\[\begin{aligned}\beta&#39;=\left( \begin{array}{c}    \hat{\beta}_0\\    \hat{\beta}_1\\    \hat{\beta}_2\\\end{array} \right)&amp;=(X&#39;^\top X&#39;)^{-1}X&#39;^\top(X&#39;-\varDelta)\beta+(X&#39;^\top X&#39;)^{-1}X&#39;^\top\varepsilon\\&amp;=(I-(X&#39;^\topX&#39;)^{-1}X&#39;^\top\varDelta)\beta+(X&#39;^\topX&#39;)^{-1}X&#39;^\top \varepsilon\\&amp;=(I-([(X+\varDelta)^\top(X+\varDelta)]^{-1}(X+\varDelta)^\top\varDelta)\beta\\&amp;\quad+(X&#39;^\top X&#39;)^{-1}X&#39;^\top \varepsilon\\\end{aligned}\]</span>\ <span class="math display">\[Then\]</span><span class="math display">\[\begin{aligned}E[\beta&#39;]&amp;=[I-E[X^\top X+X^\top\varDelta+\varDelta^\topX+\varDelta^\top \varDelta)^{-1}(X^\top \varDelta+\varDelta^\top\varDelta)]]\beta\\&amp;\ne\beta\end{aligned}\]</span><p><span class="math display">\[Ms. Noisy cannot get unbiased estimators of $\beta_1$ and $\beta_2$ in(1). Since\]</span> E[X<sup>X+X</sup>+<sup>X+</sup>)<sup>{-1}(X</sup>+^)] $$ Shecannot get consistent estimators.</p><h2 id="评价指标">评价指标</h2><h3 id="r2"><span class="math inline">\(R^2\)</span></h3><h4 id="平方和分解公式-sum-of-squares-decomposition">平方和分解公式 Sumof squares decomposition</h4><p>成立条件：</p><p><span class="math inline">\(\hat{y}\)</span>和<spanclass="math inline">\(e\)</span>正交 <span class="math display">\[\hat{y}^\top e=(X\hat{\beta})^\top e=\hat{\beta}^\top X^\top e=0\]</span></p><p><span class="math display">\[y=\hat{y}+e\]</span></p><p>回归平方和：ESS (explained sum of squares)</p><p>残差平方和：RSS(residual sum of squares)</p><p>总离差平方和：TSS(total sum of squares)</p><p>ESS+RSS=TSS</p><p><span class="math display">\[ESS=\sum^n_{i=1}(\hat{y_i}-\bar{y})^2\]</span> <span class="math display">\[RSS=\sum^n_{i=1}(\hat{y_i}-y_i)^2\]</span> <span class="math display">\[TSS=\sum^n_{i=1}(y_i-\bar{y})^2\]</span></p><p><span class="math display">\[R^2=\frac{ESS}{TSS}=1-\frac{RSS}{TSS}=\frac{Var(\hat{y})}{Var(y)}=1-\frac{MSE}{Var(y)}\]</span></p><p><span class="math display">\[TSS=ESS+RSS\\\sum(y_i-\bar{y})^2=\sum (\hat{y}_i-\bar{y})^2+\sum e_i^2\]</span></p><h4 id="r2-1"><span class="math inline">\(R^2\)</span></h4><p>R²是指拟合优度，是回归直线对观测值的拟合程度。</p><p>R-square is used to evaluate the goodness of fit of a linearregression model. It is a value between 0 and 1 and itrepresents theproportion of the variation in the dependent variable that is explainedby the independent variables in the model.</p><p>因变量的变化有多少比例可以由自变量解释</p><h4 id="r2的缺陷"><span class="math inline">\(R^2\)</span>的缺陷</h4><ul><li>对异常值敏感（平方项）</li><li>对自变量个数敏感（增加个数会增大<spanclass="math inline">\(R^2\)</span>）</li><li>无法区分线性与非线性关系</li><li>无常数项时不用R²度量拟合优度（平方和分解公式不成立）</li></ul><h4 id="样本外r2为负数">样本外<spanclass="math inline">\(R^2\)</span>为负数</h4><p>RSS&gt;TSS</p><h4 id="调整r2很多变量">调整<spanclass="math inline">\(R^2\)</span>（很多变量）</h4><p>The adjusted R square takes into account the influence of sample sizeand the number of independent variables in the regression.</p><p><spanclass="math inline">\(R^2\)</span>问题：增加自变量的个数会使<spanclass="math inline">\(R^2\)</span>增大</p><p>解决方法：调整<span class="math inline">\(R^2\)</span> <spanclass="math display">\[R_{adj}=1-\frac{\frac{RSS}{n-p-1}}{\frac{TSS}{n-1}}\]</span> <span class="math inline">\(p\)</span>是自变量个数，<spanclass="math inline">\(n\)</span>是样本数量</p><p>问题：可能为负</p><h4 id="uncentered-r2">uncentered <spanclass="math inline">\(R^2\)</span></h4><p><span class="math display">\[R^2_{uc}=\frac{\sum \hat{y_i}^2}{\sum y_i^2}\]</span></p><p>来使<span class="math inline">\(R^2\)</span>在0到1之间</p><h3 id="t检验和f检验">t检验和F检验</h3><ul><li><p>t检验和F检验的关系：单个变量的t检验平方就是自由度为1的F检验（第一自由度为1，第二自由度为样本量）</p></li><li><p>F检验和R²的关系：R²是去掉所有解释变量，只保留常数项的F检验</p><p>对于线性回归方程<spanclass="math inline">\(y_i=\beta_1+\beta_2x_{i2}+...+\beta_{K}x_{iK}+\varepsilon_i\)</span>，检验原假设<spanclass="math inline">\(H_0:\beta_2=...=\beta_{K}=0\)</span>的F统计量等于<spanclass="math inline">\(\frac{R^2/(K-1)}{(1-R^2)/(n-K)}\)</span></p><ul><li>R²并非决定F统计量的唯一因素，还取决于样本容量n与解释变量个数K</li></ul></li></ul><h2 id="xy数量变化的影响">X、Y数量变化的影响</h2><h3 id="hatbeta"><span class="math inline">\(\hat{\beta}\)</span></h3><p><span class="math display">\[\hat{\beta}=(X^\top X)^{-1}X^\top Y\]</span></p><p>Y→a倍：<span class="math inline">\(\hat{\beta}\)</span>→a倍</p><p>X→a倍：<span class="math inline">\(\hat{\beta}\)</span>→1/a倍</p><p>样本复制一倍： <span class="math display">\[\left( \begin{matrix}    \left( \begin{array}{c}    X\\    X\\\end{array} \right)&amp;        \left( \begin{array}{c}    Y\\    Y\\\end{array} \right)\\\end{matrix} \right)\]</span> 则 <span class="math display">\[\begin{aligned}\hat{\beta}&amp;=\left( \begin{matrix}    \left( \begin{array}{c}    X\\    X\\\end{array} \right) ^{\top}&amp;        \left( \begin{array}{c}    X\\    X\\\end{array} \right)\\\end{matrix} \right) ^{-1}\left( \begin{array}{c}    X\\    X\\\end{array} \right) ^{\top}\left( \begin{array}{c}    Y\\    Y\\\end{array} \right) \\&amp;=\left( \begin{matrix}    \left( \begin{matrix}    X^{\top}&amp;       X^{\top}\\\end{matrix} \right)&amp;       \left( \begin{array}{c}    X\\    X\\\end{array} \right)\\\end{matrix} \right) ^{-1}\left( \begin{matrix}    X^{\top}&amp;       X^{\top}\\\end{matrix} \right)\left( \begin{array}{c}    Y\\    Y\\\end{array} \right)\\&amp;=(2X^\top Y)^{-1}(2X^\top Y)\\&amp;=(X^\top X)^{-1}X^\top Y\end{aligned}\]</span> 当样本复制一倍的时候，相当于<spanclass="math inline">\(X\)</span>矩阵变为了<spanclass="math inline">\(\left( \begin{array}{c}  X\\  X\\ \end{array}\right)\)</span>，相应的<span class="math inline">\(X^\topX\)</span>的值就变为原来的2倍。所以，在<spanclass="math inline">\(\sigma^2\)</span>已知的情况下，<spanclass="math inline">\(Var(\hat{\beta})=\sigma^2 (X^\topX)^{-1}\)</span>变为原来的<spanclass="math inline">\(\frac{1}{2}\)</span>。</p><h3 id="t检验值">t检验值</h3><p><span class="math display">\[t=\frac{\hat{\beta}-\beta}{SE(\hat{\beta})}\]</span></p><p>Y→a倍：t→不变</p><p>X→a倍：t→不变</p><p>样本复制一倍：t→<span class="math inline">\(\sqrt{2}\)</span>t</p><h3 id="r2-2"><span class="math inline">\(R^2\)</span></h3><p>Y→a倍：<span class="math inline">\(R^2\)</span>→不变</p><p>X→a倍：<span class="math inline">\(R^2\)</span>→不变</p><p>样本复制一倍：<span class="math inline">\(R^2\)</span>→不变【<spanclass="math inline">\(\hat{\beta}\)</span>不变，预测值不变】</p><h2 id="线性回归准备">线性回归准备</h2><ol type="1"><li><p>数据清洗和预处理：这涉及将数据从原始数据中提取出来，检查和处理数据中的缺失值、异常值和重复值。确保数据符合线性回归的前提假设。</p></li><li><p>可视化和描述性统计分析：通过图表和统计指标来理解数据的分布情况、变量之间的关系和异常值的存在。这些信息可以帮助确定应该使用哪种线性回归模型，以及哪些变量需要包含在模型中。</p></li><li><p>数据划分：将数据集划分为训练集和测试集，以便评估模型的性能并防止过度拟合。</p></li><li><p>特征工程：根据问题的要求，对数据进行特征工程，例如特征选择、特征变换和特征提取等，以便更好地表示数据和提高模型的性能。相关性分析</p></li><li><p>模型选择和训练：选择合适的线性回归模型，并对模型进行训练和调整参数，以便使模型在训练集上的性能最优。</p></li><li><p>Data cleaning and preprocessing: This involves extracting datafrom raw data, checking and handling missing values, outliers, andduplicates in the data. Ensure that the data meets the assumptions oflinear regression.</p></li><li><p>Visualization and descriptive statistical analysis: Understandthe distribution of data, the relationship between variables, and thepresence of outliers through charts and statistical indicators. Thisinformation can help determine which linear regression model to use andwhich variables to include in the model.</p></li><li><p>Data partitioning: Divide the data set into training and testsets to evaluate model performance and prevent overfitting.</p></li><li><p>Feature engineering: Perform feature engineering on the dataaccording to the requirements of the problem, such as feature selection,feature transformation, and feature extraction, to better represent thedata and improve the performance of the model. Analyzecorrelation.</p></li><li><p>Model selection and training: Choose the appropriate linearregression model and train and adjust the parameters of the model toachieve optimal performance on the training set.</p></li></ol><p>数据分布对线性回归模型的影响主要表现在以下两个方面：</p><h2 id="线性回归数据分布的影响">线性回归数据分布的影响</h2><ol type="1"><li><p>数据分布是否符合线性回归的基本假设</p><p>线性回归模型的基本假设是，自变量与因变量之间存在线性关系，且残差（实际值与预测值之间的差异）是独立同分布且服从正态分布的。如果数据分布不符合这些假设，那么线性回归模型的预测结果可能会受到影响。</p><p>例如，如果因变量的分布是偏态分布，那么线性回归模型可能会出现预测偏差，因为线性回归模型假设残差服从正态分布，而偏态分布的数据可能导致残差的非正态分布。同样，如果因变量与自变量之间的关系是非线性的，那么线性回归模型可能无法捕捉到这种关系，导致预测结果不准确。</p></li><li><p>数据分布的离群值和异常值</p><p>离群值和异常值可以对线性回归模型的预测结果产生很大的影响。因为线性回归模型是基于最小二乘法来求解的，它对离群值非常敏感，如果数据集中存在离群值，那么它们可能会对模型的系数估计和预测结果产生很大的影响。</p></li></ol>]]></content>
    
    
    <categories>
      
      <category>数学</category>
      
      <category>回归</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>统计</title>
    <link href="/2023/03/03/%E6%95%B0%E5%AD%A6/%E7%BB%9F%E8%AE%A1/"/>
    <url>/2023/03/03/%E6%95%B0%E5%AD%A6/%E7%BB%9F%E8%AE%A1/</url>
    
    <content type="html"><![CDATA[<h1 id="统计">统计</h1><h2 id="好的estimator">好的estimator</h2><ol type="1"><li>unbiased: on average they yield an estimate that equals the realparameter</li></ol><p><span class="math display">\[E[\mu]=\hat{\mu}\\E[\sigma^2]=\hat{\sigma}^2\]</span></p><ol start="2" type="1"><li>low variance: variance that is lower than any other possibleestimator</li></ol><p><span class="math display">\[   Var(\mu)=E[(\hat{\mu}-\mu)^2]\\   Var(\sigma^2)=E[(\hat{\sigma}^2-\sigma^2)^2]\\\]</span></p><h2id="样本方差的分母为什么是n-1而不是n">样本方差的分母为什么是n-1，而不是n？</h2><p><imgsrc="https://jkq-pic.oss-cn-shanghai.aliyuncs.com/img/20220912113632.png" /></p><p><imgsrc="https://jkq-pic.oss-cn-shanghai.aliyuncs.com/img/20220912113646.png" /></p><p><imgsrc="https://jkq-pic.oss-cn-shanghai.aliyuncs.com/img/20220912113657.png" /></p><p><imgsrc="https://jkq-pic.oss-cn-shanghai.aliyuncs.com/img/20220912113712.png" /></p><h2id="数据有偏差时相关统计量如何改变">数据有偏差时相关统计量如何改变</h2><p>[对一些非常经典的模型做出一定调整]</p><h2 id="p值">P值</h2><p>p值是在假定原假设为真时，得到与样本相同或者更极端的结果的概率。</p><p>P值很小则拒绝原假设</p><h2 id="第一类错误第二类错误">第一类错误，第二类错误</h2><ul><li>I：<spanclass="math inline">\(P(拒绝H_0|H_0)\)</span>，弃真，原假设为真但拒绝原假设</li><li>II：<spanclass="math inline">\(P(接受H_0|H_1)\)</span>，存伪，原假设伪假但接受原假设</li></ul><p>功效=1-第二类错误概率 <span class="math display">\[功效=1-P(接受H_0|H_1)=P(拒绝H_0|H_1)\]</span>知道第一类错误发生概率，不知道第二类错误发生概率，所以拒绝原假设比较理直气壮，接受原假设没有把握</p><ul><li>一、二类错误此消彼长，要都减少只能<strong>增加样本容量</strong></li><li>假设检验时一般指定可接受<strong>一类错误</strong>的最大概率（<strong>显著性水平</strong>）</li></ul><h2 id="mle">MLE</h2><p>缺点：</p><ul><li>数据量小性能差</li><li><spanclass="math inline">\(\theta\)</span>是个唯一值，没有别的参数可用，如果他错了那整个模型就错了</li></ul>]]></content>
    
    
    <categories>
      
      <category>数学</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>线代</title>
    <link href="/2023/03/03/%E6%95%B0%E5%AD%A6/%E7%BA%BF%E4%BB%A3/"/>
    <url>/2023/03/03/%E6%95%B0%E5%AD%A6/%E7%BA%BF%E4%BB%A3/</url>
    
    <content type="html"><![CDATA[<h1 id="线代">线代</h1><h2 id="特征向量和特征值">特征向量和特征值</h2><p>矩阵<span class="math inline">\(A\)</span>，向量<spanclass="math inline">\(v\)</span>，常量<spanclass="math inline">\(\lambda\)</span> <span class="math display">\[Av=\lambda v\]</span> 矩阵乘向量：把向量transform，改变向量的大小和方向</p><ul><li><p>特征向量：所有transform以后不改变方向，只改变大小的向量的集合</p></li><li><p>特征值：向量改变大小的程度<spanclass="math inline">\(\lambda\)</span></p></li></ul><h3 id="求特征值">求特征值</h3><p><span class="math display">\[Av=\lambda v\\\]</span> <span class="math display">\[Av=\lambda I v\\\]</span> <span class="math display">\[(A-\lambda I)v=0→两个矩阵都非满秩\\\]</span> <span class="math display">\[|A-\lambda I|=0\]</span></p><h2 id="pca">PCA</h2><p>可以将高维度数据转换为低维度的数据，同时尽可能地保留数据中的信息。</p><p>PCA的基本思想是，将原始数据映射到一个新的坐标系(Coordinatesystem)下，使得新坐标系的第一维度上数据的方差最大，第二维度上数据的方差次大，以此类推</p><p>特征向量给了我们它被拉伸或转化的方向，而特征值是它被拉伸的数值</p><p>协方差矩阵：有助于比较一只股票从其平均值的变动是如何依赖于另一只股票从其平均值的变动</p><p>协方差矩阵的特征值之和大约等于我们原始矩阵的总方差之和</p><h3 id="步骤">步骤</h3><ol type="1"><li>标准化</li><li>计算协方差矩阵的特征值特征向量</li><li>对原来的矩阵乘特征向量矩阵</li></ol><p>假设我们有一个n个样本、m个特征的数据矩阵<span class="math inline">\(X\in \mathbb{R}^{n \timesm}\)</span>，其中每行表示一个样本，每列表示一个特征。PCA的目标是找到一个由k个基向量构成的新坐标系，使得在这个新坐标系下，数据的方差最大化，即找到一个线性变换<spanclass="math inline">\(W \in \mathbb{R}^{m \timesk}\)</span>，使得映射后的数据矩阵<span class="math inline">\(Z \in\mathbb{R}^{n \timesk}\)</span>的方差最大化。具体地，我们可以按照以下步骤进行计算：</p><ol type="1"><li><p>数据预处理：对原始数据进行标准化处理，使得各个特征的均值为0，方差为1。标准化后的数据矩阵为：</p><p><span class="math display">\[\tilde{X}=\frac{X-\mu}{\sigma}\]</span></p><p>其中<spanclass="math inline">\(\mu\)</span>是每个特征的均值向量，<spanclass="math inline">\(\sigma\)</span>是每个特征的标准差向量。</p></li><li><p>计算协方差矩阵：将标准化后的数据按列组成一个矩阵，计算其协方差矩阵。</p><p>协方差矩阵为： <span class="math display">\[\varSigma=\frac{1}{n}\tilde{X}^\top \tilde{X}\]</span></p></li><li><p>计算特征值和特征向量：对协方差矩阵进行特征值分解，得到特征值和对应的特征向量。</p><p>协方差矩阵的特征值和特征向量为： <span class="math display">\[\varSigma \mathbf{v_i}=\lambda_i\mathbf{v_i}\]</span> 其中<spanclass="math inline">\(\lambda_i\)</span>表示第i个特征向量对应的特征值，<spanclass="math inline">\(\mathbf{v}_i\)</span>表示第i个特征向量。</p></li><li><p>选取主成分：将特征值按大小排序，选取前k个特征值对应的特征向量作为新坐标系的基向量，这k个特征向量就是数据的主成分。</p><p>我们将前k个特征向量按列组成一个矩阵<span class="math inline">\(W_k\in \mathbb{R}^{m \times k}\)</span>，则映射后的数据矩阵为： <spanclass="math display">\[Z=\tilde{X}W_k\]</span></p></li><li><p>转换数据：将原始数据映射到新坐标系下，得到降维后的数据。具体地，将每个样本点投影到新坐标系上，得到它在新坐标系下的坐标。映射后的数据矩阵<spanclass="math inline">\(Z\)</span>的第i行表示原始数据矩阵<spanclass="math inline">\(X\)</span></p></li></ol><h3 id="pca的缺点和解决方法">PCA的缺点和解决方法</h3><ol type="1"><li>可能会导致信息损失，尤其是在保留的主成分数量较少的情况下，会丢失数据的一些细节信息。解决方法：可以通过增加主成分的数量来降低信息损失，但需要考虑到维数灾难和计算复杂度的问题。同时，可以采用其他降维算法，如t-SNE等，来综合考虑数据的全局信息和局部结构。</li><li>对于非线性数据，PCA的效果可能不好。 解决方法：可以采用核PCA（KernelPCA）来处理非线性数据，该方法通过使用核函数将数据映射到高维空间，然后再进行PCA降维。【在将数据映射到高维空间后，再使用PCA来降维，从而提取数据的主要特征。在核PCA中，使用核函数（如多项式核函数、高斯核函数等）将原始数据映射到高维空间，然后在高维空间中计算数据的协方差矩阵，最后通过对协方差矩阵进行特征值分解，得到主成分。与传统PCA相比，核PCA的主成分是在高维空间中计算得到的，可以更好地捕捉数据的非线性结构。同时，核PCA的计算复杂度较高，但可以通过使用矩阵的特殊性质和随机采样等方法来加速计算。核PCA可以应用于各种数据降维和特征提取的任务中，如图像处理、模式识别、信号处理等。】</li><li>对于非高斯分布的数据，PCA的效果也可能不佳。解决方法：可以采用基于独立成分分析（Independent Component Analysis,ICA）的降维方法，该方法能够处理非高斯分布的数据，并将数据分解为独立的成分。</li><li>需要对数据进行标准化处理，否则会出现主成分不一致的问题。解决方法：可以通过对数据进行标准化处理，使得每个特征的均值为0，方差为1，从而消除主成分不一致的问题。同时，可以采用其他方法来处理非标准化数据，如SVD等。</li><li>主成分各个特征维度的含义具有一定的模糊性，不如原始样本特征的解释性强</li></ol><h2 id="方差协方差矩阵">方差、协方差矩阵</h2><p>1列为1个变量，1行为1个样本：（m行n列） <span class="math display">\[Cov(X)=E[(X-E(X))^\top (X-E(X))]\\\]</span> <span class="math display">\[或Cov(X)=\frac{1}{m}(X-E(X))^\top (X-E(X))\\\]</span> <span class="math display">\[Var(X)=(X-E(X))^\top (X-E(X))\]</span> E(X)=0时： <span class="math display">\[Cov(X)=E[X^\top X]\\\]</span> <span class="math display">\[Var(X)=X^\top X\]</span> 协方差矩阵是<strong>半正定</strong>矩阵：</p><ul><li>任意非零向量<span class="math inline">\(x\)</span>，有<spanclass="math inline">\(x^\top \Sigma x\geqslant 0\)</span></li></ul><h2 id="cholesky分解">cholesky分解</h2><p>正定矩阵 <span class="math display">\[A=LL^\top，L是下三角矩阵\]</span></p><h3id="蒙特卡洛模拟whiteningcholesky分解">蒙特卡洛模拟whitening（cholesky分解）</h3><p>行：</p><p><spanclass="math inline">\(X\)</span>是每一行为一个随机变量的矩阵，协方差矩阵为<spanclass="math inline">\(M\)</span>，均值为0，则 <spanclass="math display">\[M=E[XX^\top]\]</span> 因为<spanclass="math inline">\(M\)</span>是对称的半正定矩阵，有平方根<spanclass="math inline">\(M^\frac{1}{2}\)</span>（不一定要对称），满足<spanclass="math inline">\(M^\frac{1}{2}(M^\frac{1}{2})^\top=M\)</span>。如果<spanclass="math inline">\(M\)</span>是正定的，<spanclass="math inline">\(M^\frac{1}{2}\)</span>可逆。矩阵<spanclass="math inline">\(Y=M^{-\frac{1}{2}}X\)</span>有协方差矩阵：</p><p><span class="math display">\[\begin{aligned}Cov(Y)&amp;=E[YY^\top]\\&amp;=M^{-\frac{1}{2}}E[XX^\top] (M^{-\frac{1}{2}})^\top\\&amp;=M^{-\frac{1}{2}}M(M^{-\frac{1}{2}})^\top\\&amp;=M^{-\frac{1}{2}}(M^{\frac{1}{2}}(M^{\frac{1}{2}})^\top)(M^{-\frac{1}{2}})^\top\\&amp;=(M^{-\frac{1}{2}}M^{\frac{1}{2}})(M^{-\frac{1}{2}}M^{\frac{1}{2}})^\top\\&amp;=I\end{aligned}\]</span></p><p>其中<spanclass="math inline">\(M^\frac{1}{2}\)</span>可以通过cholesky分解得到<spanclass="math inline">\(L\)</span>，<spanclass="math inline">\(M=LL^\top\)</span>。如果<spanclass="math inline">\(M\)</span>是正定的，<spanclass="math inline">\(L\)</span>可逆。矩阵<spanclass="math inline">\(Y=L^{-1}X\)</span>有协方差矩阵： <spanclass="math display">\[\begin{aligned}Cov(Y)&amp;=E[YY^\top]\\&amp;=L^{-1}E[XX^\top] (L^{-1})^\top\\&amp;=L^{-1}M(L^{-1})^\top\\&amp;=L^{-1}(LL^\top)(L^{-1})^\top\\&amp;=(L^{-1}L)(L^{-1}L)^\top\\&amp;=I\end{aligned}\]</span> 列：</p><p><spanclass="math inline">\(X\)</span>是每一列为一个随机变量的矩阵，协方差矩阵为<spanclass="math inline">\(M\)</span>，均值为0，则 <spanclass="math display">\[M=E[X^\top X]\]</span> 因为<spanclass="math inline">\(M\)</span>是对称的半正定矩阵，有平方根<spanclass="math inline">\(M^\frac{1}{2}\)</span>（不一定要对称），满足<spanclass="math inline">\(M^\frac{1}{2}(M^\frac{1}{2})^\top=M\)</span>。如果<spanclass="math inline">\(M\)</span>是正定的，<spanclass="math inline">\(M^\frac{1}{2}\)</span>可逆。矩阵<spanclass="math inline">\(Y=X(M^{-\frac{1}{2}})^\top\)</span>有协方差矩阵：<span class="math display">\[\begin{aligned}Cov(Y)&amp;=E[Y^\top Y]\\&amp;=M^{-\frac{1}{2}} E[X^\top X] (M^{-\frac{1}{2}})^\top\\&amp;=M^{-\frac{1}{2}} M(M^{-\frac{1}{2}})^\top\\&amp;=M^{-\frac{1}{2}}(M^{\frac{1}{2}}(M^{\frac{1}{2}})^\top)(M^{-\frac{1}{2}})^\top\\&amp;=(M^{-\frac{1}{2}}M^{\frac{1}{2}})(M^{-\frac{1}{2}}M^{\frac{1}{2}})^\top\\&amp;=I\end{aligned}\]</span></p><p>其中<spanclass="math inline">\(M^\frac{1}{2}\)</span>可以通过cholesky分解得到<spanclass="math inline">\(L\)</span>，<spanclass="math inline">\(M=LL^\top\)</span>。如果<spanclass="math inline">\(M\)</span>是正定的，<spanclass="math inline">\(L\)</span>可逆。矩阵<spanclass="math inline">\(Y=X(L^{-1})^\top\)</span>有协方差矩阵：</p><p><span class="math display">\[\begin{aligned}Cov(Y)&amp;=E[Y^\top Y]\\&amp;=L^{-1}E[X^\top X] (L^{-1})^\top\\&amp;=L^{-1}M(L^{-1})^\top\\&amp;=L^{-1}(LL^{\top})(L^{-1})^{\top}\\&amp;=(L^{-1} L)(L^{-1} L)^{\top}\\&amp;=I\end{aligned}\]</span></p>]]></content>
    
    
    <categories>
      
      <category>数学</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>基础知识</title>
    <link href="/2023/03/03/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    <url>/2023/03/03/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</url>
    
    <content type="html"><![CDATA[<h1 id="基础知识">基础知识</h1><h2 id="bit-byte">bit &amp; byte</h2><ul><li>bit：表示信息的最小单位</li><li>byte：8bit，表示256个数字。1个byte表示一个数据/字母，2个byte表示一个汉字</li></ul><p>数据存储是以Byte为单位，数据传输大多是以bit为单位</p>]]></content>
    
    
    <categories>
      
      <category>计算机</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Python</title>
    <link href="/2023/03/03/%E8%AE%A1%E7%AE%97%E6%9C%BA/Python/"/>
    <url>/2023/03/03/%E8%AE%A1%E7%AE%97%E6%9C%BA/Python/</url>
    
    <content type="html"><![CDATA[<h1 id="python">Python</h1><h2 id="垃圾回收机制">垃圾回收机制</h2><p>Python的垃圾回收机制主要是通过引用计数（referencecounting）来实现的。</p><p>当一个对象被创建时，会为该对象分配一块内存，并将该对象的引用计数设置为1。当其他对象引用该对象时，该对象的引用计数会相应地增加。当一个对象的引用计数变为0时，即没有任何对象引用该对象，Python的垃圾回收机制就会将该对象所占用的内存空间释放掉。</p><p>除了引用计数，Python还实现了一些辅助垃圾回收机制，如标记-清除（markand sweep）和分代回收（generational collection）。</p><p>标记-清除是一种在引用计数无法处理某些对象时使用的机制。该机制通过遍历所有的对象，将所有仍然被引用的对象标记，然后清除所有未标记的对象来释放内存空间。</p><p>分代回收机制则是一种通过将对象分为几个代（generation），然后针对不同代采取不同的垃圾回收策略的机制。Python中一般将新建的对象放到第0代，如果在垃圾回收后仍然存活，则将其转移到更高一代。这种机制可以提高垃圾回收的效率和性能。</p><h2 id="量化常用python技巧">量化常用Python技巧</h2><p>Pandas.Series 对象、向量操作、布尔索引、rolling/apply函数、缺失/异常值处理、Numpy 的常用函数等等</p><h2 id="单下划线和双下划线">单下划线和双下划线</h2><p>1、__name__：一种约定，Python内部的名字，用来与用户自定义的名字区分开，防止冲突。</p><p>2、_name：一种约定，用来指定变量私有。</p><p>3、__name：解释器用_classname__name来代替这个名字用以区别和其他类相同的命名。</p><h2 id="魔术方法">魔术方法</h2><p>Python中的"dunder"是指"double underscore"，而"dundermethod"则是指名称以双下划线开头和结尾的特殊方法（例如<code>__init__</code>, <code>__str__</code>,<code>__add__</code>）。这些方法也被称为"magic methods"或"specialmethods"。这些魔术方法使得我们可以更方便地创建、操作和比较对象，提高了代码的可读性和可维护性。同时，魔术方法也使得Python类的使用更加接近自然语言的表达方式，从而提高了开发效率。</p><p>在Python中，dunder方法用于定义对象和类在不同情境下的行为。例如，<code>__init__</code>方法用于定义对象的初始化方式，<code>__str__</code>方法用于定义对象的字符串表示，<code>__add__</code>方法用于定义两个对象相加的行为。</p><p>Python会在特定情境下隐式调用dunder方法，例如当你使用"+"运算符将两个对象相加时，Python会寻找<code>__add__</code>方法来确定如何将这两个对象相加。</p><p>使用dunder方法的优点包括：</p><ol type="1"><li>一致性：dunder方法采用一致的命名规则，使人们易于理解它们的工作原理和用途。</li><li>可读性：通过实现dunder方法，你可以使你的代码更加可读和表达性更强。例如，为类定义一个<code>__str__</code>方法，可以定制对象作为字符串的表示方式。</li><li>定制化：dunder方法使你能够在不同情境下定制对象的行为。例如，通过定义<code>__len__</code>方法，你可以指定对象的长度，这对于实现自定义容器非常有用。</li><li>兼容性：通过实现合适的dunder方法，你可以使你的对象与Python内置函数和库兼容，从而节省时间和精力。</li></ol><p>The main purpose of dunder methods in Python is to enable custombehavior for objects in various contexts, such as arithmetic operations,string representation, comparison, and attribute access. Dunder methodsare essential for implementing many of the built-in features of Python,such as operator overloading, slicing, iteration, and contextmanagers.</p><p>The advantages of using dunder methods include:</p><ol type="1"><li>Consistency: Dunder methods follow a consistent naming convention,making it easy to understand how they work and where they are used.</li><li>Readability: By implementing dunder methods, you can make your codemore readable and expressive. For example, defining a<code>__str__</code> method for a class allows you to customize how theobject is represented as a string.</li><li>Customization: Dunder methods enable you to customize the behaviorof your objects in various contexts. For example, by defining a<code>__len__</code> method, you can specify the length of an object,which can be useful for implementing custom containers.</li><li>Compatibility: By implementing the appropriate dunder methods, youcan make your objects compatible with built-in Python functions andlibraries, which can save you time and effort.</li></ol><h2 id="array和list的区别">Array和list的区别</h2><ol type="1"><li>类型：<code>list</code>是Python内置的数据类型，可以存储任何类型的数据。而<code>array</code>是Python标准库中的一个模块，用于存储同一种类型的数据，如整数、浮点数等。</li><li>存储方式：<code>list</code>中存储的是对象的引用，即每个元素都是一个指针，指向真正的对象。而<code>array</code>中存储的是同一种类型的数据，直接存储在连续的内存空间中。</li><li>访问速度：由于<code>array</code>中存储的是同一种类型的数据，所以访问速度比<code>list</code>更快。在对数据进行大量计算时，使用<code>array</code>可以提高程序的效率。</li><li>功能：<code>list</code>提供了更多的内置方法和操作，如添加、删除、排序等。而<code>array</code>相对简单，提供了基本的访问和修改方法。</li></ol>]]></content>
    
    
    <categories>
      
      <category>计算机</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>C++</title>
    <link href="/2023/03/03/%E8%AE%A1%E7%AE%97%E6%9C%BA/C++/"/>
    <url>/2023/03/03/%E8%AE%A1%E7%AE%97%E6%9C%BA/C++/</url>
    
    <content type="html"><![CDATA[<h1 id="c">C++</h1><h2 id="封装继承多态">封装、继承、多态</h2><ul><li><strong>Encapsulation</strong>: Binding the data and function ;allow the user to hide the information for outside world and doesn'tallow the other user to change or modify the internal values of class.Can split a large program into a number of smaller, independent parts toreduce complexity. In a sentence, encapsulation is hiding theimplementation details of a module from its user. Class is a type ofencapsulation and abstraction.</li><li><strong>Inheritance</strong>: Classes are created in hierarchies,allows the structure and methods in one class to be passed down thehierarchy. That means less programming is required when adding functionsto complex systems.</li><li><strong>Polymorphism</strong>: Single name can have multiplemeanings, depends on the situation. Enable one entity to be used as ageneral category for different types of actions.</li></ul><h2 id="虚函数">虚函数</h2><p>类函数，子类可以各有各的实现，子类的虚函数还是虚函数</p><p>在运行期间确定对象实际类型</p><p>当一个类带有虚函数时，编译系统会为该类构造一个虚函数表（位于类内其他成员前面），是一个指针数组，存放每个虚函数的入口地址。系统在进行动态联编的时间开销很少，提高了多态性的效率</p><p>动态联编：对象指针通过虚指针找到虚表，从虚表中查找对应的虚函数地址进行调用</p><p>Deciding which virtual function to use is called dynamic binding</p><ul><li><p>The <strong>static type</strong> of an expression is always knownat compile time—it is the type with which a variable is declared or thatan expression yields.</p></li><li><p>The <strong>dynamic type</strong> is the type of the object inmemory that the variable or expression represents. The dynamic type maynot be known until run time.</p></li></ul><h2 id="变量">变量</h2><p>A memory location to store data for a program</p><h2 id="对象">对象</h2><p>对象是一个变量，可以包含其他变量和函数。C++中的对象可以定义为类的实例，类是一种用户定义的数据类型，其中包含属性和方法（函数）。对象是类的实例化，通过使用类定义来创建。</p><p>It is a variable that can contain other variables and functions.Objects in C++ can be defined as instances of a class, which is auser-defined data type that contains attributes and methods.</p><h2 id="指针-引用">指针 &amp; 引用</h2><p>指针是一个变量，它存储另一个变量的地址。我们可以通过指针访问和修改存储在另一个变量的地址处的值。指针通常用于动态分配内存、在函数之间传递参数和处理数组。</p><p>引用是一个别名，它给一个变量起了另一个名称。与指针不同，引用本身不是一个对象，而只是一个已经存在对象的别名。当我们使用引用时，实际上是在使用与引用绑定的变量。引用通常用于避免复制大型对象、创建函数的别名参数和使代码更易读。</p><p>A pointer is a variable that stores the address of another variable.We can access and modify the value stored at the address of anothervariable using a pointer. Pointers are typically used for dynamic memoryallocation, passing parameters between functions, and processingarrays.</p><p>A reference is an alias that gives another name to a variable. Unlikea pointer, a reference itself is not an object, but rather just an aliasfor an existing object. When we use a reference, we are actually usingthe variable that is bound to the reference. References are typicallyused to avoid copying large objects, create alias parameters forfunctions, and make code more readable.</p><h2 id="内存管理">内存管理</h2><ol type="1"><li><p>栈内存管理：C++通过栈来管理函数调用时的局部变量。每当函数被调用时，编译器会自动在栈上为其分配一段内存，当函数执行完毕后，这段内存会自动被释放。栈内存的大小是固定的，因此只适合存储大小已知且较小的数据。</p></li><li><p>堆内存管理：在C++中，我们可以使用new和delete关键字来动态地分配和释放堆内存。堆内存是由程序员手动申请的，在程序运行时动态分配和释放。由于堆内存的大小是动态变化的，因此适合存储较大的数据结构和对象。但是，需要注意的是，在使用完堆内存后，程序员需要手动释放这段内存，否则可能会造成内存泄漏的问题。</p></li><li><p>Stack memory management: C++ manages local variables duringfunction calls via the stack. Each time a function is called, thecompiler automatically allocates a block of memory on the stack, and thememory is automatically deallocated when the function completes. Thesize of the stack memory is fixed and is suitable for storing small datathat has a known size.</p></li><li><p>Heap memory management: In C++, we can use the keywords "new" and"delete" to dynamically allocate and deallocate memory on the heap. Heapmemory is allocated and deallocated manually by programmers duringruntime. Since the size of heap memory is dynamic, it is suitable forstoring larger data structures and objects. However, it is important tonote that after using heap memory, programmers must manually release thememory to avoid memory leaks.</p></li></ol><h3 id="栈和堆">栈和堆</h3><p>在C++中，栈（stack）和堆（heap）是两种内存分配方式。栈是由系统自动分配和释放的一块连续内存区域，用于存储函数调用时的局部变量、函数参数以及函数调用的返回地址等。栈的大小是固定的，它的管理方式是先进后出（LIFO）。这意味着最后被分配的内存先被释放，栈上的数据访问速度比堆要快，但是它的大小受限，不适合存储大量的数据。</p><p>堆是由程序员手动申请和释放的一块内存区域，它的大小不是固定的。程序员可以在运行时动态地分配和释放堆内存，用于存储较大的数据结构和对象等。堆的管理方式是任意的，它不像栈那样有固定的大小和访问顺序。在堆上分配内存需要显式地使用new操作符，而释放内存则需要使用delete操作符。如果程序员不释放已经分配的堆内存，就会导致内存泄漏的问题，因为堆上分配的内存不会自动释放。</p><p>需要注意的是，栈和堆的使用场景不同，程序员需要根据需要选择合适的内存分配方式。一般来说，较小的变量可以放在栈上，较大的数据结构和对象则需要放在堆上。同时，C++中也提供了一些辅助管理内存的工具，比如智能指针等。</p><p>In C++, the stack and the heap are two different memory allocationmechanisms. The stack is a contiguous block of memory that isautomatically allocated and deallocated by the system, used to storelocal variables, function parameters, and return addresses duringfunction calls. The size of the stack is fixed, and it is managed in alast-in, first-out (LIFO) manner. This means that the most recentlyallocated memory is released first. Accessing data on the stack isfaster than the heap, but the stack has limited size and is not suitablefor storing a large amount of data.</p><p>The heap is a block of memory that is manually allocated anddeallocated by the programmer, and its size is not fixed. Programmerscan dynamically allocate and deallocate memory on the heap duringruntime, used to store large data structures and objects. The managementof the heap is arbitrary, and it does not have a fixed size or accessorder like the stack. Allocating memory on the heap requires explicituse of the "new" operator, while releasing memory requires the "delete"operator. If the programmer does not release the allocated heap memory,it can cause memory leaks because the memory allocated on the heap isnot automatically released.</p><p>It should be noted that the stack and the heap have different usecases, and programmers need to choose the appropriate memory allocationmechanism based on their needs. Generally, smaller variables can beplaced on the stack, while larger data structures and objects should beplaced on the heap. Additionally, C++ provides some tools to assist withmemory management, such as smart pointers.</p><h3 id="内存泄漏">内存泄漏</h3><p>Memory leak would happen if the pointer is dereferenced while thememory of the dynamic variable that the pointer used to point to is notfreed.</p><h2 id="浅拷贝-vs-深拷贝">浅拷贝 vs 深拷贝</h2><ul><li><p>赋值：得到对象地址，共用对象内容，所有变量都会随之变化</p></li><li><p>浅拷贝：拷贝一层，更改<strong>引用类型</strong>（对象、数组都是引用类型）的<strong>数据</strong>时，拷贝的对象还是能被影响，如果拷贝的对象里还有子对象的话，那子对象拷贝其是也只是得到一个地址指向而已</p></li><li><p>深拷贝：递归地拷贝，更改引用类型原对象也不变</p></li><li><p>Assignment: Obtain the object address, share the object content,and all variables will change accordingly</p></li><li><p>Shallow copy: When copying a layer of data of a reference type(both objects and arrays are reference types), the copied object canstill be affected. If there are any sub objects in the copied object,the sub object copy is only to obtain an address point</p></li><li><p>Deep copy: copy recursively, changing the reference type andkeeping the original object unchanged</p></li></ul><p>浅拷贝创建一个新对象，该对象存储对原始对象相同内存位置的引用。换句话说，新对象只是指向原始对象内存位置的指针。对新对象进行的任何更改也会反映在原始对象上，反之亦然。这意味着浅拷贝并不创建原始对象的新实例，而只是创建对同一实例的新引用。当我们需要快速创建对象的副本而不在内存中创建新对象实例时，通常使用浅拷贝。</p><p>深拷贝则创建一个新对象并为其所有内容分配内存。在这种情况下，新对象与原始对象完全独立，对新对象进行的任何更改都不会影响原始对象。当我们需要修改对象的副本而不影响原始对象时，通常使用深拷贝。</p><p>A shallow copy creates a new object that stores references to thesame memory locations as the original object. In other words, the newobject is just a pointer to the original object's memory locations. Anychanges made to the new object will also be reflected in the originalobject, and vice versa. This means that shallow copy does not create anew instance of the original object, but rather a new reference to thesame instance. Shallow copy is often used when we need to create a copyof an object quickly, without creating a new object instance inmemory.</p><p>A deep copy, on the other hand, creates a new object and allocatesmemory for all its contents. In this case, the new object is completelyindependent of the original object, and any changes made to the newobject will not affect the original object. Deep copy is often used whenwe need to modify a copy of an object without affecting the originalobject.</p><h2 id="类继承">类继承</h2><p>Public, Protected and Private</p><ul><li><p>private:</p><ul><li><p>It could only be accessed by the function of this class and itsfriend function.</p></li><li><p>The object of this class type can’t access directly</p></li></ul></li><li><p>protected:</p><ul><li><p>It could be accessed by the function of this class, child classand its friend function.</p></li><li><p>The object of this class type can’t access directly</p></li></ul></li><li><p>public:</p><ul><li><p>It could be accessed by the function of this class, child classand its friend function.</p></li><li><p>The object of this class type can access directly</p></li></ul></li></ul><p>Friend function:</p><ul><li><p>Friend function globally</p></li><li><p>Friend class’s member function</p></li></ul><h2 id="静态成员">静态成员</h2><p><strong>Static member variables</strong></p><ul><li><p>One variable that is shared by all the objects of aclass</p></li><li><p>Can be used for objects of the class to communicate with eachother or coordinate their actions.</p></li><li><p>Can be private so that only objects of the class can directlyaccess it.</p></li><li><p>Useful for "tracking"</p><ul><li>How often a member function is called</li><li>How many objects exist at given time</li><li>Place keyword static before type</li></ul></li></ul><p><strong>Member functions</strong> can be static</p><ul><li><p>Does not access the data of any object</p></li><li><p>you want the function to be a member of the class. Make it astatic function</p></li></ul><h2 id="友元">友元</h2><p>友元（friend）是一种允许非成员函数或非该类成员函数访问类的私有成员的机制。一个类可以将其他类或函数声明为友元，从而使这些友元能够访问它的私有成员。</p><p>友元可以在类的定义中声明，也可以在类的外部进行声明。声明为类的友元的函数可以访问该类的私有成员变量和私有成员函数。</p><p>使用友元的主要目的是提高程序的灵活性和可维护性。通过将需要访问类的私有成员的函数声明为友元，可以避免暴露这些成员或使用公共接口进行访问。这有助于减少意外的错误，并允许更精细地控制类的封装性。</p><p>A friend is a mechanism that allows non-member functions ornon-member functions of another class to access the private members of aclass. A class can declare other classes or functions as friends,enabling those friends to access its private members.</p><p>Friends can be declared within the class definition or outside of it.Functions declared as friends of a class can access that class's privatemember variables and private member functions.</p><p>The primary purpose of using friends is to increase the flexibilityand maintainability of programs. By declaring functions that need toaccess a class's private members as friends, you can avoid exposingthose members or accessing them through a public interface. This helpsto reduce unintended errors and allows for finer control over theencapsulation of a class.</p><h2 id="python-vs-c">Python vs C++</h2><ol type="1"><li><p>类型：C++是一种静态类型语言，这意味着变量必须在编译时声明其类型，并且在程序运行时无法更改类型。Python是一种动态类型语言，这意味着变量的类型在程序运行时可以更改，并且无需显式声明。</p></li><li><p>性能：由于C++是一种编译语言，它通常比Python更快。Python是一种解释型语言，通常比C++慢一些。但是，Python有许多用于优化性能的工具和库。</p></li><li><p>内存管理：在C++中，程序员必须手动管理内存，包括分配和释放内存。在Python中，内存管理是由解释器自动处理的，有垃圾回收机制。</p></li><li><p>python对函数的参数类型与返回值类型没有严格限定，c++有</p></li><li><p>python不需要定义变量就能使用，c++需要先定义才能使用</p></li><li><p>python中的变量作用域更广，c++中的作用域更严格</p><p>python中循环内部定义的变量在外面可以用，但是c++不行</p></li><li><p>Type: C++ is a statically typed language, which means thatvariables must declare their type at compile-time and cannot change typeat runtime. Python is a dynamically typed language, which means thatvariables can change type at runtime and do not need to be explicitlydeclared.</p></li><li><p>Performance: Because C++ is a compiled language, it is generallyfaster than Python. Python is an interpreted language and is generallyslower than C++. However, Python has many tools and libraries foroptimizing performance.</p></li><li><p>Memory management: In C++, programmers must manually managememory, including allocating and freeing memory. In Python, memorymanagement is handled automatically by the interpreter, has garbagecollection.</p></li></ol><h2 id="python为什么比c慢">Python为什么比C++慢</h2><p>Python is interpreted and executed sentence by sentence, with a highdegree of abstraction. C++ is compiled first and converted to machinecode, without dynamic typing or dynamic checking.</p><h2 id="c11新特性">C++11新特性</h2><ol type="1"><li><p>auto关键字：可以用于自动推断变量类型，减少重复代码，提高代码可读性。</p></li><li><p>nullptr关键字：代表空指针，替代了NULL和0。</p></li><li><p>Range-basedfor循环：一种新的循环语法，可以用：遍历数组、向量、列表等容器中的元素。</p></li><li><p>Lambda表达式（lambdaexpression）：一种轻量级的匿名函数（anonymousfunction），可以在需要时动态创建，减少了代码量。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">auto</span> lambda = [](<span class="hljs-type">int</span> a, <span class="hljs-type">int</span> b) &#123; <span class="hljs-keyword">return</span> a + b; &#125;;<br>std::cout &lt;&lt; <span class="hljs-built_in">lambda</span>(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>) &lt;&lt; std::endl; <span class="hljs-comment">// 输出3</span><br></code></pre></td></tr></table></figure></li><li><p>Move语义：通过std::move函数，可以将资源的所有权从一个对象移动到另一个对象，减少了不必要的拷贝构造函数和析构函数的调用，提高了程序效率。</p></li><li><p>右值引用：提供了一种新的引用类型，可以绑定到临时对象和表达式上，使得临时对象的创建和销毁更加高效。</p></li><li><p>列表初始化：一种新的初始化语法，可以通过{}对列表进行初始化，使得代码更加简洁和易于理解。</p></li><li><p>智能指针：提供了一种管理动态内存的方式，可以自动进行内存释放，避免内存泄漏和悬空指针。std::unique_ptr、std::shared_ptr和std::weak_ptr</p></li><li><p>constexpr</p><p>constexpr修饰的是真正的常量，它会在编译期间就会被计算出来，整个运行过程中都不可以被改变，constexpr可以用于修饰函数，这个函数的返回值会尽可能在编译期间被计算出来当作一个常量，但是如果编译期间此函数不能被计算出来，那它就会当作一个普通函数被处理</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><br><span class="hljs-function"><span class="hljs-keyword">constexpr</span> <span class="hljs-type">int</span> <span class="hljs-title">func</span><span class="hljs-params">(<span class="hljs-type">int</span> i)</span> </span>&#123;<br>   <span class="hljs-keyword">return</span> i + <span class="hljs-number">1</span>;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br>   <span class="hljs-type">int</span> i = <span class="hljs-number">2</span>;<br>   <span class="hljs-built_in">func</span>(i);<span class="hljs-comment">// 普通函数</span><br>   <span class="hljs-built_in">func</span>(<span class="hljs-number">2</span>);<span class="hljs-comment">// 编译期间就会被计算出来</span><br>&#125;<br></code></pre></td></tr></table></figure></li><li><p>final &amp;override：final修饰一个类或虚函数，表示禁止该类进一步派生和虚函数的进一步重载</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">struct</span> <span class="hljs-title class_">Base</span> <span class="hljs-keyword">final</span> &#123;<br>    <span class="hljs-function"><span class="hljs-keyword">virtual</span> <span class="hljs-type">void</span> <span class="hljs-title">func</span><span class="hljs-params">()</span> </span>&#123;<br>        cout &lt;&lt; <span class="hljs-string">&quot;base&quot;</span> &lt;&lt; endl;<br>    &#125;<br>&#125;;<br><br><span class="hljs-keyword">struct</span> <span class="hljs-title class_">Derived</span> : <span class="hljs-keyword">public</span> Base&#123; <span class="hljs-comment">// 编译失败，final修饰的类不可以被继承</span><br>    <span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">func</span><span class="hljs-params">()</span> <span class="hljs-keyword">override</span> </span>&#123;<br>        cout &lt;&lt; <span class="hljs-string">&quot;derived&quot;</span> &lt;&lt; endl;<br>    &#125;<br><br>&#125;;<br></code></pre></td></tr></table></figure><p>override用于修饰派生类中的成员函数，标明该函数重写了基类函数，如果一个函数声明了override但父类却没有这个虚函数，编译报错，使用override关键字可以避免开发者在重写基类函数时无意产生的错误</p></li></ol><h3 id="智能指针">智能指针</h3><h4 id="unique_ptr">unique_ptr</h4><p>unique_ptr是一种独占式智能指针，它拥有被指向对象的独占权，不能被拷贝，只能移动。当unique_ptr被销毁时，它所指向的对象也将被销毁。unique_ptr适用于管理单个对象的所有权，是一种安全、高效的内存管理方式。</p><p>unique_ptr的主要作用是：</p><ul><li>管理动态分配的内存对象的生命周期，可以确保内存的正确释放。</li><li>通过移动语义，实现所有权的转移，防止悬空指针的出现，避免内存泄漏。</li></ul><h4 id="shared_ptr">shared_ptr</h4><p>shared_ptr是一种共享式智能指针，多个shared_ptr可以共享同一块内存，可以记录有多少个shared_ptr共同拥有这块内存。当共享计数器归零时，该内存将被自动销毁。shared_ptr支持拷贝和移动操作。</p><p>shared_ptr的主要作用是：</p><ul><li>管理动态分配的内存对象的生命周期，可以确保内存的正确释放。</li><li>可以在多个地方共享同一份内存资源，避免了内存多次释放或者使用悬空指针的问题。</li></ul><p>shared_ptr通过一个控制块来实现共享计数器的维护，该控制块包含有关指向对象的引用计数和删除器等信息。</p><h4 id="weak_ptr">weak_ptr</h4><p>weak_ptr是一种弱引用智能指针，它不会增加被指向对象的引用计数，也不拥有对象的所有权，不能直接访问其指向的对象，必须通过lock()成员函数获取一个shared_ptr才能访问。当shared_ptr被销毁时，weak_ptr不会影响该对象的销毁。要与std::shared_ptr一起使用，一个std::weak_ptr对象看作是std::shared_ptr对象管理的资源的观察者，不影响共享资源的生命周期</p><p>weak_ptr的主要作用是：</p><ul><li>解决shared_ptr的循环引用问题，避免内存泄漏。</li><li>用于观察shared_ptr所管理的对象，防止shared_ptr被释放后，还有其他地方使用悬空指针的问题。</li></ul><p>weak_ptr通过lock()成员函数可以获取一个shared_ptr，用于访问被指向的对象，如果没有可用的shared_ptr，则lock()会返回一个空的shared_ptr。</p><h5 id="循环引用">循环引用</h5><p>循环引用是指两个或多个对象之间相互引用，导致它们的引用计数不为0，无法被自动释放，从而导致内存泄漏。在使用<code>shared_ptr</code>时，如果出现循环引用，将会导致对象无法被正确地释放，从而出现内存泄漏问题。</p><p><code>weak_ptr</code>可以通过对<code>shared_ptr</code>进行弱引用来解决循环引用问题。<code>weak_ptr</code>可以访问<code>shared_ptr</code>所指向的对象，但不会改变其引用计数。当<code>shared_ptr</code>所指向的对象被释放时，所有的<code>weak_ptr</code>都会自动失效，无需手动释放内存。</p><h4 id="悬空指针-dangling-pointer">悬空指针 dangling pointer</h4><p>C++中的悬空指针是指一个指针变量指向了已经被释放或者不存在的内存地址。</p><p>当一个指针变量指向一个已经释放的内存地址时，该指针变量就变成了一个悬空指针。这种情况可能会导致程序崩溃或者出现其他错误，因为程序试图在无效的内存地址上执行操作。</p><p>悬空指针通常是由于程序员没有正确管理内存所导致的，例如在使用<code>delete</code>关键字释放指针指向的内存后，却继续使用指针变量，这会导致指针变量成为悬空指针。</p><p>为了避免悬空指针，程序员应该在使用完指针后，及时将其指向<code>nullptr</code> 或者重新赋值为有效的内存地址。此外，使用智能指针和RAII 等技术也可以有效避免悬空指针的问题。</p><h3 id="stdmove">std::move</h3><h4 id="move的作用">move的作用</h4><p><code>std::move</code> 是 C++11标准中的一个函数模板，用于将对象的值转移（移动）到另一个对象中，同时将原来对象的值置为默认值（通常是零值或者空值）。这个操作称为“移动语义”，可以显著提高程序的性能。</p><p>在 C++中，当我们需要复制一个对象时，我们通常会使用拷贝构造函数或拷贝赋值运算符。但是在某些情况下，我们并不需要完全复制一个对象，而只需要转移它的资源所有权，即将指向这些资源的指针或引用从原来的对象中“移动”到新的对象中。这种情况下，使用<code>std::move</code> 可以实现“移动语义”。</p><p>具体来说，使用 <code>std::move</code>可以将一个左值转换为右值引用，以便进行移动语义操作。例如：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br>    std::vector&lt;<span class="hljs-type">int</span>&gt; v1 &#123;<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>&#125;;<br>    std::vector&lt;<span class="hljs-type">int</span>&gt; v2 &#123;std::<span class="hljs-built_in">move</span>(v1)&#125;;<br>    std::cout &lt;&lt; <span class="hljs-string">&quot;v1 size = &quot;</span> &lt;&lt; v1.<span class="hljs-built_in">size</span>() &lt;&lt; std::endl; <span class="hljs-comment">// 输出 0</span><br>    std::cout &lt;&lt; <span class="hljs-string">&quot;v2 size = &quot;</span> &lt;&lt; v2.<span class="hljs-built_in">size</span>() &lt;&lt; std::endl; <span class="hljs-comment">// 输出 3</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><p>在上面的代码中，我们创建了两个 <code>std::vector&lt;int&gt;</code>对象 <code>v1</code> 和 <code>v2</code>。然后，我们使用<code>std::move</code> 将 <code>v1</code> 的资源所有权转移给<code>v2</code>，从而避免了不必要的拷贝操作，并且可以提高程序的性能。最后，我们打印了<code>v1</code> 和 <code>v2</code> 的大小，可以看到 <code>v1</code>的大小为 0，因为其资源已经被移动到了 <code>v2</code> 中。</p><p>需要注意的是，使用 <code>std::move</code>不会真正地移动数据，而只是将对象的指针或引用转移到另一个对象中。因此，在移动完成后，原来的对象可能处于未定义的状态，不应该再使用它。</p><p>The purpose of <code>std::move</code> is to allow objects to beefficiently moved or transferred between scopes, rather than copied. Itdoes this by converting an lvalue to an rvalue reference, which enablesthe move constructor or move assignment operator to be called, if theyexist. This can lead to improved performance in cases where copying isexpensive or unnecessary.</p><h4 id="move和拷贝的比较">move和拷贝的比较</h4><ol type="1"><li>避免了不必要的拷贝操作：使用拷贝构造函数或拷贝赋值运算符进行复制操作时，会将对象的数据逐一复制到新的对象中，这个过程可能非常耗时。而使用<code>std::move</code>可以将对象的资源所有权转移到新的对象中，避免了这个耗时的复制操作。</li><li>可以利用移动构造函数和移动赋值运算符：对于许多对象，移动操作的时间复杂度比拷贝操作低很多，因为移动操作不需要复制数据，而只需要转移指针或引用。使用<code>std::move</code>可以将对象转换为右值引用，从而让编译器使用移动构造函数或移动赋值运算符，提高程序的性能。</li></ol><p>Using <code>std::move</code> can be better than direct copyingbecause it avoids unnecessary copying operations, which can be slow andresource-intensive. Instead, it transfers ownership of an object'sresources to a new object, which can be faster and more efficient.Additionally, <code>std::move</code> can take advantage of moveconstructors and move assignment operators, which can be faster thancopy constructors and copy assignment operators for many objects.</p><h4 id="move和指针的区别">move和指针的区别</h4><p><code>std::move</code> 和指针的主要区别是，<code>std::move</code>本质上是将一个左值强制转换为右值引用，从而实现“移动语义”，而指针只是一个指向内存地址的变量，没有实现“移动语义”。</p><p>具体来说，使用 <code>std::move</code>可以将对象的资源所有权转移给另一个对象，同时将原来对象的值置为默认值，这个过程被称为“移动语义”。而指针只是一个变量，它存储了指向某个内存地址的值。对指针进行赋值或传递时，只是将指针的值传递给另一个变量或函数，不会影响指针所指向的内存地址的内容或所有权。</p><p>另外，使用指针需要自己手动管理内存分配和释放，容易引起内存泄漏或悬空指针等问题。而使用<code>std::move</code>可以避免这些问题，因为移动语义可以确保资源的所有权被正确地管理。</p><p>he main difference between <code>std::move</code> and a pointer isthat <code>std::move</code> provides move semantics, while a pointeronly stores a memory address. <code>std::move</code> enables objects tobe transferred between scopes more efficiently, by transferringownership of an object's resources to a new object.</p><p>A pointer only stores the address of an object in memory, and doesnot provide any special semantics for transferring ownership or managingresources. Additionally, using pointers requires manual memorymanagement, which can be error-prone and lead to memory leaks ordangling pointers.</p><h3 id="左值和右值">左值和右值</h3><p>左值表示一个表达式所代表的内存位置，即一个可寻址的内存区域，可以被修改。左值可以出现在赋值操作的左边，表示将右边的值赋给左边的内存位置，也可以出现在表达式的任何位置。</p><p>例如，变量、数组元素和结构体成员都是左值。</p><p>右值表示表达式的值，即一些无法被修改的临时数据，通常出现在赋值操作的右边，表示将该值赋给左边的内存位置，或者在表达式中的常量或计算结果。</p><p>例如，字面常量、函数返回值和表达式计算结果都是右值。</p><p><strong>概念1：</strong></p><p>左值：可以放到等号左边的东西叫左值。</p><p>右值：不可以放到等号左边的东西就叫右值。</p><p><strong>概念2</strong>：</p><p>左值：可以取地址并且有名字的东西就是左值。</p><p>右值：不能取地址的没有名字的东西就是右值。</p><h4 id="将亡值">将亡值</h4><p>将亡值是指C++11新增的和右值引用相关的表达式，通常指将要被移动的对象、T&amp;&amp;函数的返回值、std::move函数的返回值、转换为T&amp;&amp;类型转换函数的返回值，将亡值可以理解为即将要销毁的值，通过“盗取”其它变量内存空间方式获取的值，在确保其它变量不再被使用或者即将被销毁时，可以避免内存空间的释放和分配，延长变量值的生命周期，常用来完成移动构造或者移动赋值的特殊任务。</p><h4 id="右值引用">右值引用</h4><p>如果使用右值引用，那表达式等号右边的值需要时右值，可以使用std::move函数强制把左值转换为右值。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-type">int</span> a = <span class="hljs-number">4</span>;<br><span class="hljs-type">int</span> &amp;&amp;b = a; <span class="hljs-comment">// error, a是左值</span><br><span class="hljs-type">int</span> &amp;&amp;c = std::<span class="hljs-built_in">move</span>(a); <span class="hljs-comment">// ok</span><br></code></pre></td></tr></table></figure><p>右值引用具有移动语义，可以将一个右值引用绑定到临时对象上，然后将这个对象的状态转移给新的对象。这种操作称为移动语义。</p><p>移动语义可以提高程序的效率和性能，尤其是在进行资源管理时。例如，一个对象在拷贝时需要分配内存，而移动时只需要将指向该内存的指针转移即可，避免了复制内存的开销。</p><h5 id="移动语义">移动语义</h5><p>移动语义，可以理解为转移所有权，之前的拷贝是对于别人的资源，自己重新分配一块内存存储复制过来的资源，而对于移动语义，类似于转让或者资源窃取的意思，对于那块资源，转为自己所拥有，别人不再拥有也不会再使用，通过C++11新增的移动语义可以省去很多拷贝负担，通过移动构造函数。</p><p>移动语义仅针对于那些实现了移动构造函数的类的对象，对于那种基本类型int、float等没有任何优化作用，还是会拷贝，因为它们实现没有对应的移动构造函数。</p>]]></content>
    
    
    <categories>
      
      <category>计算机</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>神经网络</title>
    <link href="/2023/03/02/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <url>/2023/03/02/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<h1 id="机器学习">机器学习</h1><h2 id="基本原理">基本原理</h2><ul><li><p>神经元：函数（第一层是每一个像素点，隐层是零件一步步组装的过程）</p></li><li><p>权重：关注什么样的像素图案</p></li><li><p>bias：加权和要有多大才能使神经元的激活有意义</p></li></ul><p><span class="math display">\[a^{(1)}=\sigma(Wa^{(0)}+b)\]</span></p><p>其中<span class="math inline">\(a\)</span>是某一层的神经元向量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Network</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, *args, **kwargs</span>):<br>        <span class="hljs-keyword">pass</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">feedforward</span>(<span class="hljs-params">self, a</span>):<br>        <span class="hljs-keyword">for</span> b, w <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(self.biases, self.weights):<br>            a = sigmoid(np.dot(w, a) + b)<br>        <span class="hljs-keyword">return</span> a<br></code></pre></td></tr></table></figure><ul><li>ReLU函数<span class="math inline">\(ReLU(a)=max(0,a)\)</span>比sigmoid函数更好训练</li></ul><h3 id="梯度下降法">梯度下降法</h3><p>单个样本的cost：</p><p><img src="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20220322214549.png" style="zoom:50%;" /></p><p>损失：样本的平均cost</p><p>函数的梯度指出了函数最陡的增长方向，所以沿梯度的负方向走函数值下降得最快</p><h3 id="反向传播算法">反向传播算法</h3><p><imgsrc="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20220323150037.png" /></p><ul><li>随机梯度下降：划分minibatch（比如一个minibatch是100个样本），用minibatch的平均改变值代替负梯度（全部样本的平均改变值）</li></ul><p>【醉汉下山法】</p><p>假设每层只有1个神经元：</p><p><imgsrc="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20220323151113.png" /></p><p><imgsrc="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20220323151332.png" /></p><p><imgsrc="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20220323151213.png" /></p><p>负梯度：</p><p><imgsrc="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20220323151247.png" /></p><p>拓展到多个神经元：</p><p><imgsrc="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20220323151620.png" /></p><h3 id="激活函数">激活函数</h3><ul><li>非线性：导数不是常数，多层网络不退化成单层网络</li><li>几乎处处可为：保证优化中梯度的可计算性</li><li>计算简单</li></ul><h4 id="sigmoid函数">sigmoid函数</h4><p>缺点：</p><ul><li>计算量大，反向传播求误差梯度时，求导复杂</li><li>反向传播的时候，很容易出现梯度消失的情况，从而无法完成深度神经网络的训练（导数从0开始，很快又趋近于0）</li></ul><h4 id="relu函数">ReLU函数</h4><p>优点：梯度不会消失，计算速度很快，收敛很快</p><p>缺点：输出不是0均值</p><p>dead relu problem，某些神经元可能永远不会激活，导致参数不会更新</p><h4 id="softmax函数">softmax函数</h4><p>multiclassification</p><h4 id="tanh函数">tanh函数</h4><p>是0均值</p><p>梯度消失问题，但是缓解了一点</p><h3 id="层">层</h3><p>图像处理：</p><p>input→（卷积层<spanclass="math inline">\(\times\)</span>N+池化层）<spanclass="math inline">\(\times\)</span>M→全连接层<spanclass="math inline">\(\times\)</span>K→output</p><h4 id="全连接层-fully-connected-layer">全连接层 Fully ConnectedLayer</h4><h4 id="卷积层-convolutional-layer">卷积层 Convolutional layer</h4><ul><li>局部感知野</li><li>参数共享</li></ul><h4 id="池化层-pooling-layer">池化层 pooling layer</h4><ul><li>使特征图变小，简化网络</li><li>特征压缩，提取主要特征</li></ul><p>常用池化层：</p><ul><li>最大池化</li><li>平均池化</li></ul><h3 id="迭代">迭代</h3><ul><li><p>epoch：用训练集所有样本对模型进行一次完整训练</p></li><li><p>batch：用训练集中一小部分样本对模型权重进行一次反向传播的参数更新</p></li><li><p>iteration：用一个batch的数据对模型进行一次参数更新的过程</p></li></ul><p>梯度下降：</p><table><thead><tr class="header"><th>梯度下降方式</th><th>Training Set Size</th><th>Batch Size</th><th>Number of Batches</th></tr></thead><tbody><tr class="odd"><td>BGD</td><td>N</td><td>N</td><td>1</td></tr><tr class="even"><td>SGD</td><td>N</td><td>1</td><td>N</td></tr><tr class="odd"><td>Mini-Batch</td><td>N</td><td>B</td><td>N/B</td></tr></tbody></table><h2 id="循环神经网络">循环神经网络</h2><h3 id="rnn">RNN</h3><p><span class="math display">\[a^{(t)}=tanh(W_{ax}x^{(t)}+W_{aa}a^{(t-1)}+b_a)\]</span></p><p><span class="math display">\[\hat{y}^{(t)}=softmax(W_{ya}a^{(t)}+b_y)\]</span></p><p><imgsrc="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20220324153051.png" /></p><p><imgsrc="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20220324154733.png" /></p><p>特点：</p><ul><li>串联结构，后面结果的生成要参考前面的信息</li><li>所有特征共享同一套参数<ul><li>面对不同的输入（两个方面），能学到不同的相应的结果</li><li>极大减少了训练参数量</li><li>输入和输出数据在不同例子中可以有不同长度</li></ul></li></ul><h4 id="损失函数">损失函数</h4><p>单个时间步的损失函数： <span class="math display">\[L_t(\hat{y}_t,y_t)=-y_tlog y_t-(1-y_t)log(1-y_t)\]</span></p><p>整个序列的损失函数：所有时间步的损失求和</p><p><span class="math display">\[L(\hat{y},y)=\sum^T_{t=1}L_t(\hat{y}_t,y_t)\]</span></p><h4 id="缺点">缺点</h4><h5 id="梯度消失">梯度消失</h5><p>当序列太长时，连乘过多，容易导致梯度消失，无法将信息传递过去，参数更新只能捕捉到局部依赖关系，没法再捕捉序列之间的长期关联或依赖关系。这会导致渐变在向后传播时呈指数级收缩。由于梯度极小，内部权重几乎没有调整，因此较早的层无法进行任何学习。<span class="math display">\[\frac{\partial L_t}{\partial W_x}=\sum^t_{k=1}[\frac{\partialL_t}{\partial O_t}\frac{\partial O_t}{\partialS_t}(\prod^t_{j=t+1-k}\frac{\partial tanh(\theta_j)}{\partial\theta_j})X_{t+1-k}W_s^{k-1}]\]</span> 由于<spanclass="math inline">\(W_s^{k-1}\)</span>很小的时候连乘会趋于0，因此会梯度消失</p><p>梯度消失解决办法：</p><ul><li><p>选择relu等梯度大部分落在常数上的激活函数。relu函数的导数在正数部分是恒等于1的，因此在深层网络中使用relu激活函数就不会导致梯度消失的问题。</p></li><li><p>batch normalization</p><p>BN就是通过对每一层的输出规范为均值和方差一致的方法，消除了权重参数放大缩小带来的影响，进而解决梯度消失的问题，或者可以理解为BN将输出从饱和区拉到了非饱和区。采用Batch Normalization 层，对网络中计算得到中间值进行归一化，使得中间计算结果的取值在均值为 0，方差为 1 这样的分布内。那么此时，在 sigmoid和 tanh 中，函数取值处于中间变化较大的部分，梯度取值也相对较大</p></li></ul><h5 id="梯度爆炸">梯度爆炸</h5><p>使网络变得不稳定。在极端情况下，权重的值变得非常大，以至于溢出，导致NaN值</p><p>解决办法：</p><ul><li>梯度修剪，观察梯度向量，如果大于某个阈值，缩放梯度向量，保证它不会太大</li><li>权重正则化：检查网络权重的大小，并惩罚产生较大权重值的损失函数</li><li>改变损失函数：把sigmoid改为ReLU，relu函数的导数在正数部分是恒等于1的，因此在深层网络中使用relu激活函数就不会导致梯度消失的问题</li></ul><h3 id="lstm-long-short-term-memory">LSTM Long Short-Term Memory</h3><p>RNN：想把所有东西记住，不管有没有用</p><p>LSTM：设计记忆细胞，具备选择性记忆的功能，记忆重要信息，过滤掉噪声信息，减轻记忆负担</p><p><imgsrc="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20220324162626.png" /></p><p><imgsrc="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20220324164635.png" /></p><ul><li><spanclass="math inline">\(C_t\)</span>，记忆细胞：在LSTM每个时间步里面都有一个以及细胞，让LSTM有能力自由选择每个时间步里记忆的内容。</li><li><spanclass="math inline">\(h_t\)</span>，状态：【本门考试分数】</li><li><span class="math inline">\(f_t\)</span>，遗忘门：[0,1]的向量，选择性遗忘上一个时间步带来的记忆【遗忘复习高数时对线代无关的记忆】</li><li><span class="math inline">\(i_t\)</span>，输入门：[0,1]的向量，选择性保留这一个时间步新生成的记忆（来自<spanclass="math inline">\(X_t\)</span>）【复习线代获得的记忆】</li></ul><p><span class="math display">\[C_t=f_tC_{t-1}+i_t\tilde{C_t}\]</span></p><ul><li><spanclass="math inline">\(o_t\)</span>，输出门：【只用到其中一部分记忆用来答题】</li><li>tanh：【记忆转化为答题能力的过程】</li></ul><h4 id="lstm优点">LSTM优点</h4><ul><li>缓解RNN的梯度消失问题：通过调节<spanclass="math inline">\(W_{hf},W_{hi},W_{hg}\)</span>可以灵活控制<spanclass="math inline">\(\frac{\partial C_j}{\partialC_{j-1}}\)</span>的值，当要从n时刻长期记忆某个东西直到m时刻时，该路径上的<spanclass="math inline">\(\prod_{t=n}^m\limits \frac{\partial C_j}{\partialC_{j-1}}\approx 1\times1\times...\times1\)</span></li></ul><h4 id="解决lstm梯度消失问题">解决LSTM梯度消失问题</h4><ol type="1"><li>使用门控循环单元（GRU）替代LSTM。GRU也是一种递归神经网络，可以在处理序列数据时具有与LSTM相似的记忆能力，但是GRU只有两个门控单元，因此具有更少的参数，同时也更容易训练。</li><li>使用长短期记忆网络（LSTM）的变种，如PeepholeLSTM，对输入门、遗忘门和输出门进行扩展，以便更好地控制信息的流动，从而减少梯度消失的问题。</li><li>对于输入数据进行归一化处理，将其缩放到一个较小的范围内。这有助于减少梯度消失问题的影响，并使神经网络更容易训练。</li><li>使用梯度裁剪技术。这种方法可以在训练过程中对梯度进行剪切，使其不会超出一个预定的范围。这有助于避免梯度爆炸问题，并减少梯度消失的问题。</li><li>采用注意力机制（AttentionMechanism）。该方法能够使网络在处理序列数据时更加关注重要的部分，并将注意力集中在最相关的信息上，从而避免了长序列中梯度消失的问题。</li></ol><h3 id="注意力机制">注意力机制</h3><p>序列预测任务是给定了n个词，去预测第n+1个词，把关注点放在前面n个词的哪一些。引入注意力机制可以更好地解释应该重点关注前面序列里的哪一些</p><h4 id="transformer和lstm区别">transformer和LSTM区别</h4><p>RNN是串行训练，要把上一步输入全部输入进来才能做下一步训练，transformer加了注意力机制可以并行训练，没有记忆力这个东西，只是通过注意力机制去找到这个东西是要关注前面的哪一些词，但并没有记得之前一共发生过什么东西</p><p>Transformer和LSTM是用于处理序列数据的两种不同的神经网络架构，它们有以下几个区别：</p><ol type="1"><li>结构不同：LSTM是一种递归神经网络，使用门控单元（如输入门、遗忘门、输出门）来控制序列中信息的流动。而Transformer则是一种基于自注意力机制的神经网络，使用多头注意力机制来对序列数据进行编码。</li><li>并行性不同：由于LSTM的递归结构，每个时间步的计算都依赖于前一个时间步的输出。这意味着LSTM在计算时必须按照时间步序列化计算，无法进行并行化处理。相比之下，Transformer的计算是全局的，可以并行计算，从而更加高效。</li><li>记忆能力不同：LSTM通过门控单元来控制信息的流动，从而具有很强的记忆能力，能够处理长序列数据。而Transformer使用自注意力机制，可以同时考虑序列中的所有位置，从而不需要像LSTM一样依赖于递归计算，因此在长序列上表现更好。</li><li>可解释性不同：由于LSTM的递归结构，很难对其内部的计算过程进行解释。而Transformer的注意力机制可以将模型的注意力集中在输入序列中的不同部分，因此更容易解释模型的决策过程。</li></ol><p>总的来说，LSTM适合处理单向、多层次、长期的时间序列数据，例如语音识别和自然语言处理。而Transformer适合处理双向、并行化、全局的时间序列数据，例如机器翻译和自然语言生成。</p><h2 id="生成对抗网络">生成对抗网络</h2><h3 id="gan">GAN</h3><p><imgsrc="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20220329111525.png" /></p><p><imgsrc="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20220329112132.png" /></p><ul><li>全连接层升维</li><li>把7$<spanclass="math inline">\(7的数据变成28\)</span>$28的图像：插值（UpSampling2D）/反卷积</li></ul><p><imgsrc="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20220329112427.png" /></p><ul><li>图像二分类：表示层（卷积池化卷积池化），应用层</li></ul><p><imgsrc="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20220329112632.png" /></p><p><imgsrc="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20220329112702.png" /></p><ol type="1"><li>固定G，把真实样本和generate的样本输入D，训练D</li><li>固定D，训练G</li><li>输入噪声给G，生成样本</li></ol><h4 id="缺点-1">缺点</h4><p><imgsrc="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20220329113720.png" /></p><p><imgsrc="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20220329113745.png" /></p><p>原问题化为KL散度的问题：</p><p><imgsrc="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20220329153116.png" /></p><ul><li>生成器没生成真实的样本：惩罚小</li><li>生成器生成不真实的样本：惩罚大</li></ul><p>loss偏向于生成“稳妥”的样本</p><h3 id="wgan">WGAN</h3><p><imgsrc="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20220329153349.png" /></p><ul><li>训练稳定性大幅增长</li></ul><p><imgsrc="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20220329153517.png" /></p><h3 id="cgan">CGAN</h3><p>输入G的不只是噪声，还有条件（比如红头发），把概率分布改为条件概率分布</p><h3 id="dcgan">DCGAN</h3><p>卷积神经网络+对抗神经网络</p><ul><li>在不改变GAN原理的情况下，增加增强稳定性的tricks</li></ul><p><imgsrc="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20220329113339.png" /></p><h2 id="防止过拟合">防止过拟合</h2><ul><li>池化</li><li>dropout</li><li>early stopping</li><li>正则化</li></ul><h2 id="强化学习">强化学习</h2><p>强化学习有很多状态，要学习在每个状态下要采取什么动作。</p><p><imgsrc="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20220325165554.png" /></p>]]></content>
    
    
    <categories>
      
      <category>计算机</category>
      
      <category>机器学习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>面试题</title>
    <link href="/2023/03/02/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E9%9D%A2%E8%AF%95%E9%A2%98/"/>
    <url>/2023/03/02/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E9%9D%A2%E8%AF%95%E9%A2%98/</url>
    
    <content type="html"><![CDATA[<h1 id="面试题">面试题</h1><p>Logistic回归的损失函数和梯度分别是多少、SVM的数学推导、GBDT回归的梯度代表什么</p>]]></content>
    
    
    <categories>
      
      <category>计算机</category>
      
      <category>机器学习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>机器学习</title>
    <link href="/2023/03/02/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    <url>/2023/03/02/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h1 id="机器学习">机器学习</h1><h2 id="特征工程">特征工程</h2><p>以下是进行预测任务特征工程时需要考虑的关键步骤：</p><ol type="1"><li>数据探索：在开始进行特征工程之前，需要探索数据集并了解不同变量之间的关系。这可以涉及可视化数据，计算汇总统计信息，识别潜在的相关性或模式。</li><li>特征选择：并非所有特征都与特定预测任务相关或有用。特征选择涉及识别最重要的特征，并删除冗余、无关或噪声较大的特征。这可以使用相关性分析、特征重要性排名或基于模型的选择等方法实现。</li><li>特征提取：有时可用的特征可能不够信息丰富，或者需要转换以显示有用的模式或关系。特征提取涉及从现有特征中创建新特征，例如计算统计量、跨时间段聚合信息或将分类变量转换为数值形式。</li><li>特征缩放：许多机器学习模型要求特征在相似范围内进行缩放，以获得良好的性能。特征缩放涉及将数据规范化，以减少异常值的影响并确保所有特征具有相似的范围。</li><li>特征编码：分类变量在预测任务中可能会出现问题，因为需要将它们转换为数值形式。特征编码涉及将分类变量转换为可用于机器学习模型的形式，例如进行独热编码或使用标签编码。</li><li>迭代细化：特征工程通常是一个迭代过程，在每一轮特征工程后，需要监视模型的性能。这可以帮助识别需要进一步改进和微调特征工程过程的领域。</li></ol><h3 id="特征选择">特征选择</h3><ol type="1"><li><p>相关性分析：计算每个特征与目标变量之间的相关系数，并排除与目标变量相关性较低的特征。</p></li><li><p>方差分析：计算每个特征的方差，并排除方差较小的特征，这些特征可能对结果没有显著贡献。</p></li><li><p>特征重要性：使用决策树等算法，计算每个特征的重要性，排除重要性较低的特征。</p></li><li><p>逐步回归：从所有特征开始，每次迭代去掉最不重要的特征，直到剩下较为重要的特征。</p></li><li><p>L1正则化：使用L1正则化方法，使得模型的权重矩阵中有许多权重为0，因此可以通过选择非零权重对应的特征来筛选特征。</p></li><li><p>PCA降维：使用主成分分析(PCA)等方法将特征降维，选择最重要的成分作为新的特征，从而达到特征筛选的目的。</p></li><li><p>Correlation analysis: Calculate the correlation coefficientbetween each feature and the target variable, and exclude features withlow correlation.</p></li><li><p>Analysis of variance (ANOVA): Calculate the variance of eachfeature and exclude those with small variances, which may not contributesignificantly to the outcome.</p></li><li><p>Feature importance: Use algorithms such as decision trees tocalculate the importance of each feature, and exclude those with lowimportance.</p></li><li><p>Stepwise regression: Start with all features and iterativelyremove the least important feature until only the most importantfeatures remain.</p></li><li><p>L1 regularization: Use L1 regularization to force many weights inthe model's weight matrix to be zero. This can be used to select thecorresponding non-zero features.</p></li><li><p>PCA dimensionality reduction: Use principal component analysis(PCA) to reduce the dimensionality of the features and select the mostimportant components as new features.</p></li></ol><h4 id="anova方差分析">ANOVA方差分析</h4><p>ANOVA(Analysis ofVariance，方差分析)是一种用于比较两个或多个组之间差异的统计方法。其原理基于方差分解的思想，将总方差分解为两部分，一部分是由于组间差异所导致的方差，另一部分是由于组内差异所导致的方差。如果组间方差显著大于组内方差，则说明不同组之间的差异是显著的。</p><p>ANOVA (Analysis of Variance) is a statistical method used to comparedifferences between two or more groups. Its principle is based on theidea of variance decomposition, which decomposes the total variance intotwo parts: one due to differences between groups and the other due todifferences within groups. If the variance between groups issignificantly greater than the variance within groups, then thedifferences between groups are considered significant.</p><p>ANOVA可以用来衡量不同特征对于目标变量的影响程度，因此可以用于特征选择。在进行ANOVA特征选择时，我们将特征的变量分为不同组，并计算各组的平均值和方差，然后计算F值。F值越大，表示该特征与目标变量之间的差异越大，因此越有可能是重要特征。</p><p>在ANOVA中，我们通常将数据分为k个组，每个组有n个样本。设<spanclass="math inline">\(x_{ij}\)</span>表示第i个组中第j个样本的观测值，<spanclass="math inline">\(\overline{x}_i\)</span>表示第i个组的平均值，<spanclass="math inline">\(\overline{x}\)</span>表示所有观测值的平均值。则总方差可以表示为：</p><p><span class="math display">\[SS_{total} = \sum_{i=1}^k \sum_{j=1}^n (x_{ij} - \overline{x})^2\]</span></p><p>组间方差可以表示为：</p><p><span class="math display">\[SS_{between} = n \sum_{i=1}^k (\overline{x}_i - \overline{x})^2\]</span></p><p>组内方差可以表示为：</p><p><span class="math display">\[SS_{within} = \sum_{i=1}^k \sum_{j=1}^n (x_{ij} - \overline{x}_i)^2\]</span></p><p>然后我们可以计算方差比值<spanclass="math inline">\(F\)</span>，它是组间方差与组内方差的比值：</p><p><span class="math display">\[F = \frac{SS_{between} / (k-1)}{SS_{within} / (n-k)}\]</span></p><p>其中，<span class="math inline">\(k-1\)</span>和<spanclass="math inline">\(n-k\)</span>分别为自由度，自由度表示在统计推断中独立变化的变量数量。如果<spanclass="math inline">\(F\)</span>值越大，则说明组间方差越大，不同组之间的差异越显著。</p><p>在ANOVA中，我们通常还需要进行方差分析表的构建，方差分析表展示了各种方差的计算结果、自由度和均方值，以及<spanclass="math inline">\(F\)</span>值和<spanclass="math inline">\(p\)</span>值，用于判断组间差异的显著性。</p><h2id="数据不服从高斯分布为什么也可以ml">数据不服从高斯分布为什么也可以ML</h2><p>渐进理论，样本量足够大，用中心极限定理可以近似高斯分布</p><p>机器学习算法的理论基础通常建立在样本满足一些假设条件的基础上，例如正态分布或同方差性等。然而，在实际应用中，数据往往不满足这些假设条件，特别是在大数据集中，这种假设条件往往难以满足。因此，机器学习算法可以用于非正态分布数据。</p><p>以下是一些原因：</p><ol type="1"><li>机器学习算法的鲁棒性：许多机器学习算法具有一定的鲁棒性，即它们不会受到数据分布的轻微偏差的影响，而且能够在许多实际应用中表现出良好的性能。例如，决策树、随机森林和支持向量机等算法经常被用于非正态分布数据。</li><li>数据预处理：数据预处理是数据科学中的重要步骤。在处理非正态分布数据时，可以通过一些预处理技术来使数据更适合机器学习算法。例如，可以进行对数变换、幂变换或归一化等处理，以使数据更符合算法假设条件。这些技术通常可以在保持数据质量的同时提高算法的性能。</li><li>非参数方法：非参数方法是指不需要假设数据分布的一类机器学习算法。这些算法可以对非正态分布的数据建模，并能够在一些领域中取得良好的性能。例如，K近邻、神经网络和决策树等算法不需要数据分布假设，因此可以用于非正态分布数据。</li></ol><p>综上所述，尽管机器学习算法的理论基础建立在一些假设条件的基础上，但在实际应用中，这些假设条件经常难以满足。在处理非正态分布数据时，可以使用一些机器学习算法和数据预处理技术来提高模型性能，并在许多实际应用中获得良好的结果。</p><h2 id="bias-variance-tradeoff">bias variance tradeoff</h2><ul><li>bias：预测值的期望和实际值的差</li><li>variance：预测值的方差</li></ul><p>嘈杂的数据集：</p><ul><li>模型复杂度越大，偏差(Bias)越小</li><li>模型复杂度越大，方差(Variance)越大</li></ul><p>通常一个模型Variance很高，意味着它往往很复杂，而复杂的模型往往会对某些样本点非常敏感，也就很可能Overfit。同样的，一个模型如果Bias很高，意味着他一直都很不准，所以Underfitting。而我们的目标则是在模型的复杂度之间找到一个平衡，从而使Bias和Variance都尽可能低，从而得到一个Good Fitting的结果</p><p><imgsrc="https://jkq-pic.oss-cn-shanghai.aliyuncs.com/img/20221213155957.png" /></p><p><imgsrc="https://jkq-pic.oss-cn-shanghai.aliyuncs.com/img/20221213155840.png" /></p><h2 id="损失函数">损失函数</h2><h4 id="交叉熵-cross-entropy-loss-function">交叉熵 cross entropy lossfunction</h4><p>用于衡量两个分布之间的距离</p><p>逻辑回归</p><p>logLoss (对数损失函数，LR)</p><p>hinge loss (合页损失函数，SVM)</p><p>exp-loss (指数损失函数，AdaBoost)</p><p>cross-entropy loss (交叉熵损失函数，Softmax)</p><p>quadratic loss (平方误差损失函数，线性回归)</p><p>absolution loss (绝对值损失函数， )</p><p>0-1 loss (0-1损失函数)</p><h3 id="rmse-vs-mse">RMSE vs MSE</h3><p>使用 MSE作为损失函数时，误差的平方值被用于计算模型预测与实际值之间的差距。由于误差被平方，因此较大的误差将被放大，而这可能会导致模型过度关注那些误差较大的样本。这意味着，如果存在异常值，则模型将过度关注这些异常值而忽略其他样本。而使用RMSE 作为损失函数时，可以避免这个问题，因为 RMSE是对误差的平方根进行计算，这使得大误差的影响被降低，而小误差的影响则被放大。因此，RMSE对于训练一个更健壮（robust）的回归模型来说是更好的选择。</p><h2 id="评价指标">评价指标</h2><h3 id="roc-auc-sensitivity-specificity">ROC, AUC, Sensitivity,Specificity</h3><ul><li><strong>真阳性率（True PositiveRate，简称TPR）</strong>：TP/(TP+FN)，代表的含义是：实际观测为<strong>阳</strong>的样本中，模型能够正确识别出来的比例。TPR又称为Sensitivity。</li><li><strong>真阴性率（True NegativeRate，简称TNR）</strong>：TN/(FP+TN)，代表的含义是：实际观测为<strong>阴</strong>的样本中，模型能够正确识别出来的比例。TNR又称为Specificity。</li><li><strong>假阳性率（False PositiveRate，简称FPR）</strong>：FP/(FP+TN)，代表的含义是：实际观测为<strong>阴</strong>的样本中，被模型错误地划分成阳性的比例。FPR=1-Specificity。</li></ul><p><strong>TPR越高、FPR越低</strong>，说明模型的预测能力越好</p><p>以FPR为横坐标、TPR为纵坐标，将每一个阈值所对应的(FPR,TPR)放入坐标系中。用红色线条将所有的点连接起来——此即为<strong>ROC曲线</strong></p><p><imgsrc="https://jkq-pic.oss-cn-shanghai.aliyuncs.com/img/20221213104352.png" /></p><p><strong>AUC</strong>：ROC曲线下的面积</p><p>优点：不受正负样本比例的影响</p><p>缺点：</p><ul><li><p>对FPR和TPR两种错误的代价同等看待</p></li><li><p>没有给出模型误差的空间分布信息</p></li></ul><h2 id="svm">SVM</h2><p>SVM（支持向量机）是一种用于分类和回归分析的监督式学习算法，其原理可以用以下公式来解释：</p><p>对于一个二分类问题，假设有训练样本集D={（x1,y1），（x2,y2），…，（xn,yn）}，其中xi为输入变量，yi∈{+1,-1}为输出变量（类别标签）。SVM的目标是找到一个最优的超平面，将数据集D分成两个类别，使得不同类别之间的间隔最大化。该超平面由以下公式给出：</p><p>w·x+b=0</p><p>其中，w是超平面的法向量（Normalvector），b是偏移量。对于输入变量x，它属于超平面上方的类别+1，否则为-1。</p><p>SVM的训练过程是通过最大化间隔来找到最优超平面。假设超平面距离正例样本最近点和负例样本最近点的距离之和为2M，则最大化间隔问题可以转化为以下凸优化问题：</p><p>max M</p><p>s.t. yi(w·xi+b)≥M, i=1,2,...,n</p><p>||w||=1</p><p>其中，第一个约束条件确保每个样本都位于超平面两侧，第二个约束条件保证法向量w为单位向量。通过求解上述凸优化问题，我们可以得到最优的超平面，使得数据集D被分为两个类别并且间隔最大。</p><p>在实际应用中，有时数据集可能无法用线性超平面分开，这时可以使用核函数将数据映射到高维空间中，使得数据集变得可分。SVM的核函数可以是线性核、多项式核、径向基函数（RBF）核等。</p><h3 id="svm-vs-神经网络">SVM vs 神经网络</h3><p><imgsrc="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20210111163052.png" /></p><h3 id="svn的最大间隔思想">SVN的”最大间隔”思想</h3><p><imgsrc="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20210111163533.png" /></p><h3 id="对偶问题及其解的稀疏性">对偶问题及其解的稀疏性</h3><p><imgsrc="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20210111214930.png" /></p><p><imgsrc="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20210111215131.png" /></p><ul><li>SVM解的稀疏性: 训练完成后, 大部分的训练样本都不需保留,最终模型仅与支持向量有关</li></ul><h4 id="求解方法smo">求解方法——SMO</h4><p>基本思路：不断执行如下两个步骤直至收敛</p><ol type="1"><li>选取一对需更新的变量<spanclass="math inline">\(\alpha_i\)</span>和<spanclass="math inline">\(\alpha_j\)</span></li><li>固定<span class="math inline">\(\alpha_i\)</span>和<spanclass="math inline">\(\alpha_j\)</span>以外的参数,求解<strong>对偶问题</strong>更新<spanclass="math inline">\(\alpha_i\)</span>和<spanclass="math inline">\(\alpha_j\)</span></li></ol><p><imgsrc="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20210111215734.png" /></p><h3id="核函数通过向高维空间映射解决线性不可分的问题">核函数——通过向高维空间映射解决线性不可分的问题</h3><p>基本想法：不显式地设计核映射, 而是设计核函数</p><p><imgsrc="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20210111215945.png" /></p><ul><li>Mercer定理(充分非必要)：只要一个<strong>对称函数</strong>所对应的核矩阵<strong>半正定</strong>,则它就能作为核函数来使用</li></ul><h3id="引入软间隔缓解特征空间中线性不可分的问题">引入”软间隔”缓解特征空间中线性不可分的问题</h3><p>基本想法：最大化间隔的同时, 让不满足约束的样本应尽可能少</p><p><imgsrc="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20210111220305.png" /></p><ul><li><p>替代损失：hinge函数：max(0, 1-z)</p><p><imgsrc="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20210111221533.png" /></p></li></ul><h3id="将支持向量的思想应用到回归问题上得到支持向量回归">将支持向量的思想应用到回归问题上得到支持向量回归</h3><p><imgsrc="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20210111222847.png" /></p><p><imgsrc="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20210111222905.png" /></p><p><imgsrc="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20210111222938.png" /></p><h3 id="section"></h3>]]></content>
    
    
    <categories>
      
      <category>计算机</category>
      
      <category>机器学习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>树</title>
    <link href="/2023/03/02/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E6%A0%91/"/>
    <url>/2023/03/02/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E6%A0%91/</url>
    
    <content type="html"><![CDATA[<h1 id="树">树</h1><h2 id="决策树">决策树</h2><h3 id="构建过程">构建过程</h3><ol type="1"><li>选择特征：选择最优特征作为当前节点的分裂特征。通常使用信息增益、基尼指数或卡方检验等方法来选择特征。</li><li>划分数据集：将数据集根据选定的特征值进行划分，生成子集。</li><li>递归构建子树：对于每个子集，重复步骤1和2，直到当前节点为叶子节点（即数据集中的所有实例属于同一类别），或满足预设的终止条件（例如达到树的最大深度、样本数量不足等）。</li><li>剪枝：通过剪枝来避免过拟合。通常有预剪枝和后剪枝两种方法。预剪枝是在构建过程中，设置一个阈值，若划分当前节点不能增加决策树的泛化能力，则停止划分。后剪枝是在构建完整个树之后，对树进行剪枝。具体做法是从下往上遍历树中的节点，对每个节点进行剪枝，计算剪枝后的准确率，选择准确率最高的方式进行剪枝。</li><li>测试决策树：用测试数据集来测试决策树的分类性能。通常采用交叉验证等方法来评估决策树的泛化能力。</li><li>使用决策树进行预测：用新数据样本来预测目标变量的值，通过遍历决策树的各个节点来决定样本的类别。</li></ol><h3 id="递归停止条件">递归停止条件</h3><ul><li>当前结点包含的<strong>样本全部属于同一类别</strong></li><li>当前<strong>属性集为空</strong>，或所有样本在<strong>所有属性上取值相同</strong></li><li>当前结点包含的<strong>样本集合为空</strong></li></ul><h3 id="决策树的分裂节点标准">决策树的分裂节点标准</h3><h4 id="信息增益id3">信息增益ID3</h4><p><span class="math display">\[Gain\left( D,a \right) =Ent\left( D \right) -\sum_{v=1}^V{\frac{\left|D^v \right|}{\left| D \right|}Ent\left( D^v \right)}\]</span></p><p>不足：偏好<strong>取值数目较多</strong>的属性</p><h4 id="增益率">增益率</h4><p><imgsrc="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20210111102456.png" /></p><p>不足：偏好<strong>取值数目较少</strong>的属性</p><h5 id="c4.5">C4.5</h5><ol type="1"><li>先找出<strong>信息增益高于平均水平</strong>的属性</li><li>从中选<strong>增益率最高</strong>的属性</li></ol><h4 id="基尼指数min最优">基尼指数（min最优）</h4><p><imgsrc="https://finclaw.oss-cn-shenzhen.aliyuncs.com/img/20210111103059.png" /></p><ul><li>反映了从数据集中随机抽取两个样本，其类别标记不一致的概率</li></ul><ol type="1"><li><p>信息增益（Information Gain）</p><p>信息增益是指根据特征属性将数据集分成若干子集后，得到的子集的信息不确定性减少的程度。在决策树中，信息增益越大的特征属性越优先被选择作为分裂节点。</p><p>信息增益的计算公式为：<span class="math inline">\(Gain(S, A) =Entropy(S) - \sum_{v\inValues(A)}\frac{|S_v|}{|S|}Entropy(S_v)\)</span></p><p>其中，<span class="math inline">\(S\)</span>表示原始数据集，<spanclass="math inline">\(A\)</span>表示待分裂的特征属性，<spanclass="math inline">\(Values(A)\)</span>表示特征属性<spanclass="math inline">\(A\)</span>的可能取值，<spanclass="math inline">\(S_v\)</span>表示取值为<spanclass="math inline">\(v\)</span>的子集，<spanclass="math inline">\(|S|\)</span>和<spanclass="math inline">\(|S_v|\)</span>分别表示数据集<spanclass="math inline">\(S\)</span>和<spanclass="math inline">\(S_v\)</span>的样本数，<spanclass="math inline">\(Entropy(S)\)</span>和<spanclass="math inline">\(Entropy(S_v)\)</span>分别表示数据集<spanclass="math inline">\(S\)</span>和<spanclass="math inline">\(S_v\)</span>的信息熵。</p></li><li><p>基尼不纯度（Gini Impurity）</p><p>基尼不纯度是指从数据集中随机选择两个样本，这两个样本在分类上的误差率。在决策树中，基尼不纯度越小的特征属性越优先被选择作为分裂节点。</p><p>基尼不纯度的计算公式为：<span class="math inline">\(Gini(S) =\sum_{k=1}^{K}\sum_{k&#39;\neq k}p_kp_{k&#39;} = 1 -\sum_{k=1}^{K}p_k^2\)</span></p><p>其中，<span class="math inline">\(S\)</span>表示数据集，<spanclass="math inline">\(K\)</span>表示类别数，<spanclass="math inline">\(p_k\)</span>表示样本属于第<spanclass="math inline">\(k\)</span>个类别的概率。</p><p>需要注意的是，不同的分裂节点标准可能会得到不同的决策树结构，而选择合适的分裂节点标准取决于具体的问题和数据集。</p></li></ol><h3 id="id3c4.5cart区别">ID3，C4.5，CART区别</h3><p>ID3、C4.5和CART都是决策树算法，它们的共同点是都可以用于分类和回归问题，都可以生成二叉决策树，并且都采用贪心的策略进行特征选择，即选择当前最优的特征作为分裂节点。</p><p>下面是它们的不同点：</p><ol type="1"><li><p>算法基础</p><p>ID3和C4.5基于信息论，采用信息增益和信息增益比来选择特征属性。而CART则是基于基尼不纯度，采用基尼指数来选择特征属性。</p></li><li><p>分类和回归</p><p>ID3和C4.5只能用于分类问题，而CART既可以用于分类也可以用于回归问题。</p></li><li><p>多叉树和二叉树</p><p>ID3和C4.5生成的决策树是多叉树，即一个节点可以有多个子节点。而CART生成的决策树是二叉树，即每个节点只有两个子节点。</p></li><li><p>连续特征处理</p><p>ID3和C4.5不支持处理连续特征，需要对连续特征进行离散化（Discretization）。而CART可以直接处理连续特征，通过选择一个阈值将连续特征转化为二元特征进行处理。</p></li><li><p>树的剪枝</p><p>C4.5和CART都支持对生成的决策树进行剪枝，以避免过拟合。而ID3没有剪枝过程。</p></li><li><p>Algorithm foundation</p><p>ID3 and C4.5 are based on information theory, using information gainand gain ratio to select feature attributes. CART, on the other hand, isbased on Gini impurity and uses the Gini index to select featureattributes.</p></li><li><p>Classification and Regression</p><p>ID3 and C4.5 can only be used for classification problems, while CARTcan be used for both classification and regression problems.</p></li><li><p>Multi-way tree and binary tree</p><p>The decision tree generated by ID3 and C4.5 is a multi-way tree,meaning that a node can have multiple child nodes. In contrast, thedecision tree generated by CART is a binary tree, meaning that each nodehas only two child nodes.</p></li><li><p>Continuous feature processing</p><p>ID3 and C4.5 do not support processing continuous features andrequire discretization of continuous features. CART can directly processcontinuous features by selecting a threshold to convert continuousfeatures into binary features.</p></li><li><p>Tree pruning</p><p>C4.5 and CART both support pruning the generated decision tree toavoid overfitting, while ID3 does not have a pruning process.</p></li></ol><h4 id="优点">优点</h4><ul><li>因为是非参数模型，不需要对样本进行预先假设，可以处理复杂样本。</li><li>计算速度快，结果可解释性强。</li><li>可以同时处理分类和预测问题，对缺失值不敏感。</li></ul><h4 id="缺点">缺点</h4><ul><li>容易过拟合</li><li>特征之间存在相互关联时，数据结果表现较差。</li></ul><h4 id="优化方法">优化方法</h4><ul><li><p>进行剪枝防止过拟合。</p><p>剪枝包括<strong>预剪枝(Prepruning)</strong>和<strong>后剪枝(Postpruning),</strong>前者通过对连续型变量设置阈值，来控制树的深度，或者控制节点的个数，在节点开始划分之前就进行操作，进而防止过拟合现象。后者是自底向上对非叶节点进行考察，如果这个内部节点换成叶节点能提升决策树的泛化能力，那就把它换掉。</p></li><li><p>运用交叉验证的方法选择合适的参数。</p></li><li><p>通过模型集成的方法（Bagging-parallel/Boosting-serial），基于决策树构建更复杂的模型</p></li></ul><h2 id="回归树">回归树</h2><h3 id="cart">CART</h3><p>分割准则：分割后两个区域的MSE的和最小</p><p>假设<span class="math inline">\(n\)</span>个样本<spanclass="math inline">\(D={(x_1,y_1),(x_2,y_2),...,(x_n,y_n)}\)</span>，<spanclass="math inline">\(x\)</span>是<spanclass="math inline">\(d\)</span>维的（<spanclass="math inline">\(d\)</span>个特征），选取第<spanclass="math inline">\(i\)</span>个特征<spanclass="math inline">\(d_i\)</span>的取值<spanclass="math inline">\(s\)</span>作为划分两个区域的阈值,<spanclass="math inline">\(R_1=\{y_i|x_{i,d_i}\}\leq s\)</span>，<spanclass="math inline">\(R_2=\{y_i|x_{i,d_i}\}&gt;s\)</span>，每个区域内<strong>样本<spanclass="math inline">\(y_i\)</span>的均值</strong>作为该区域的预测值，计算两个区域的样本真实值和预测值的平方误差之和。遍历每一个特征的每一个样本的取值，选择使得平方误差最小的取值，使用该值将样本分成两个区域，对这两个区域继续上面的步骤，一致递归生成，直到满足停止条件。</p><h4 id="步骤">步骤</h4><ol type="1"><li><p>选择最优切分变量<spanclass="math inline">\(d_i\)</span>以及切分点<spanclass="math inline">\(s\)</span>，求解 <span class="math display">\[\underset{d_i,s}{\min}\{\underset{x_i\in R_1 (d_i,s)}{\min}(y_i-c_1)^2+\underset{x_i\in R_2 (d_i, s)}{\min}(y_i-c_2)^2\}\]</span> 遍历所有的特征维度<span class="math inline">\(d_i\)</span>，固定<span class="math inline">\(d_i\)</span>，遍历样本在特征<spanclass="math inline">\(d_i\)</span>处的取值s，使得式子最小。</p></li><li><p>根据选取的<spanclass="math inline">\((d_i,s)\)</span>将样本划分成两个区域，计算两个区域的平方误差</p></li><li><p>选取使得式（1）最小的<spanclass="math inline">\((d_i,s)\)</span>将样本划分成两个区域，每个区域的预测值是所有样本<spanclass="math inline">\(y\)</span>值的均值（根据平方误差的导数为0可得）。</p></li><li><p>对于两个子区域继续调用1），2），3），直到满足停止条件。</p></li><li><p>最后划分为M个区域<spanclass="math inline">\(R_1,R_2,...,R_m\)</span>，生成决策树 <spanclass="math display">\[f(x)=\sum^M_{i=1} c_iI(x\in R_i)\]</span></p></li></ol><h2 id="剪枝">剪枝</h2><h4 id="预剪枝边建树边剪枝">预剪枝（边建树，边剪枝）</h4><p>决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点记为叶结点，其类别标记为训练样例数最多的类别</p><ol type="1"><li>针对上述数据集，基于信息增益准则，选取属性v划分训练集。</li><li>分别计算划分前（即直接将该结点作为叶结点）及划分后的验证集精度，判断是否需要划分。</li><li>若划分后能提高验证集精度，则划分，对划分后的属性，执行同样判断；否则，不划分</li></ol><p>优点：</p><ul><li>降低过拟合风险</li><li>显著减少训练时间和测试时间开销</li></ul><p>缺点：</p><ul><li><strong>欠拟合</strong>风险：有些分支的当前划分虽然不能提升泛化性能，但在其基础上进行的后续划分却有可能导致性能显著提高。</li><li>预剪枝基于“贪心”本质禁止这些分支展开，带来了欠拟合风险</li></ul><h4 id="后剪枝先建树后剪枝">后剪枝（先建树，后剪枝）</h4><p>优点：</p><ul><li>后剪枝比预剪枝保留了更多的分支，欠拟合风险小，<strong>泛化性能</strong>往往优于预剪枝决策树</li></ul><p>缺点：</p><ul><li><strong>训练时间开销大</strong>：后剪枝过程是在生成完全决策树之后进行的，需要自底向上对所有非叶结点逐一考察；其训练时间要远大于预剪枝决策树</li></ul><h2 id="缺失值处理">缺失值处理</h2><h4 id="缺失值处理-1">缺失值处理</h4><h5id="在属性缺失的情况下选择划分属性">在属性缺失的情况下，选择划分属性</h5><ul><li>跟传统决策树⼀致，只不过仅在有属性值的子集上计算信息增益，不考虑无属性值的样本</li></ul><p><span class="math display">\[Gain\left( D,a \right) =\rho \times Gain\left( \tilde{D},a \right)\]</span></p><h5id="给定划分属性若样本在该属性上的值缺失如何对该样本进行划分">给定划分属性，若样本在该属性上的值缺失，如何对该样本进行划分？</h5><ul><li>让同⼀个样本以不同的概率划入到不同的子节点中去</li></ul><h2 id="xgboost">XGBoost</h2><h3 id="xgboost特征重要性">XGBoost特征重要性</h3><ol type="1"><li><p>基于覆盖次数的特征重要性</p><p>在 XGBoost中，每个决策树都会选择最佳特征进行划分，因此可以根据每个特征在决策树中被使用的次数来计算特征重要性。具体来说，特征的覆盖次数可以通过计算所有决策树中该特征被使用的次数之和来获得。XGBoost将所有特征的覆盖次数标准化并转换为百分比，以便比较它们的重要性。</p></li><li><p>基于特征分裂次数的特征重要性</p><p>除了覆盖次数之外，XGBoost还可以基于特征分裂的平均增益来计算特征重要性。对于每个决策树，XGBoost都会计算每个特征的分裂增益，即将特征用作分裂点后目标变量的下降量。然后，XGBoost将每个特征的分裂增益加权平均，以获得该特征的平均分裂增益。最后，XGBoost将所有特征的平均分裂增益标准化并转换为百分比，以便比较它们的重要性。</p></li></ol>]]></content>
    
    
    <categories>
      
      <category>计算机</category>
      
      <category>机器学习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>聚类</title>
    <link href="/2023/03/02/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E8%81%9A%E7%B1%BB/"/>
    <url>/2023/03/02/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E8%81%9A%E7%B1%BB/</url>
    
    <content type="html"><![CDATA[<h1 id="聚类">聚类</h1><h2 id="分类">分类</h2><ul><li>基于划分 partition</li><li>基于层次 hierarchical</li><li>基于密度 density-based</li><li>基于网格</li><li>基于模型 model-based</li></ul><h3 id="基于划分">基于划分</h3><ul><li><p>代表算法：K-means</p></li><li><p>优点：收敛速度快</p></li><li><p>缺点：要求类别数目 k可以合理地估计，并且初始中心的选择和噪声会对聚类结果产生很大影响</p></li></ul><h4 id="k-means-vs-knn">K-means vs KNN</h4><ul><li><p>K-means：聚类，无监督</p><ol type="1"><li>随机选择k个样本作为初始聚类中心 a=a1,a2,…ak ；</li><li>针对数据集中每个样本<span class="math inline">\(x_i\)</span>计算它到 k个聚类中心的距离并将其分到距离最小的聚类中心所对应的类中；</li><li>针对每个类别<span class="math inline">\(a_j\)</span>，重新计算它的聚类中心<span class="math inline">\(a_j=\frac{1}{|ci|}\sum\limits_{x∈c_i}x\)</span>（即属于该类的所有样本的质心）；</li><li>重复上面2、3两步，直到达到某个中止条件（迭代次数、最小误差变化等）</li></ol><p>优点：</p><ul><li>容易理解，聚类效果不错，虽然是局部最优， 但往往局部最优就够了；</li><li>处理大数据集的时候，该算法可以保证较好的伸缩性；</li><li>当簇近似高斯分布的时候，效果非常不错；</li><li>算法复杂度低。</li></ul><p>缺点：</p><ul><li>K 值需要人为设定，不同 K 值得到的结果不一样；</li><li>对初始的簇中心敏感，不同选取方式会得到不同结果；</li><li>对异常值敏感；</li><li>不适合太离散的分类、样本类别不平衡的分类、非凸形状的分类。</li></ul></li><li><p>KNN：分类/回归，有监督</p><p>将预测点与所有点距离进行计算，然后保存并排序，选出前面K个值看看哪些类别比较多，则预测的点属于哪类</p><p>优点：</p><ul><li>简单易用，相比其他算法，KNN算是比较简洁明了的算法。即使没有很高的数学基础也能搞清楚它的原理。</li><li>模型训练时间快，上面说到KNN算法是惰性的，这里也就不再过多讲述。预测效果好。</li><li>对异常值不敏感</li></ul><p>缺点：</p><ul><li>对内存要求较高，因为该算法存储了所有训练数据</li><li>对不相关的功能和数据规模敏感</li></ul></li></ul><h4 id="knn-k值选取">KNN-K值选取</h4><p>从选取一个较小的K值开始，不断增加K的值，然后计算验证集合的方差，最终找到一个比较合适的K值</p><h3 id="基于层次">基于层次</h3><p>对给定的数据进行层次分解，直到满足某种条件为止。分为自底向上法（AGNES）和自顶向下法（DIANA）</p><ul><li>优点：距离和规则的相似度容易定义，限制少，不需要预先制定簇的个数，可以发现簇的层次关系</li><li>缺点：计算复杂度太高，奇异值也能产生很大影响，算法很可能聚类成链状</li></ul><h3 id="基于密度">基于密度</h3><p>寻找被低密度区域分离的高密度区域。与基于距离的聚类算法不同的是，基于距离的聚类算法的聚类结果是球状（凸）的簇，而基于密度的聚类算法可以发现任意形状的簇。基于密度的聚类方法是从数据对象分布区域的密度着手的。如果给定类中的数据对象在给定的范围区域中，则数据对象的密度超过某一阈值就继续聚类。这种方法通过连接密度较大的区域，能够形成不同形状的簇，而且可以消除孤立点和噪声对聚类质量的影响，以及发现任意形状的簇</p><h4 id="dbscan">DBSCAN</h4><p>DBSCAN（Density-Based Spatial Clustering of Applications withNoise）是一种基于密度的聚类算法，它能够将具有高密度的样本聚集成簇，并可以识别出异常点（噪声）。</p><p>DBSCAN的基本原理是将数据集中的样本点分为三类：核心点（CorePoint）、边界点（Border Point）和噪声点（NoisePoint）。其中核心点是在一个给定半径ε内含有不少于MinPts个样本的点；边界点是在半径ε内含有少于MinPts个样本的点，但是该点距离核心点不超过ε；噪声点是既不是核心点也不是边界点的点。该算法的聚类过程如下：</p><ol type="1"><li>任选一个未被访问过的点，以该点为中心，半径为ε构建一个以该点为核心点的邻域；</li><li>如果该邻域内包含不少于MinPts个点，则该邻域内的所有点被标记为同一簇，同时这些点也是核心点，然后进入下一步继续扩展该簇；</li><li>如果该邻域内包含少于MinPts个点，则该点被标记为边界点，并跳过该点，重复步骤1；</li><li>如果该点为噪声点，则直接跳过该点，重复步骤1。</li></ol><p>扩展簇的方法为：</p><ol type="1"><li>以核心点为中心，以半径ε构建以该核心点为中心的邻域；</li><li>遍历该邻域内所有的点，如果该点未被访问过，则将该点标记为该簇的一个成员，并检查该点是否为核心点；</li><li>如果该点为核心点，则继续以该点为中心，以半径ε构建以该点为中心的邻域，并将该邻域内的所有点都加入该簇；</li><li>如果该点为边界点，则将该点加入该簇，并不再扩展该点。</li></ol><p>重复以上过程，直到所有点都被访问过为止。DBSCAN算法的核心思想是：一个簇是一些密度相连的点的集合，这个集合与其他的密度相连的点的集合是分开的。同时，该算法能够识别出噪声点，避免了噪声点对聚类结果的干扰。</p><h4 id="optics">OPTICS</h4><p>OPTICS（Ordering Points To Identify the ClusteringStructure）是一种密度聚类算法，用于发现任意形状和任意密度的聚类结构。它可以处理高维数据和噪声数据，同时不需要预先指定簇的数量。</p><p>OPTICS算法的主要思想是，对于每个数据点，通过计算其与周围点的距离来评估其密度，并根据密度大小将其分类为核心点、边界点或噪声点。接着，从核心点开始构建聚类簇，通过对距离和密度进行扫描，将相邻的核心点分配到同一个簇中。最终，OPTICS算法可以得到一个聚类结构图，其中每个簇对应一个局部密度较高的区域。</p><p>OPTICS算法有一些优点。首先，它可以自适应地发现不同形状和密度的聚类结构，而不需要事先知道簇的数量。其次，它可以处理噪声数据和高维数据。此外，它不需要预先设置距离阈值，因此可以灵活地处理不同密度的数据。</p><p>然而，OPTICS算法也存在一些缺点。首先，它的计算复杂度较高，因此对于大规模数据集，其性能可能较差。其次，其结果可能受到参数设置的影响，例如邻域大小等。因此，在使用OPTICS算法时，需要根据具体情况进行参数调整和结果评估。</p><h3 id="基于网格">基于网格</h3><p>将空间量化为有限数目的单元，可以形成一个网格结构，所有聚类都在网格上进行。基本思想就是将每个属性的可能值分割成许多相邻的区间，并创建网格单元的集合。每个对象落入一个网格单元，网格单元对应的属性空间包含该对象的值</p><ul><li>优点：处理速度快，其处理时间独立于数据对象数，而仅依赖于量化空间中的每一维的单元数</li><li>缺点：只能发现边界是水平或垂直的簇，而不能检测到斜边界。另外，在处理高维数据时，网格单元的数目会随着属性维数的增长而成指数级增长</li></ul><h3 id="基于模型">基于模型</h3><p>试图优化给定的数据和某些数学模型之间的适应性的。该方法给每一个簇假定了一个模型，然后寻找数据对给定模型的最佳拟合。假定的模型可能是代表数据对象在空间分布情况的密度函数或者其他函数。这种方法的基本原理就是假定目标数据集是由一系列潜在的概率分布所决定的。簇的数目是基于标准的统计数字自动决定的，噪声或孤立点也是通过统计数字来分析的。基于模型的聚类方法试图优化给定的数据和某些数据模型之间的适应性</p>]]></content>
    
    
    <categories>
      
      <category>计算机</category>
      
      <category>机器学习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  
</search>
